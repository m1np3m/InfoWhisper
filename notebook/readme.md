# ğŸ¤– Chatbot Há»i ÄÃ¡p RAG Há»™i Thoáº¡i (Conversational RAG)

Há»‡ thá»‘ng chatbot nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng theo kiáº¿n trÃºc **Retrieval-Augmented Generation (RAG)**, há»— trá»£ kháº£ nÄƒng há»™i thoáº¡i tá»± nhiÃªn vá»›i ngÆ°á»i dÃ¹ng, sá»­ dá»¥ng káº¿t há»£p:

- âœ… **Gemini LLM** (Google Generative AI)
- ğŸ” **Qdrant** cho truy váº¥n vector ngá»¯ nghÄ©a
- ğŸ§  **Semantic Cache** sá»­ dá»¥ng Redis
- ğŸ“š **Lá»‹ch sá»­ há»™i thoáº¡i** vÃ  **Reranking** theo bá»‘i cáº£nh
- ğŸ›¡ï¸ **PhÃ¡t hiá»‡n Prompt Injection** qua Lakera Guard
- ğŸ” **Theo dÃµi pipeline** vá»›i Langfuse (tracing)

---

## ğŸ§± ThÃ nh pháº§n chÃ­nh

### 1. `GeminiLLM`

Lá»›p trá»«u tÆ°á»£ng cho Gemini LLM, há»— trá»£ cáº¥u hÃ¬nh `temperature`, `top_k`, `top_p` vÃ  tÃ­ch há»£p Langfuse Ä‘á»ƒ tracking cÃ¡c truy váº¥n.

```python
llm = GeminiLLM(model_name="gemini-pro", api_key="...", llm_config=config)
response = llm.generate(query="Explain RAG", system_prompt="You are a helpful assistant.")
```

### 2. RAGSingleVectorSearch

Pipeline chÃ­nh cho RAG:
- MÃ£ hÃ³a truy váº¥n thÃ nh vector
- Truy váº¥n Qdrant
- XÃ¢y dá»±ng prompt vá»›i ngá»¯ cáº£nh phÃ¹ há»£p
- Sinh cÃ¢u tráº£ lá»i vá»›i Gemini LLM

```python
rag = RAGSingleVectorSearch(llm, embedding_model, qdrant_client, config)
response = rag.generate_response(query, context, system_prompt)
```

### 3. HistoryGuidedReranker

Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a tÃ¬m kiáº¿m báº±ng cÃ¡ch sáº¯p xáº¿p láº¡i káº¿t quáº£ dá»±a trÃªn:
- Äá»™ tÆ°Æ¡ng Ä‘á»“ng vá»›i truy váº¥n
- Má»©c liÃªn quan Ä‘áº¿n lá»‹ch sá»­ há»™i thoáº¡i (theo thá»i gian)

```python
reranker = HistoryGuidedReranker(embedding_model)
reranked_contexts = reranker.rerank(query, contexts, history_texts)
```

### 4. SemanticPromptCache

Redis cache dÃ¹ng Ä‘á»ƒ lÆ°u cÃ¡c prompt tÆ°Æ¡ng tá»± theo nghÄ©a, giÃºp:
- TrÃ¡nh gá»i láº¡i LLM khÃ´ng cáº§n thiáº¿t
- Truy xuáº¥t nhanh vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao

```python
cached = semantic_cache.get_cached_response(prompt)
semantic_cache.cache_response(prompt, response)
```

### 5. ConversationalRAGChatbot

Há»‡ thá»‘ng chatbot Ä‘áº§y Ä‘á»§, tÃ­ch há»£p:
- LÆ°u lá»‹ch sá»­ há»™i thoáº¡i (Redis)
- Kiá»ƒm tra cache semantic
- TÃ¬m kiáº¿m RAG vÃ  reranking
- Báº£o máº­t prompt injection
- Truy váº¿t pipeline vá»›i Langfuse

```python
bot = ConversationalRAGChatbot(
    rag_pipeline=rag,
    config=config,
    reranker=reranker,
    semantic_cache=semantic_cache,
    langfuse_client=...
)

response = bot.chat("Giáº£i thÃ­ch attention trong transformer?")
```

## ğŸ“¦ TÃ­nh NÄƒng Ná»•i Báº­t
| TÃ­nh nÄƒng                   | MÃ´ táº£                             |
| --------------------------- | --------------------------------- |
| MÃ´ hÃ¬nh ngÃ´n ngá»¯ (LLM)      | Gemini cá»§a Google                 |
| TÃ¬m kiáº¿m vector             | Qdrant (single vector)            |
| Semantic Caching            | Redis + cosine similarity         |
| Ghi nhá»› há»™i thoáº¡i           | Redis TTL, cÃ³ nÃ©n gzip            |
| Kiá»ƒm tra prompt injection   | Lakera Guard API                  |
| Reranking ngá»¯ cáº£nh          | Dá»±a vÃ o lá»‹ch sá»­ + vector truy váº¥n |
| Tracking & quan sÃ¡t         | Langfuse tracing                  |
| Giá»›i háº¡n chiá»u dÃ i ngá»¯ cáº£nh | Tá»± Ä‘á»™ng cáº¯t bá»›t theo token        |
| Gzip compression (tÃ¹y chá»n) | NÃ©n dá»¯ liá»‡u Ä‘á»ƒ tá»‘i Æ°u Redis       |

## ğŸ§ª VÃ­ dá»¥ sá»­ dá»¥ng
```python
query = "CÆ¡ cháº¿ attention hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?"
response = bot.chat(query)
print(response)
```

## ğŸ’¡ Má»Ÿ Rá»™ng Dá»… DÃ ng
- Thay Gemini báº±ng OpenAI, Claude, hoáº·c mÃ´ hÃ¬nh local
- Thay Qdrant báº±ng FAISS hoáº·c Pinecone
- TÃ­ch há»£p thÃªm multi-vector search hoáº·c hybrid search
