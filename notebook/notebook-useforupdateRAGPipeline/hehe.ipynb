{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "798bbe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:üîÑ Trying HuggingFace Pipeline approach...\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3\n",
      "INFO:__main__:üìö Embeddings loaded: BAAI/bge-m3\n",
      "INFO:__main__:üîÑ Trying HuggingFace Pipeline approach...\n",
      "INFO:__main__:üì• Loading pipeline for: vinhthuan/vietnamese-news-summarizer-v2\n",
      "ERROR:__main__:‚ùå Failed to load pipeline: Unrecognized model in vinhthuan/vietnamese-news-summarizer-v2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "WARNING:__main__:‚ö†Ô∏è Failed to load vinhthuan/vietnamese-news-summarizer-v2, trying fallback: Unrecognized model in vinhthuan/vietnamese-news-summarizer-v2. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v3, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glm4, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen3, qwen3_moe, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam_vision_model, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "INFO:__main__:üîÑ Trying fallback model...\n",
      "INFO:__main__:üì• Loading simple pipeline for: microsoft/DialoGPT-medium\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from pydantic import Field\n",
    "import torch\n",
    "from typing import Optional, List, Any\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CustomVietnameseLLM(LLM):\n",
    "    \"\"\"\n",
    "    Custom LangChain LLM wrapper cho Vietnamese model - s·ª≠ d·ª•ng HuggingFace Transformers thay v√¨ Unsloth\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define Pydantic fields\n",
    "    model_name: str = Field(default=\"vinhthuan/vietnamese-news-summarizer-v2\")\n",
    "    max_seq_length: int = Field(default=2048)\n",
    "    device: str = Field(default=\"auto\")\n",
    "    model: Any = Field(default=None, exclude=True)\n",
    "    tokenizer: Any = Field(default=None, exclude=True)\n",
    "    \n",
    "    def __init__(self, model_name: str = \"vinhthuan/vietnamese-news-summarizer-v2\", \n",
    "                 max_seq_length: int = 2048, device: str = \"auto\", **kwargs):\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=max_seq_length,\n",
    "            device=device,\n",
    "            **kwargs\n",
    "        )\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model v√† tokenizer using HuggingFace Transformers\"\"\"\n",
    "        try:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            \n",
    "            logger.info(f\"üì• Loading model: {self.model_name}\")\n",
    "            \n",
    "            # Determine device\n",
    "            if self.device == \"auto\":\n",
    "                if torch.cuda.is_available():\n",
    "                    device = \"cuda\"\n",
    "                    logger.info(\"üöÄ Using CUDA\")\n",
    "                else:\n",
    "                    device = \"cpu\"\n",
    "                    logger.info(\"üíª Using CPU\")\n",
    "            else:\n",
    "                device = self.device\n",
    "            \n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Add pad token if not exists\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            # Load model with appropriate settings\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            }\n",
    "            \n",
    "            # Add quantization for GPU if available\n",
    "            if device == \"cuda\":\n",
    "                try:\n",
    "                    from transformers import BitsAndBytesConfig\n",
    "                    quantization_config = BitsAndBytesConfig(\n",
    "                        load_in_4bit=True,\n",
    "                        bnb_4bit_compute_dtype=torch.float16,\n",
    "                        bnb_4bit_use_double_quant=True,\n",
    "                        bnb_4bit_quant_type=\"nf4\"\n",
    "                    )\n",
    "                    model_kwargs[\"quantization_config\"] = quantization_config\n",
    "                    logger.info(\"üîß Using 4-bit quantization\")\n",
    "                except ImportError:\n",
    "                    logger.warning(\"‚ö†Ô∏è BitsAndBytesConfig not available, loading without quantization\")\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            if device == \"cuda\" and \"quantization_config\" not in model_kwargs:\n",
    "                self.model = self.model.to(device)\n",
    "            \n",
    "            # Set to evaluation mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            logger.info(\"‚úÖ Model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to load model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_qa_prompt(self, context: str, question: str) -> str:\n",
    "        \"\"\"\n",
    "        T·∫°o prompt cho Q&A task\n",
    "        \"\"\"\n",
    "        return f\"\"\"<|im_start|>system\n",
    "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† h·ªØu √≠ch. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p. \n",
    "H√£y tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn v√† h·ªØu √≠ch. N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y th√¥ng b√°o m·ªôt c√°ch l·ªãch s·ª±.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "D·ª±a v√†o th√¥ng tin sau ƒë√¢y:\n",
    "\n",
    "{context}\n",
    "\n",
    "H√£y tr·∫£ l·ªùi c√¢u h·ªèi: {question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Main method ƒë·ªÉ generate response\"\"\"\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=self.max_seq_length - 200  # Leave space for generation\n",
    "            )\n",
    "            \n",
    "            # Move to device if needed\n",
    "            device = next(self.model.parameters()).device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    min_new_tokens=10,\n",
    "                    max_new_tokens=kwargs.get('max_new_tokens', 200),\n",
    "                    do_sample=True,\n",
    "                    temperature=kwargs.get('temperature', 0.7),\n",
    "                    top_p=kwargs.get('top_p', 0.9),\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "            \n",
    "            # Extract answer from response\n",
    "            if \"<|im_start|>assistant\" in response:\n",
    "                answer = response.split(\"<|im_start|>assistant\")[-1]\n",
    "                # Clean up the answer\n",
    "                for token in [\"</s>\", \"<|im_end|>\", \"<|endoftext|>\"]:\n",
    "                    if token in answer:\n",
    "                        answer = answer.split(token)[0]\n",
    "                answer = answer.strip()\n",
    "                if answer:\n",
    "                    return answer\n",
    "            \n",
    "            # Fallback: get text after the original prompt\n",
    "            input_length = len(self.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True))\n",
    "            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if len(full_response) > input_length:\n",
    "                answer = full_response[input_length:].strip()\n",
    "                if answer:\n",
    "                    return answer\n",
    "            \n",
    "            return \"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error during generation: {str(e)}\")\n",
    "            return f\"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_vietnamese_llm\"\n",
    "\n",
    "# Alternative: Using HuggingFace Pipeline (simpler approach)\n",
    "class HuggingFaceVietnameseLLM(LLM):\n",
    "    \"\"\"\n",
    "    Simpler implementation using HuggingFace Pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name: str = Field(default=\"vinhthuan/vietnamese-news-summarizer-v2\")\n",
    "    max_length: int = Field(default=512)\n",
    "    pipeline: Any = Field(default=None, exclude=True)\n",
    "    \n",
    "    def __init__(self, model_name: str = \"vinhthuan/vietnamese-news-summarizer-v2\", \n",
    "                 max_length: int = 512, **kwargs):\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            max_length=max_length,\n",
    "            **kwargs\n",
    "        )\n",
    "        self._load_pipeline()\n",
    "    \n",
    "    def _load_pipeline(self):\n",
    "        \"\"\"Load HuggingFace pipeline\"\"\"\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "            \n",
    "            logger.info(f\"üì• Loading pipeline for: {self.model_name}\")\n",
    "            \n",
    "            # Determine device\n",
    "            device = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n",
    "            \n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model_name,\n",
    "                tokenizer=self.model_name,\n",
    "                device=device,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            logger.info(\"‚úÖ Pipeline loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to load pipeline: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate response using pipeline\"\"\"\n",
    "        try:\n",
    "            # Generate response\n",
    "            result = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=kwargs.get('max_new_tokens', 200),\n",
    "                temperature=kwargs.get('temperature', 0.7),\n",
    "                top_p=kwargs.get('top_p', 0.9),\n",
    "                do_sample=True,\n",
    "                return_full_text=False,  # Only return generated text\n",
    "                pad_token_id=self.pipeline.tokenizer.eos_token_id,\n",
    "            )\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                generated_text = result[0]['generated_text'].strip()\n",
    "                \n",
    "                # Clean up the response\n",
    "                for token in [\"</s>\", \"<|im_end|>\", \"<|endoftext|>\"]:\n",
    "                    if token in generated_text:\n",
    "                        generated_text = generated_text.split(token)[0]\n",
    "                \n",
    "                return generated_text.strip() if generated_text else \"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p.\"\n",
    "            \n",
    "            return \"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error during generation: {str(e)}\")\n",
    "            return f\"L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"huggingface_vietnamese_llm\"\n",
    "\n",
    "# Fallback: Simple Vietnamese LLM using a more reliable model\n",
    "class SimpleVietnameseLLM(LLM):\n",
    "    \"\"\"\n",
    "    Fallback implementation using a simpler, more reliable Vietnamese model\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name: str = Field(default=\"vinai/phobert-base\")\n",
    "    pipeline: Any = Field(default=None, exclude=True)\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/DialoGPT-medium\", **kwargs):\n",
    "        # Use a more reliable model that works better with transformers\n",
    "        super().__init__(\n",
    "            model_name=model_name,\n",
    "            **kwargs\n",
    "        )\n",
    "        self._load_pipeline()\n",
    "    \n",
    "    def _load_pipeline(self):\n",
    "        \"\"\"Load a simple text generation pipeline\"\"\"\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "            \n",
    "            logger.info(f\"üì• Loading simple pipeline for: {self.model_name}\")\n",
    "            \n",
    "            device = 0 if torch.cuda.is_available() else -1\n",
    "            \n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model_name,\n",
    "                device=device,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            logger.info(\"‚úÖ Simple pipeline loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to load simple pipeline: {str(e)}\")\n",
    "            # Ultimate fallback - just return input processing\n",
    "            self.pipeline = None\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate simple response\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            return \"ƒê√¢y l√† c√¢u tr·∫£ l·ªùi m·∫´u. Model ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.\"\n",
    "        \n",
    "        try:\n",
    "            result = self.pipeline(\n",
    "                prompt,\n",
    "                max_new_tokens=kwargs.get('max_new_tokens', 100),\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                return result[0]['generated_text'].strip()\n",
    "            \n",
    "            return \"Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error in simple generation: {str(e)}\")\n",
    "            return f\"L·ªói: {str(e)}\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"simple_vietnamese_llm\"\n",
    "\n",
    "class RAGPipelineWithCustomModel:\n",
    "    \"\"\"\n",
    "    RAG Pipeline v·ªõi multiple fallback options cho Vietnamese models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, qdrant_url, qdrant_api_key, \n",
    "                 model_name=\"vinhthuan/vietnamese-news-summarizer-v2\",\n",
    "                 hf_embedding_model=\"BAAI/bge-m3\",\n",
    "                 use_pipeline=True):\n",
    "        self.QDRANT_URL = qdrant_url\n",
    "        self.QDRANT_API_KEY = qdrant_api_key\n",
    "        self.model_name = model_name\n",
    "        self.hf_embedding_model = hf_embedding_model\n",
    "        self.use_pipeline = use_pipeline\n",
    "        \n",
    "        # Initialize components\n",
    "        self.embeddings = self.load_embeddings()\n",
    "        self.llm = self.load_custom_model()\n",
    "        self.prompt = self.load_prompt_template()\n",
    "        self.current_source = None\n",
    "        \n",
    "        logger.info(\"üöÄ RAG Pipeline with Custom Model initialized!\")\n",
    "    \n",
    "    def load_embeddings(self):\n",
    "        \"\"\"Load HuggingFace embeddings\"\"\"\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=self.hf_embedding_model)\n",
    "        logger.info(f\"üìö Embeddings loaded: {self.hf_embedding_model}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def load_custom_model(self):\n",
    "        \"\"\"Load custom Vietnamese model with fallbacks\"\"\"\n",
    "        try:\n",
    "            if self.use_pipeline:\n",
    "                # Try HuggingFace Pipeline approach first\n",
    "                logger.info(\"üîÑ Trying HuggingFace Pipeline approach...\")\n",
    "                llm = HuggingFaceVietnameseLLM(model_name=self.model_name)\n",
    "            else:\n",
    "                # Try custom implementation\n",
    "                logger.info(\"üîÑ Trying custom implementation...\")\n",
    "                llm = CustomVietnameseLLM(model_name=self.model_name)\n",
    "            \n",
    "            logger.info(f\"ü§ñ Custom LLM loaded: {self.model_name}\")\n",
    "            return llm\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Failed to load {self.model_name}, trying fallback: {str(e)}\")\n",
    "            \n",
    "            try:\n",
    "                # Fallback to a simpler model\n",
    "                logger.info(\"üîÑ Trying fallback model...\")\n",
    "                llm = SimpleVietnameseLLM()\n",
    "                logger.info(\"ü§ñ Fallback LLM loaded\")\n",
    "                return llm\n",
    "                \n",
    "            except Exception as e2:\n",
    "                logger.error(f\"‚ùå All model loading attempts failed: {str(e2)}\")\n",
    "                raise RuntimeError(\"Unable to load any Vietnamese LLM model\")\n",
    "    \n",
    "    def load_retriever(self, retriever_name):\n",
    "        \"\"\"Load Qdrant retriever\"\"\"\n",
    "        # Initialize Qdrant client\n",
    "        client = QdrantClient(\n",
    "            url=self.QDRANT_URL,\n",
    "            api_key=self.QDRANT_API_KEY,\n",
    "            prefer_grpc=False\n",
    "        )\n",
    "\n",
    "        # Create vector store for querying\n",
    "        db = QdrantVectorStore(\n",
    "            client=client,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=retriever_name,\n",
    "            content_payload_key=\"page_content\",\n",
    "        )\n",
    "\n",
    "        # Configure retriever\n",
    "        retriever = db.as_retriever(\n",
    "            search_kwargs={\"k\": 5}\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"üîç Retriever loaded for collection: {retriever_name}\")\n",
    "        return retriever\n",
    "    \n",
    "    def load_prompt_template(self):\n",
    "        \"\"\"Load prompt template optimized for Q&A task\"\"\"\n",
    "        query_template = '''\n",
    "B·ªëi c·∫£nh th√¥ng tin:\n",
    "{context}\n",
    "\n",
    "C√¢u h·ªèi: {input}\n",
    "\n",
    "H∆∞·ªõng d·∫´n:\n",
    "1. ƒê·ªçc k·ªπ c√¢u h·ªèi v√† t√¨m th√¥ng tin li√™n quan trong b·ªëi c·∫£nh\n",
    "2. Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn v√† ƒë·∫ßy ƒë·ªß\n",
    "3. S·ª≠ d·ª•ng ti·∫øng Vi·ªát\n",
    "4. N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªß, th√¥ng b√°o l·ªãch s·ª±\n",
    "\n",
    "C√¢u tr·∫£ l·ªùi:\n",
    "'''\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=query_template, \n",
    "            input_variables=[\"context\", \"input\"]\n",
    "        )\n",
    "        return prompt\n",
    "    \n",
    "    def load_rag_pipeline(self, llm, retriever, prompt):\n",
    "        \"\"\"Create RAG chain\"\"\"\n",
    "        rag_chain = create_retrieval_chain(\n",
    "            retriever=retriever,\n",
    "            combine_docs_chain=create_stuff_documents_chain(llm, prompt)\n",
    "        )\n",
    "        return rag_chain\n",
    "    \n",
    "    def rag(self, source):\n",
    "        \"\"\"Get RAG pipeline for specific source\"\"\"\n",
    "        # If source hasn't changed, return existing pipeline\n",
    "        if source == self.current_source:\n",
    "            return self.rag_pipeline\n",
    "        else:\n",
    "            # Recreate pipeline for new source\n",
    "            self.retriever = self.load_retriever(retriever_name=source)\n",
    "            self.rag_pipeline = self.load_rag_pipeline(\n",
    "                llm=self.llm, \n",
    "                retriever=self.retriever, \n",
    "                prompt=self.prompt\n",
    "            )\n",
    "            self.current_source = source\n",
    "            logger.info(f\"üîÑ RAG pipeline updated for source: {source}\")\n",
    "            return self.rag_pipeline\n",
    "    \n",
    "    def ask(self, source: str, question: str, **kwargs) -> dict:\n",
    "        \"\"\"Ask question using RAG pipeline\"\"\"\n",
    "        try:\n",
    "            # Get RAG pipeline\n",
    "            rag_pipeline = self.rag(source)\n",
    "            \n",
    "            # Get answer\n",
    "            result = rag_pipeline.invoke({\n",
    "                \"input\": question,\n",
    "                **kwargs\n",
    "            })\n",
    "            \n",
    "            return {\n",
    "                \"answer\": result.get(\"answer\", \"Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi\"),\n",
    "                \"context\": result.get(\"context\", []),\n",
    "                \"source_documents\": result.get(\"context\", [])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error in ask method: {str(e)}\")\n",
    "            return {\n",
    "                \"answer\": f\"L·ªói: {str(e)}\",\n",
    "                \"context\": [],\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "# Example usage with fallbacks\n",
    "def example_usage():\n",
    "    \"\"\"Example with multiple fallback approaches\"\"\"\n",
    "    \n",
    "    # Try different approaches\n",
    "    approaches = [\n",
    "        {\"use_pipeline\": True, \"name\": \"HuggingFace Pipeline\"},\n",
    "        {\"use_pipeline\": False, \"name\": \"Custom Implementation\"},\n",
    "    ]\n",
    "    \n",
    "    for approach in approaches:\n",
    "        try:\n",
    "            logger.info(f\"üîÑ Trying {approach['name']} approach...\")\n",
    "            \n",
    "            rag_custom = RAGPipelineWithCustomModel(\n",
    "                qdrant_url=QDRANT_URL,\n",
    "                qdrant_api_key=QDRANT_API_KEY,\n",
    "                model_name=\"vinhthuan/vietnamese-news-summarizer-v2\",\n",
    "                use_pipeline=approach[\"use_pipeline\"]\n",
    "            )\n",
    "            \n",
    "            # Test simple question\n",
    "            result = rag_custom.ask(\n",
    "                source=\"news_collection\",\n",
    "                question=\"T√¨nh h√¨nh kinh t·∫ø Vi·ªát Nam nh∆∞ th·∫ø n√†o?\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n‚úÖ {approach['name']} approach successful!\")\n",
    "            print(\"C√¢u tr·∫£ l·ªùi:\", result[\"answer\"])\n",
    "            print(\"S·ªë t√†i li·ªáu tham kh·∫£o:\", len(result[\"source_documents\"]))\n",
    "            break\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ùå {approach['name']} approach failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå All approaches failed. Check your environment setup.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f4dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_URL = \"\"\n",
    "QDRANT_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f3153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3\n",
      "INFO:__main__:üìö Embeddings loaded: BAAI/bge-m3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\"CustomVietnameseLLM\" object has no field \"model_name\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rag_custom \u001b[38;5;241m=\u001b[39m \u001b[43mRAGPipelineWithCustomModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQDRANT_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqdrant_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQDRANT_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvinhthuan/vietnamese-news-summarizer-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m rag_custom\u001b[38;5;241m.\u001b[39mask(\n\u001b[0;32m      8\u001b[0m     source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT√¨nh h√¨nh kinh t·∫ø nh∆∞ th·∫ø n√†o?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n",
      "Cell \u001b[1;32mIn[1], line 136\u001b[0m, in \u001b[0;36mRAGPipelineWithCustomModel.__init__\u001b[1;34m(self, qdrant_url, qdrant_api_key, model_name, hf_embedding_model)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Initialize components\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_embeddings()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_custom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_prompt_template()\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 150\u001b[0m, in \u001b[0;36mRAGPipelineWithCustomModel.load_custom_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_custom_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load custom Vietnamese model\"\"\"\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mCustomVietnameseLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mü§ñ Custom LLM loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m, in \u001b[0;36mCustomVietnameseLLM.__init__\u001b[1;34m(self, model_name, max_seq_length, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinhthuan/vietnamese-news-summarizer-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     25\u001b[0m              max_seq_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m \u001b[38;5;241m=\u001b[39m model_name\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m max_seq_length\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\myenv\\lib\\site-packages\\pydantic\\main.py:995\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    993\u001b[0m     setattr_handler(\u001b[38;5;28mself\u001b[39m, name, value)\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# if None is returned from _setattr_handler, the attribute was set directly\u001b[39;00m\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (setattr_handler \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setattr_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    996\u001b[0m     setattr_handler(\u001b[38;5;28mself\u001b[39m, name, value)  \u001b[38;5;66;03m# call here to not memo on possibly unknown fields\u001b[39;00m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_setattr_handlers__[name] \u001b[38;5;241m=\u001b[39m setattr_handler\n",
      "File \u001b[1;32md:\\anaconda\\envs\\myenv\\lib\\site-packages\\pydantic\\main.py:1042\u001b[0m, in \u001b[0;36mBaseModel._setattr_handler\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_fields__:\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextra\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1041\u001b[0m         \u001b[38;5;66;03m# TODO - matching error\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;66;03m# attribute does not exist, so put it in extra\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_extra__[name] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: \"CustomVietnameseLLM\" object has no field \"model_name\""
     ]
    }
   ],
   "source": [
    "rag_custom = RAGPipelineWithCustomModel(\n",
    "    qdrant_url=QDRANT_URL,\n",
    "    qdrant_api_key=QDRANT_API_KEY,\n",
    "    model_name=\"vinhthuan/vietnamese-news-summarizer-v2\"\n",
    ")\n",
    "\n",
    "result = rag_custom.ask(\n",
    "    source=\"news_collection\",\n",
    "    question=\"T√¨nh h√¨nh kinh t·∫ø nh∆∞ th·∫ø n√†o?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb561de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import os\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "class RAGPipelineSetup:\n",
    "    def __init__(self, qdrant_url, qdrant_api_key, gemini_api_key, hf_api_key, \n",
    "                 hf_model_name=\"BAAI/bge-m3\"):\n",
    "        self.QDRANT_URL = qdrant_url\n",
    "        self.QDRANT_API_KEY = qdrant_api_key\n",
    "        self.GEMINI_API_KEY = gemini_api_key\n",
    "        self.HF_API_KEY = hf_api_key\n",
    "        self.HF_MODEL_NAME = hf_model_name\n",
    "        self.embeddings = self.load_embeddings()\n",
    "        self.pipe = self.load_model_pipeline()\n",
    "        self.prompt = self.load_prompt_template()\n",
    "        self.current_source = None  # Initialize current source as None\n",
    "\n",
    "    def load_embeddings(self):\n",
    "        # Use HuggingFaceEndpointEmbeddings for API-based embeddings\n",
    "        embeddings = HuggingFaceEndpointEmbeddings(\n",
    "             model=\"BAAI/bge-m3\",\n",
    "             task=\"feature-extraction\",\n",
    "             huggingfacehub_api_token=\"\"\n",
    "         )\n",
    "        #embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "        return embeddings\n",
    "\n",
    "    def load_retriever(self, retriever_name):\n",
    "        # Initialize Qdrant client\n",
    "        client = QdrantClient(\n",
    "            url=self.QDRANT_URL,\n",
    "            api_key=self.QDRANT_API_KEY,\n",
    "            prefer_grpc=False\n",
    "        )\n",
    "\n",
    "        # Create vector store for querying\n",
    "        db = QdrantVectorStore(\n",
    "            client=client,\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=retriever_name,\n",
    "            content_payload_key=\"page_content\",  # Key for content\n",
    "    \n",
    "        )\n",
    "\n",
    "        # Configure retriever to get up to 5 results with MMR search\n",
    "        retriever = db.as_retriever(\n",
    "            search_kwargs={\"k\": 5}\n",
    "        )\n",
    "        return retriever\n",
    "\n",
    "    def load_model_pipeline(self, max_output_tokens=1024):\n",
    "        # llm = ChatGoogleGenerativeAI(\n",
    "        #     model=\"gemini-2.0-flash-lite\",\n",
    "        #     temperature=0,\n",
    "        #     max_output_tokens=max_output_tokens,\n",
    "        #     api_key=self.GEMINI_API_KEY,  # ƒë·ªïi t·ª´ google_api_key th√†nh api_key\n",
    "        #     client_options={\"api_endpoint\": \"https://gateway.helicone.ai\"},\n",
    "        #     additional_headers={\n",
    "        #         \"helicone-auth\": f\"Bearer sk-helicone-7z6guyy-scsul3i-sdoousq-gwjriui\",\n",
    "        #         \"helicone-target-url\": \"https://generativelanguage.googleapis.com\"\n",
    "        #     },\n",
    "        #     transport=\"rest\",\n",
    "        # )\n",
    "\n",
    "        # hf = HuggingFacePipeline.from_model_id(\n",
    "        #     model_id=\"google/gemma-3-1b-it\",\n",
    "        #     task=\"text-generation\",\n",
    "        #     device_map='auto'\n",
    "        # )\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",  # ho·∫∑c \"fp4\"\n",
    "            bnb_4bit_compute_dtype=\"float16\"\n",
    "        )\n",
    "        model_id = \"vinhthuan/vietnamese-news-summarizer-v3\" \n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config\n",
    "        )\n",
    "\n",
    "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, return_full_text=False)\n",
    "        hf = HuggingFacePipeline(pipeline=pipe)\n",
    "        return hf\n",
    "\n",
    "    def load_prompt_template(self):\n",
    "        # Structure prompt for assistant\n",
    "        query_template = '''\n",
    "      ### B·ªëi c·∫£nh tin t·ª©c:\n",
    "      {context}\n",
    "\n",
    "      ### C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n",
    "      {input}\n",
    "\n",
    "      ### H∆∞·ªõng d·∫´n cho Tr·ª£ l√Ω:\n",
    "      1. ƒê·ªçc k·ªπ c√¢u h·ªèi v√† x√°c ƒë·ªãnh r√µ m·ª•c ƒë√≠ch c·ªßa ng∆∞·ªùi d√πng.\n",
    "      2. T√¨m ki·∫øm th√¥ng tin ch√≠nh x√°c v√† li√™n quan nh·∫•t trong ph·∫ßn b·ªëi c·∫£nh ph√≠a tr√™n.\n",
    "      3. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng v√† ƒë√∫ng tr·ªçng t√¢m ƒë·ªÉ gi·∫£i ƒë√°p c√¢u h·ªèi.\n",
    "      4. N·∫øu kh√¥ng th·ªÉ t√¨m th·∫•y c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp t·ª´ b·ªëi c·∫£nh, h√£y l·ªãch s·ª± th√¥ng b√°o v√† c√≥ th·ªÉ g·ª£i √Ω h∆∞·ªõng t√¨m hi·ªÉu th√™m.\n",
    "      5. Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\n",
    "      6. N·∫øu ph·∫ßn b·ªëi c·∫£nh kh√¥ng c√≥ th√¥ng tin ho·∫∑c qu√° √≠t, h√£y th√¥ng b√°o cho ng∆∞·ªùi d√πng m·ªôt c√°ch kh√©o l√©o.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        prompt = PromptTemplate(template=query_template, input_variables=[\"context\", \"input\"])\n",
    "        return prompt\n",
    "\n",
    "    def load_rag_pipeline(self, llm, retriever, prompt):\n",
    "        # Create Retrieval Augmented Generation chain\n",
    "        rag_chain = create_retrieval_chain(\n",
    "            retriever=retriever,\n",
    "            combine_docs_chain=create_stuff_documents_chain(llm, prompt)\n",
    "        )\n",
    "        \n",
    "        return rag_chain\n",
    "\n",
    "    def rag(self, source):\n",
    "        # If current source hasn't changed, return existing pipeline\n",
    "        if source == self.current_source:\n",
    "            return self.rag_pipeline\n",
    "        else:\n",
    "            # If source changed, recreate pipeline components\n",
    "            self.retriever = self.load_retriever(retriever_name=source)\n",
    "            self.pipe = self.load_model_pipeline()\n",
    "            self.prompt = self.load_prompt_template()\n",
    "            self.rag_pipeline = self.load_rag_pipeline(llm=self.pipe, retriever=self.retriever, prompt=self.prompt)\n",
    "            self.current_source = source  # Update current source\n",
    "            return self.rag_pipeline\n",
    "    \n",
    "    # Function to debug retrieved documents\n",
    "    def debug_retrieve(self, source, query):\n",
    "        if source != self.current_source:\n",
    "            self.retriever = self.load_retriever(retriever_name=source)\n",
    "            self.current_source = source\n",
    "            \n",
    "        docs = self.retriever.get_relevant_documents(query)\n",
    "        return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f09d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\myenv\\lib\\site-packages\\transformers\\quantizers\\auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Device set to use cuda:0\n",
      "d:\\anaconda\\envs\\myenv\\lib\\site-packages\\transformers\\quantizers\\auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√¢u h·ªèi: Google va iphone\n",
      "Tr·∫£ l·ªùi:  Ng∆∞·ªùi d√πng: B·∫°n c√≥ th·ªÉ gi√∫p t√¥i hi·ªÉu r√µ v·ªÅ s·ª± kh√°c bi·ªát gi·ªØa Google v√† iPhone kh√¥ng? T√¥i ƒëang t√¨m hi·ªÉu v·ªÅ v·ªã tr√≠ c·ªßa ch√∫ng trong th·ªã tr∆∞·ªùng di ƒë·ªông.\n",
      "\n",
      "Assistant: 1. **Google** - M·ªôt c√¥ng c·ª• t√¨m ki·∫øm kh·ªïng l·ªì c·ªßa Alphabet Inc., n∆°i m√† Google Holdings Inc. n·∫Øm gi·ªØ c·ªï ph·∫ßn nh·ªè. Google n·ªïi ti·∫øng v·ªõi h·ªá th·ªëng x·∫øp h·∫°ng k·∫øt qu·∫£ t√¨m ki·∫øm theo ƒë·ªô li√™n quan v√† t·∫ßm quan tr·ªçng, thay v√¨ ch·ªß ƒë·ªÅ nh∆∞ Facebook hay Instagram.\n",
      "\n",
      "2. **iPhone** - S·∫£n ph·∫©m di ƒë·ªông ph·ªï bi·∫øn nh·∫•t c·ªßa Apple Inc., ra ƒë·ªùi v√†o nƒÉm 2007. iPhone n·ªïi ti·∫øng v·ªõi m√†n h√¨nh hi·ªÉn th·ªã l·ªõn, h·ªá ƒëi·ªÅu h√†nh iOS, v√† kh·∫£ nƒÉng k·∫øt n·ªëi internet nhanh ch√≥ng qua m·∫°ng Wi-Fi v√† 4G LTE.\n",
      "\n",
      "Trong ph·∫ßn m·ªÅm di ƒë·ªông, Google th∆∞·ªùng ƒë∆∞·ª£c ∆∞u ti√™n b·ªüi c√°c nh√† ph√°t tri·ªÉn ·ª©ng d·ª•ng v√¨ t√≠nh ·ªïn ƒë·ªãnh v√† hi·ªáu su·∫•t cao c·ªßa n·ªÅn t·∫£ng Android. Ng∆∞·ª£c l·∫°i, iPhone n·ªïi b·∫≠t b·ªüi t√≠nh linh ho·∫°t v√† d·ªÖ d√†ng t√πy ch·ªânh c·ªßa iOS.\n",
      "\n",
      "Tuy nhi√™n, s·ª± c·∫°nh tranh ng√†y c√†ng tƒÉng gi·ªØa c√°c h√£ng di ƒë·ªông, ƒë·∫∑c bi·ªát l√† sau khi Apple ra m·∫Øt iPad Pro, khi·∫øn c·∫£ Google l·∫´n iPhone ph·∫£i c√¢n nh·∫Øc h∆∞·ªõng ƒëi ti·∫øp theo c·ªßa m√¨nh. Google ƒëang t·∫≠p trung v√†o lƒ©nh v·ª±c k√≠nh th√¥ng minh v√† ph√¢n t√≠ch d·ªØ li·ªáu ƒë·ªÉ t·∫°o ra tr·∫£i nghi·ªám ng∆∞·ªùi d√πng m·ªõi, trong khi Apple ti·∫øp t·ª•c n√¢ng c·∫•p iPhone ƒë·ªÉ c·∫£i thi·ªán tr·∫£i nghi·ªám di ƒë·ªông. \n",
      "\n",
      "ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ v·ªÅ v·ªã tr√≠ c·ªßa Google v√† iPhone trong th·ªã tr∆∞·ªùng di ƒë·ªông, c·∫£ hai ƒë·ªÅu ƒëang ph·∫£i ƒë·ªëi m·∫∑t v·ªõi th·ª≠ th√°ch v·ªÅ vi·ªác duy tr√¨ s·ª± h·∫•p d·∫´n c·ªßa ng∆∞·ªùi d√πng v√† gi·ªØ v·ªØng v·ªã th·∫ø c·ªßa m√¨nh trong l√≤ng ng∆∞·ªùi d√πng. Tuy nhi√™n, c·∫£ hai v·∫´n c√≥ ti·ªÅm nƒÉng ƒë·ªÉ ti·∫øp t·ª•c ph√°t tri·ªÉn v√† thay ƒë·ªïi trong t∆∞∆°ng lai. \n",
      "\n",
      "T√¥i hy v·ªçng c√¢u tr·∫£ l·ªùi n√†y s·∫Ω gi√∫p b·∫°n hi·ªÉu r√µ h∆°n v·ªÅ s·ª± kh√°c bi·ªát gi·ªØa Google v√† iPhone. N·∫øu b·∫°n c√≥ th√™m c√¢u h·ªèi, ƒë·ª´ng ng·∫ßn ng·∫°i li√™n h·ªá! (C√¢u tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát)\n"
     ]
    }
   ],
   "source": [
    "QDRANT_URL = \"\"\n",
    "QDRANT_API_KEY = \"\"\n",
    "HUGGINGFACE_API_KEY = \"\"\n",
    "EMBEDDINGS_MODEL_NAME = \"BAAI/bge-m3\"\n",
    "GEMINI_API_KEY = \"\"\n",
    "\n",
    "# Kh·ªüi t·∫°o pipeline RAG\n",
    "rag_setup = RAGPipelineSetup(\n",
    "    qdrant_url=QDRANT_URL,\n",
    "    qdrant_api_key=QDRANT_API_KEY,\n",
    "    hf_api_key=HUGGINGFACE_API_KEY,\n",
    "    hf_model_name=EMBEDDINGS_MODEL_NAME,\n",
    "    gemini_api_key=GEMINI_API_KEY,\n",
    ")\n",
    "\n",
    "# Ch·ªçn ngu·ªìn d·ªØ li·ªáu (collection trong Qdrant)\n",
    "source = \"news_embeddings\"\n",
    "\n",
    "# L·∫•y pipeline RAG t∆∞∆°ng ·ª©ng ngu·ªìn\n",
    "rag_pipeline = rag_setup.rag(source)\n",
    "\n",
    "# C√¢u h·ªèi m·∫´u\n",
    "question = \"Google va iphone\"\n",
    "\n",
    "# Ch·∫°y pipeline ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi\n",
    "result = rag_pipeline.invoke({\"input\": question})\n",
    "\n",
    "print(\"C√¢u h·ªèi:\", question)\n",
    "print(\"Tr·∫£ l·ªùi:\", result['answer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac788f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce GTX 1650 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b65e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n",
      "11.8\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)             # ph·∫£i ch·ª©a cu120\n",
    "print(torch.version.cuda)            # ph·∫£i l√† 12.0\n",
    "print(torch.backends.cudnn.version())  # ph·∫£i l√† s·ªë > 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c1c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"gpt2\"  # b·∫°n c√≥ th·ªÉ thay b·∫±ng \"meta-llama/Llama-2-7b-chat-hf\" n·∫øu mu·ªën m√¥ h√¨nh m·∫°nh h∆°n\n",
    "\n",
    "# C·∫•u h√¨nh 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # ho·∫∑c \"fp4\"\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "# Load tokenizer v√† model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# T·∫°o pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
