{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZtLaTn3B8OH"
      },
      "outputs": [],
      "source": [
        "!pip -q install \\\n",
        "  langfuse \\\n",
        "  qdrant-client \\\n",
        "  sentence-transformers \\\n",
        "  fastembed \\\n",
        "  groq \\\n",
        "  redis \\\n",
        "  langchain\\\n",
        "  langchain-community \\\n",
        "  langchain-google-genai\\\n",
        "  google-generativeai \\\n",
        "  pymongo \\\n",
        "  langchain-redis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVhwRBCICHPS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import time\n",
        "import hashlib\n",
        "\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, List, Dict, Union, Any, Callable, Tuple\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import SparseVector\n",
        "\n",
        "from fastembed import TextEmbedding, SparseTextEmbedding, LateInteractionTextEmbedding\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import langfuse\n",
        "from langfuse import Langfuse\n",
        "from langfuse import observe, get_client\n",
        "from uuid import uuid4\n",
        "\n",
        "from groq import Groq\n",
        "from pymongo import MongoClient\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.genai.types import HttpOptions, GenerateContentConfig\n",
        "from google.genai import Client as GeminiClient\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory.chat_message_histories import RedisChatMessageHistory\n",
        "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "\n",
        "import re\n",
        "import redis\n",
        "import gzip\n",
        "import base64\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Logging setup\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwOuTcH5CIV6"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CacheConfig:\n",
        "    redis_host: str\n",
        "    redis_port: int\n",
        "    redis_password: Optional[str]\n",
        "    redis_db: int\n",
        "    cache_ttl: int\n",
        "    enable_cache: bool\n",
        "\n",
        "@dataclass\n",
        "class SearchConfig:\n",
        "    enable_reranking: bool\n",
        "    enable_query_expansion: bool\n",
        "    rerank_top_k: int\n",
        "    final_top_k: int\n",
        "    query_expansion_count: int\n",
        "    context_overlap_threshold: float\n",
        "\n",
        "@dataclass\n",
        "class RAGConfig:\n",
        "    \"\"\"Configuration for the RAG pipeline, including caching and enhanced search features.\"\"\"\n",
        "\n",
        "    # Qdrant / Vector Database config\n",
        "    qdrant_url: str = ...\n",
        "    qdrant_api_key: Optional[str] = ...\n",
        "    collection_name: str = \"deeplearning_ai_news_embeddings\"\n",
        "    qdrant_top_k: int = 5\n",
        "    score_threshold: float = 0.5  # Use more strict default from EnhancedRAGConfig\n",
        "\n",
        "    # Embedding model\n",
        "    embedding_model_name: str = \"BAAI/bge-m3\"\n",
        "\n",
        "    # Scoring weights\n",
        "    dense_score: float = 0.5\n",
        "    colbert_score: float = 0.3\n",
        "    sparse_score: float = 0.2\n",
        "\n",
        "    # Context\n",
        "    max_context_length: int = 4000  # Use shorter max length from EnhancedRAGConfig\n",
        "\n",
        "    # Gemini LLM config\n",
        "    gemini_api_key: str = ...\n",
        "    gemini_model_name: str = \"gemini-2.5-flash\"\n",
        "    temperature: float = 0.15\n",
        "    top_k: int = 3\n",
        "    top_p: float = 0.85\n",
        "\n",
        "    # Langfuse observability\n",
        "    langfuse_public_key: str = ...\n",
        "    langfuse_secret_key: str = ...\n",
        "    langfuse_host: str = ...\n",
        "\n",
        "    # Lakera Guard\n",
        "    lakera_guard_api_key: Optional[str] = ...\n",
        "\n",
        "    # Redis caching config\n",
        "    redis_host: str = ...\n",
        "    redis_port: int = ...\n",
        "    redis_password: Optional[str] = ...\n",
        "    redis_db: int = 0\n",
        "    cache_ttl: int = 3600\n",
        "    enable_cache: bool = True\n",
        "\n",
        "    # MongoDB Config\n",
        "    mongo_uri: str = ...\n",
        "    mongo_db_name: str = \"deeplearning_ai_news\"\n",
        "\n",
        "    # Search enhancement config\n",
        "    enable_reranking: bool = True\n",
        "    enable_query_expansion: bool = True\n",
        "    rerank_top_k: int = 20\n",
        "    final_top_k: int = 5\n",
        "    query_expansion_count: int = 3\n",
        "    context_overlap_threshold: float = 0.8\n",
        "    reranker_model_name: str = 'multi-qa-MiniLM-L6-cos-v1'\n",
        "\n",
        "    # Converational Chatbot\n",
        "    redis_url: str = f\"redis://:{redis_password}@{redis_host}:{redis_port}\"\n",
        "    session_id: str = \"default_session\"\n",
        "    max_history_pairs: int = 10\n",
        "    max_final_contexts: int = 10\n",
        "    enable_compression: bool = True\n",
        "\n",
        "    def get_redis_url(self) -> str:\n",
        "        \"\"\"Build Redis URL from config.\"\"\"\n",
        "        return f\"redis://:{self.redis_password}@{self.redis_host}:{self.redis_port}\"\n",
        "\n",
        "    def to_cache_config(self) -> CacheConfig:\n",
        "        return CacheConfig(\n",
        "            redis_host=self.redis_host,\n",
        "            redis_port=self.redis_port,\n",
        "            redis_password=self.redis_password,\n",
        "            redis_db=self.redis_db,\n",
        "            cache_ttl=self.cache_ttl,\n",
        "            enable_cache=self.enable_cache\n",
        "        )\n",
        "\n",
        "    def to_search_config(self) -> SearchConfig:\n",
        "        return SearchConfig(\n",
        "            enable_reranking=self.enable_reranking,\n",
        "            enable_query_expansion=self.enable_query_expansion,\n",
        "            rerank_top_k=self.rerank_top_k,\n",
        "            final_top_k=self.final_top_k,\n",
        "            query_expansion_count=self.query_expansion_count,\n",
        "            context_overlap_threshold=self.context_overlap_threshold\n",
        "        )\n",
        "\n",
        "# Initialize config\n",
        "config = RAGConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozm1lm63CNhI"
      },
      "outputs": [],
      "source": [
        "class GeminiLLM:\n",
        "    def __init__(self, model_name: str, api_key: str, llm_config: RAGConfig=None):\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key\n",
        "        self.config = llm_config\n",
        "        self.model = self._setup_model()\n",
        "\n",
        "    def _setup_model(self):\n",
        "        return ChatGoogleGenerativeAI(\n",
        "            model=self.model_name,\n",
        "            google_api_key=self.api_key,\n",
        "            temperature=self.config.temperature,\n",
        "            top_k=self.config.top_k,\n",
        "            top_p=self.config.top_p,\n",
        "        )\n",
        "\n",
        "    def generate(self, query: str, system_prompt: str = None, langfuse_client: Optional[object] = None) -> str:\n",
        "        messages = []\n",
        "        if system_prompt:\n",
        "            messages.append(SystemMessage(content=system_prompt))\n",
        "        messages.append(HumanMessage(content=query))\n",
        "\n",
        "        if langfuse_client:\n",
        "            with langfuse_client.start_as_current_span(\n",
        "                name=\"llm_generation\",\n",
        "                metadata={\"model\": self.model_name}\n",
        "            ) as root_span:\n",
        "\n",
        "                with langfuse_client.start_as_current_span(name=\"llm.generate_content\") as llm_span:\n",
        "                    llm_span.update(input={\"prompt\": query, \"system_prompt\": system_prompt})\n",
        "                    try:\n",
        "                        logger.info(\"Gemini is generating answer...\")\n",
        "                        response = self.model.invoke(messages).content\n",
        "                        llm_span.update(output={\"response\": response}, status_message=\"success\")\n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Error in llm.generate: {e}\")\n",
        "                        llm_span.update(output={\"error\": str(e)}, status_message=\"error\")\n",
        "                        raise\n",
        "\n",
        "                root_span.update(output={\"status\": \"completed\"})\n",
        "                return response\n",
        "        else:\n",
        "            try:\n",
        "                logger.info(\"Gemini is generating answer...\")\n",
        "                response = self.model.invoke(messages).content\n",
        "                return response\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in llm.generate: {e}\")\n",
        "                raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYXscsClCN6n"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"\"\"You are an AI assistant specialized in answering questions related to technology in the fields Machine Learning/ Deep Learning/ Artificial Intelligence.\n",
        "Your task is to generate clear and accurate answers based on the provided **Context** retrieved from relevant documents.\n",
        "\n",
        "Instructions:\n",
        "- Read the \"Question\" carefully and determine user's intent.\n",
        "- If the \"Question\" is written in Vietnamese, translate it into English and treat it as the question to be answered.\n",
        "- Always answer based on the content given after the word **\"Context:\"** and ignore the \"Question\" after processing its language.\n",
        "- If the provided **Context** is too short (for example, fewer than 30 words), politely ask the user to provide more details. Remember use a gentle and respectful Vietnamese tone.\n",
        "- If conflicting info appears in the context, prioritize the most recent or clearly stated.\n",
        "\n",
        "When answering, please focus on:\n",
        "- The most relevant and accurate information from the context that addresses the user's question.\n",
        "- Clarify core concepts or definitions that are necessary for the answer.\n",
        "- Explain any relevant approaches, techniques, applications, or research findings mentioned in the context.\n",
        "- Maintain logical flow and coherence in the answer.\n",
        "\n",
        "Your answer must:\n",
        "- Be written in Vietnamese only.\n",
        "- Be clear, concise, and accessible to a broad Vietnamese audience.\n",
        "- Be strictly faithful to the original context without adding personal opinions or unsupported conclusions.\n",
        "- Contain 4–6 sentences, each under 30 words, unless the context is very short.\n",
        "- Use a professional, friendly, and approachable tone—like how a knowledgeable Vietnamese communicator explains technical topics naturally.\n",
        "\n",
        "To improve coherence:\n",
        "- Use natural connectors like “ngoài ra”, “hơn nữa”, “tuy nhiên”, “bên cạnh đó”, “kết lại” when appropriate.\n",
        "- Present ideas in a logically connected and smooth-flowing paragraph.\n",
        "\n",
        "Avoid:\n",
        "- Including minor or irrelevant details.\n",
        "- Overusing technical jargon unless explained clearly.\n",
        "\n",
        "Formatting requirements:\n",
        "- **Write your answer as a single paragraph.**\n",
        "- **Begin the answer with a polite Vietnamese opening such as**:\n",
        "  - “Dạ, câu trả lời là…”\n",
        "  - “Vâng, theo nội dung thì…”\n",
        "  - “Dạ em xin trả lời như sau…”\n",
        "  - “Sau đây là phần giải đáp ạ…”\n",
        "  - “Dạ, nội dung chính là…”\n",
        "- **Do not translate domain-specific terms like \"summarization\", \"techniques\", \"use cases\", etc.**\n",
        "- **Only output Vietnamese text.**\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXzrBHYfCPCG"
      },
      "outputs": [],
      "source": [
        "class RAGSingleVectorSearch:\n",
        "    \"\"\"Main RAG Pipeline with Single Vector Search\"\"\"\n",
        "    def __init__(self, llm: GeminiLLM, embedding_model: object,\n",
        "                 qdrant_client: QdrantClient, config: RAGConfig):\n",
        "\n",
        "        self.config = config\n",
        "        self.embedding_model = embedding_model\n",
        "        self.llm = llm\n",
        "        self.system_prompt = None\n",
        "        self.qdrant_client = qdrant_client\n",
        "        self.gemini_client = GeminiClient(\n",
        "            api_key=self.config.gemini_api_key,\n",
        "            http_options=HttpOptions(api_version='v1')\n",
        "        )\n",
        "\n",
        "        # Config for search\n",
        "        self.top_k = self.config.qdrant_top_k\n",
        "        self.max_context_length = self.config.max_context_length\n",
        "        self.score_threshold = self.config.score_threshold\n",
        "\n",
        "        logger.info(\"RAG Pipeline with Single Vector Search initialized successfully\")\n",
        "\n",
        "    def _count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Count tokens from text\"\"\"\n",
        "        try:\n",
        "            response = self.gemini_client.models.count_tokens(\n",
        "                model=self.config.gemini_model_name,\n",
        "                contents=text\n",
        "            )\n",
        "            return response.total_tokens\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to count tokens: {e}\")\n",
        "            # Fallback: rough estimation (1 token ≈ 4 characters)\n",
        "            return len(text) // 4\n",
        "\n",
        "    def _load_default_system_prompt(self):\n",
        "        return DEFAULT_SYSTEM_PROMPT\n",
        "\n",
        "    def _encode_query(self, query: str, langfuse_client: Optional[object] = None) -> object:\n",
        "        \"\"\"Encode query with single embedding model\"\"\"\n",
        "        if langfuse_client:\n",
        "            with langfuse_client.start_as_current_span(name=\"rag.encode_query\") as encode_span:\n",
        "                encode_span.update(input={\"query\": query})\n",
        "\n",
        "                try:\n",
        "                    vector = self._do_encode_query(query)\n",
        "                    encode_span.update(\n",
        "                        output={\n",
        "                            \"vector_dim\": len(vector) if hasattr(vector, '__len__') else \"unknown\",\n",
        "                            \"vector_preview\": str(vector[:10]) + \"...\" if hasattr(vector, '__len__') else \"unknown\"\n",
        "                        },\n",
        "                        status_message=\"success\"\n",
        "                    )\n",
        "                    return vector\n",
        "\n",
        "                except Exception as e:\n",
        "                    encode_span.update(status_message=\"error\", output={\"error\": str(e)})\n",
        "                    raise\n",
        "        else:\n",
        "            return self._do_encode_query(query)\n",
        "\n",
        "    def _do_encode_query(self, query: str) -> object:\n",
        "        \"\"\"Actually encode query with embedding model\"\"\"\n",
        "        return self.embedding_model.encode(query)\n",
        "\n",
        "    def _search_vector(self, vector, limit: int = None, langfuse_client: Optional[object] = None) -> object:\n",
        "        \"\"\"Search with single vector embedding\"\"\"\n",
        "        if limit is None:\n",
        "            limit = self.top_k\n",
        "\n",
        "        if langfuse_client:\n",
        "            with langfuse_client.start_as_current_span(name=\"rag.search_vector\") as search_span:\n",
        "                try:\n",
        "                    results = self._do_search_vector(vector, limit)\n",
        "                    results_review = []\n",
        "                    for pt in results.points[:limit]:\n",
        "                        results_review.append({\n",
        "                            \"id\": pt.id,\n",
        "                            \"score\": round(pt.score, 4),\n",
        "                            \"text_preview\": pt.payload.get(\"page_content\", \"\")[:100]  # short preview\n",
        "                        })\n",
        "                    search_span.update(\n",
        "                        input={\n",
        "                            \"limit\": limit,\n",
        "                            \"score_threshold\": self.score_threshold\n",
        "                        },\n",
        "                        output={\n",
        "                            \"result_count\": len(results.points),\n",
        "                            \"results_preview\": results_review\n",
        "                        },\n",
        "                        status_message=\"success\"\n",
        "                    )\n",
        "                    return results\n",
        "\n",
        "                except Exception as e:\n",
        "                    search_span.update(status_message=\"error\", output={\"error\": str(e)})\n",
        "                    raise\n",
        "        else:\n",
        "            return self._do_search_vector(vector, limit)\n",
        "\n",
        "    def _do_search_vector(self, vector, limit: int) -> object:\n",
        "        \"\"\"Actually perform vector search\"\"\"\n",
        "        return self.qdrant_client.query_points(\n",
        "            collection_name=self.config.collection_name,\n",
        "            query=vector,\n",
        "            limit=limit,\n",
        "            with_payload=True,\n",
        "            score_threshold=self.score_threshold\n",
        "        )\n",
        "\n",
        "    def _search_context(self, query: str, langfuse_client: Optional[object] = None) -> List[str]:\n",
        "        \"\"\"Single vector search context from Qdrant\"\"\"\n",
        "        if langfuse_client:\n",
        "            with langfuse_client.start_as_current_span(name=\"rag.search_context\") as search_span:\n",
        "                search_span.update(input={\"query\": query})\n",
        "\n",
        "                try:\n",
        "                    context_texts = self._do_search_context(query, langfuse_client)\n",
        "                    search_span.update(\n",
        "                        output={\n",
        "                            \"final_context_count\": len(context_texts),\n",
        "                            \"score_threshold\": self.score_threshold\n",
        "                        },\n",
        "                        status_message=\"success\"\n",
        "                    )\n",
        "                    return context_texts\n",
        "\n",
        "                except Exception as e:\n",
        "                    search_span.update(status_message=\"error\", output={\"error\": str(e)})\n",
        "                    raise\n",
        "        else:\n",
        "            return self._do_search_context(query, langfuse_client)\n",
        "\n",
        "    def _do_search_context(self, query: str, langfuse_client: Optional[object] = None) -> List[str]:\n",
        "        \"\"\"Actually perform single vector search\"\"\"\n",
        "        # Encode query with single embedding model\n",
        "        query_vector = self._encode_query(query, langfuse_client)\n",
        "\n",
        "        # Search with single vector\n",
        "        search_results = self._search_vector(query_vector, limit=self.top_k, langfuse_client=langfuse_client)\n",
        "\n",
        "        # Extract text from filtered results\n",
        "        texts = []\n",
        "        for result in search_results.points:\n",
        "            page_content = result.payload.get(\"page_content\")\n",
        "            if page_content:  # Check if content exists\n",
        "                texts.append(page_content.strip())\n",
        "        return texts\n",
        "\n",
        "    def _build_context(self, search_results: List[str]) -> str:\n",
        "        \"\"\"Build context from search results\"\"\"\n",
        "        if not search_results:\n",
        "            return \"\"\n",
        "\n",
        "        full_context = \"\"\n",
        "        for idx, result in enumerate(search_results):\n",
        "            full_context += f\"Context {idx+1}: {result}\\n\"\n",
        "        return full_context[:self.max_context_length]\n",
        "\n",
        "    def generate_response(self, query: str, context: str, system_prompt: str, langfuse_client: Optional[object] = None) -> str:\n",
        "        \"\"\"Generate response using LLM with context and system prompt\"\"\"\n",
        "        # Create Prompt\n",
        "        final_system_prompt = f\"\"\"{system_prompt.strip()}\n",
        "--- Here are the contexts ---:\n",
        "{context}\n",
        "\"\"\"\n",
        "        response = self.llm.generate(query, final_system_prompt)\n",
        "\n",
        "        # Count tokens\n",
        "        query_tokens = self._count_tokens(query)\n",
        "        prompt_tokens = self._count_tokens(final_system_prompt)\n",
        "        completion_tokens = self._count_tokens(response)\n",
        "        total_tokens = prompt_tokens + completion_tokens\n",
        "\n",
        "        # Langfuse trace\n",
        "        if langfuse_client:\n",
        "            with langfuse_client.start_as_current_generation(\n",
        "                name=\"rag.llm_generate\",\n",
        "                model=getattr(self.llm, \"model_name\", \"unknown-model\")\n",
        "            ) as generation:\n",
        "                generation.update(\n",
        "                    input={\"query\": query, \"system_prompt\": final_system_prompt},\n",
        "                    output={\"response\": response},\n",
        "                    usage_details={\n",
        "                        \"query_tokens\": query_tokens,\n",
        "                        \"prompt_tokens\": prompt_tokens,\n",
        "                        \"completion_tokens\": completion_tokens,\n",
        "                        \"total_tokens\": total_tokens,\n",
        "                    },\n",
        "                )\n",
        "\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY4YocVWCQOA"
      },
      "outputs": [],
      "source": [
        "class HistoryGuidedReranker:\n",
        "    def __init__(self, embedding_model, similarity_weight=0.7, history_weight=0.3, max_final_contexts=10):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.similarity_weight = similarity_weight\n",
        "        self.history_weight = history_weight\n",
        "        self.max_final_contexts = max_final_contexts\n",
        "\n",
        "    def _normalize(self, vector):\n",
        "        norm = np.linalg.norm(vector)\n",
        "        return vector / norm if norm > 0 else vector\n",
        "\n",
        "    def _compute_cosine_similarity(self, vec1, vec2):\n",
        "        return float(np.dot(self._normalize(vec1), self._normalize(vec2)))\n",
        "\n",
        "    def _compute_history_relevance(self, context_embedding, history_embeddings):\n",
        "        if history_embeddings is None or len(history_embeddings) == 0:\n",
        "            return 0.0\n",
        "        similarities = [\n",
        "            self._compute_cosine_similarity(context_embedding, hist_emb)\n",
        "            for hist_emb in history_embeddings\n",
        "        ]\n",
        "        weights = [1.0 / (i + 1) for i in range(len(similarities))]  # recency weighting\n",
        "        weighted_sum = sum(sim * w for sim, w in zip(similarities, weights))\n",
        "        return weighted_sum / sum(weights)\n",
        "\n",
        "    def rerank(self, query, contexts, history_texts=None):\n",
        "        query_embedding = self.embedding_model.encode(query)\n",
        "        history_embeddings = self.embedding_model.encode(history_texts) if history_texts else []\n",
        "\n",
        "        results = []\n",
        "        for context in contexts:\n",
        "            ctx_emb = self.embedding_model.encode(context)\n",
        "            sim = self._compute_cosine_similarity(query_embedding, ctx_emb)\n",
        "            hist_rel = self._compute_history_relevance(ctx_emb, history_embeddings)\n",
        "            final_score = self.similarity_weight * sim + self.history_weight * hist_rel\n",
        "            results.append({\n",
        "                \"context\": context,\n",
        "                \"query_similarity\": round(sim, 4),\n",
        "                \"history_relevance\": round(hist_rel, 4),\n",
        "                \"final_score\": round(final_score, 4)\n",
        "            })\n",
        "        return sorted(results, key=lambda x: x[\"final_score\"], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCD8pCgPCRen"
      },
      "outputs": [],
      "source": [
        "class SemanticPromptCache:\n",
        "    def __init__(self, redis_url: str, embedding_model: SentenceTransformer):\n",
        "        self.redis = redis.from_url(redis_url, decode_responses=True)\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def _cosine_similarity(self, a, b):\n",
        "        return np.dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "    def get_cached_response(self, prompt: str, similarity_threshold: float = 0.9) -> Optional[str]:\n",
        "        new_embedding = self.embedding_model.encode(prompt)\n",
        "\n",
        "        # Get prompt\n",
        "        all_cache = self.redis.lrange(\"semantic_cache\", 0, -1)\n",
        "        for item in all_cache:\n",
        "            data = json.loads(item)\n",
        "            cached_embedding = np.array(data[\"embedding\"])\n",
        "            sim = self._cosine_similarity(new_embedding, cached_embedding)\n",
        "\n",
        "            if sim >= similarity_threshold:\n",
        "                return data[\"response\"]\n",
        "        return None\n",
        "\n",
        "    def cache_response(self, prompt: str, response: str):\n",
        "        embedding = self.embedding_model.encode(prompt)\n",
        "        cache_entry = {\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response,\n",
        "            \"embedding\": embedding.tolist()\n",
        "        }\n",
        "        self.redis.rpush(\"semantic_cache\", json.dumps(cache_entry))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wP-pPfNCTCb"
      },
      "outputs": [],
      "source": [
        "class ConversationalRAGChatbot:\n",
        "    \"\"\"\n",
        "    Enhanced Conversational RAG Chatbot with History-Guided Reranking\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rag_pipeline,\n",
        "        config: RAGConfig,\n",
        "        session_id: str = \"default_session\",\n",
        "        reranker: Optional[object] = None,\n",
        "        langfuse_client: Optional[object] = None,\n",
        "        semantic_cache: Optional[object] = None\n",
        "    ):\n",
        "        self.rag_pipeline = rag_pipeline\n",
        "        self.config = config\n",
        "        self.session_id = session_id\n",
        "        self.max_history_pairs = config.max_history_pairs\n",
        "        self.max_final_contexts = config.max_final_contexts\n",
        "        self.enable_compression = config.enable_compression\n",
        "        self.langfuse_client = langfuse_client\n",
        "\n",
        "        # Chat history\n",
        "        self.history = RedisChatMessageHistory(\n",
        "            url=self.config.redis_url,\n",
        "            session_id=self.session_id,\n",
        "            ttl=1800\n",
        "        )\n",
        "\n",
        "        # Semantic caching\n",
        "        self.semantic_cache = semantic_cache\n",
        "\n",
        "        # Reranker\n",
        "        self.reranker = reranker\n",
        "\n",
        "        # Security check\n",
        "        self.lakera_session = requests.Session()\n",
        "        self.lakera_guard_api_key = config.lakera_guard_api_key\n",
        "\n",
        "        # Dev\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    # def _generate_session_hash(self, session_id: str) -> str:\n",
        "    #     return hashlib.md5(session_id.encode()).hexdigest()\n",
        "\n",
        "    def _compress_data(self, data: str) -> str:\n",
        "        if not self.enable_compression:\n",
        "            return data\n",
        "        try:\n",
        "            compressed = gzip.compress(data.encode('utf-8'))\n",
        "            return base64.b64encode(compressed).decode('utf-8')\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Compression failed: {e}\")\n",
        "            return data\n",
        "\n",
        "    def _decompress_data(self, data: str) -> str:\n",
        "        if not self.enable_compression or not self._is_base64_gzip(data):\n",
        "            return data\n",
        "        try:\n",
        "            decoded = base64.b64decode(data.encode('utf-8'))\n",
        "            return gzip.decompress(decoded).decode('utf-8')\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Decompression failed: {e}\")\n",
        "            return data\n",
        "\n",
        "    def _check_prompt_injection(self, prompt: str, root_span: Optional[object] = None) -> Optional[str]:\n",
        "        \"\"\"Check prompt for injection using Lakera Guard API and log via Langfuse\"\"\"\n",
        "        try:\n",
        "            response = self.lakera_session.post(\n",
        "                \"https://api.lakera.ai/v2/guard\",\n",
        "                json={\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "                headers={\"Authorization\": f\"Bearer {self.config.lakera_guard_api_key}\"},\n",
        "                timeout=10  # Add timeout to prevent hanging\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            response_json = response.json()\n",
        "            results = response_json.get(\"results\", [])\n",
        "            flagged = any(result.get(\"flagged\", False) for result in results)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in LakeraGuard: {e}\")\n",
        "            # Don't block on Lakera Guard failure, just log and continue\n",
        "            return None\n",
        "\n",
        "        output_msg = \"[PROMPT INJECTION] Lakera Guard identified a prompt injection. No user was harmed by this LLM.\"\n",
        "\n",
        "        if self.langfuse_client:\n",
        "            with self.langfuse_client.start_as_current_span(name=\"prompt_injection_check\") as prompt_injection_span:\n",
        "                prompt_injection_span.update(input={\"prompt\": prompt})\n",
        "                if flagged:\n",
        "                    prompt_injection_span.update(\n",
        "                        output={\n",
        "                            \"lakera_output\": output_msg,\n",
        "                            \"lakera_response\": response_json\n",
        "                        },\n",
        "                        status_message=\"warning\"\n",
        "                    )\n",
        "                    if root_span:\n",
        "                        root_span.update(\n",
        "                            output={\"final_response\": output_msg},\n",
        "                            status_message=\"warning\"\n",
        "                        )\n",
        "                    return output_msg\n",
        "                else:\n",
        "                    prompt_injection_span.update(\n",
        "                        output={\n",
        "                            \"lakera_output\": \"No prompt injection detected.\",\n",
        "                            \"lakera_response\": response_json\n",
        "                        },\n",
        "                        status_message=\"success\"\n",
        "                    )\n",
        "        else:\n",
        "            if flagged:\n",
        "                return output_msg\n",
        "        return None\n",
        "\n",
        "    def _load_default_system_prompt(self):\n",
        "        return self.rag_pipeline._load_default_system_prompt()\n",
        "\n",
        "    def _validate_inputs(self, query: str) -> Tuple[bool, str]:\n",
        "        if not query or not query.strip():\n",
        "            return False, \"Query cannot be empty\"\n",
        "        if len(query.strip()) > 10000:\n",
        "            return False, \"Query is too long (max 10000 characters)\"\n",
        "        return True, \"\"\n",
        "\n",
        "    def _is_base64_gzip(self, data: str) -> bool:\n",
        "        try:\n",
        "            decoded = base64.b64decode(data.encode('utf-8'))\n",
        "            return decoded[:2] == b'\\x1f\\x8b'  # Magic bytes of gzip\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "    def _trim_history(self) -> None:\n",
        "        try:\n",
        "            messages = self.history.messages\n",
        "            if len(messages) <= 2 * self.max_history_pairs:\n",
        "                return\n",
        "\n",
        "            messages_to_keep = messages[-2 * self.max_history_pairs:]\n",
        "            self.history.clear()\n",
        "\n",
        "            for message in messages_to_keep:\n",
        "                content = message.content\n",
        "\n",
        "                if self.enable_compression and not self._is_base64_gzip(content):\n",
        "                    content = self._compress_data(content)\n",
        "\n",
        "                if isinstance(message, HumanMessage):\n",
        "                    self.history.add_user_message(content)\n",
        "                elif isinstance(message, AIMessage):\n",
        "                    self.history.add_ai_message(content)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error trimming history: {e}\")\n",
        "\n",
        "    def _get_history_texts(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract and format history texts as a list of strings like:\n",
        "        [\"User: ...\", \"Assistant: ...\"]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            messages = self.history.messages\n",
        "            formatted_texts = []\n",
        "\n",
        "            for message in messages:\n",
        "                content = message.content\n",
        "                if self.enable_compression:\n",
        "                    content = self._decompress_data(content)\n",
        "\n",
        "                if isinstance(message, HumanMessage):\n",
        "                    formatted_texts.append(f\"User: {content}\")\n",
        "                elif isinstance(message, AIMessage):\n",
        "                    formatted_texts.append(f\"Assistant: {content}\")\n",
        "\n",
        "            return formatted_texts\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error formatting history texts: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _enhanced_search_with_reranking(self, query: str, root_span: Optional[object] = None) -> List[str]:\n",
        "        \"\"\"\n",
        "        Enhanced search with reranking using history context\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            root_span: Optional Langfuse span for tracing\n",
        "\n",
        "        Returns:\n",
        "            List of reranked context strings\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if root_span and self.langfuse_client:\n",
        "                with self.langfuse_client.start_as_current_span(\n",
        "                    name=\"rag.enhanced_search_with_reranking\"\n",
        "                ) as search_span:\n",
        "                    search_span.update(input={\"query\": query})\n",
        "\n",
        "                    # Get initial search results from RAG pipeline\n",
        "                    initial_contexts = self.rag_pipeline._do_search_context(query, self.langfuse_client)\n",
        "\n",
        "                    # Apply reranking if reranker is available\n",
        "                    if self.reranker and initial_contexts is not None and len(initial_contexts) > 0:\n",
        "                        # Get history texts for reranking\n",
        "                        history_texts = self._get_history_texts()\n",
        "\n",
        "                        with self.langfuse_client.start_as_current_span(\n",
        "                            name=\"conversational_rag.rerank_contexts\"\n",
        "                        ) as rerank_span:\n",
        "                            # Rerank contexts\n",
        "                            reranked_results = self.reranker.rerank(\n",
        "                                query=query,\n",
        "                                contexts=initial_contexts,\n",
        "                                history_texts=history_texts\n",
        "                            )\n",
        "\n",
        "                            # Extract contexts from reranked results\n",
        "                            final_contexts = [result[\"context\"] for result in reranked_results]\n",
        "                            rerank_span.update(\n",
        "                                input={\n",
        "                                    'initial_context': initial_contexts,\n",
        "                                    'history_texts': history_texts\n",
        "                                },\n",
        "                                output={\n",
        "                                    'final_context': final_contexts,\n",
        "                                    'full_rerank_results': reranked_results\n",
        "\n",
        "                                },\n",
        "                                status_message='success'\n",
        "                            )\n",
        "\n",
        "                        search_span.update(\n",
        "                            output={\n",
        "                                \"initial_context_count\": len(initial_contexts),\n",
        "                                \"final_context_count\": len(final_contexts),\n",
        "                                \"reranker_used\": True,\n",
        "                                \"history_texts_count\": len(history_texts)\n",
        "                            },\n",
        "                            status_message=\"success\"\n",
        "                        )\n",
        "\n",
        "                        self.logger.debug(f\"Reranked {len(initial_contexts)} contexts to {len(final_contexts)}\")\n",
        "                        return final_contexts\n",
        "                    else:\n",
        "                        # Return original contexts if no reranker\n",
        "                        search_span.update(\n",
        "                            output={\n",
        "                                \"context_count\": len(initial_contexts),\n",
        "                                \"reranker_used\": False\n",
        "                            },\n",
        "                            status_message=\"success\"\n",
        "                        )\n",
        "                        return initial_contexts\n",
        "\n",
        "            else:\n",
        "                # No tracing - direct execution\n",
        "                # Get initial search results from RAG pipeline\n",
        "                initial_contexts = self.rag_pipeline._do_search_context(query)\n",
        "\n",
        "                # Apply reranking if reranker is available\n",
        "                if self.reranker and initial_contexts:\n",
        "                    # Get history texts for reranking\n",
        "                    history_texts = self._get_history_texts()\n",
        "\n",
        "                    # Rerank contexts\n",
        "                    reranked_results = self.reranker.rerank(\n",
        "                        query=query,\n",
        "                        contexts=initial_contexts,\n",
        "                        history_texts=history_texts\n",
        "                    )\n",
        "\n",
        "                    # Extract contexts from reranked results\n",
        "                    final_contexts = [result[\"context\"] for result in reranked_results]\n",
        "\n",
        "                    self.logger.debug(f\"Reranked {len(initial_contexts)} contexts to {len(final_contexts)}\")\n",
        "                    return final_contexts\n",
        "                else:\n",
        "                    # Return original contexts if no reranker\n",
        "                    return initial_contexts\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in enhanced search with reranking: {e}\")\n",
        "            # Fallback to basic search\n",
        "            try:\n",
        "                return self.rag_pipeline._do_search_context(query)\n",
        "            except Exception as fallback_e:\n",
        "                self.logger.error(f\"Fallback search also failed: {fallback_e}\")\n",
        "                return []\n",
        "\n",
        "    def chat(self, query: str, system_prompt: Optional[str] = None) -> str:\n",
        "        is_valid, error_msg = self._validate_inputs(query)\n",
        "        if not is_valid:\n",
        "            self.logger.warning(f\"Invalid input: {error_msg}\")\n",
        "            return f\"Error: {error_msg}\"\n",
        "\n",
        "        if self.langfuse_client:\n",
        "            with self.langfuse_client.start_as_current_span(name=f\"{self.session_id}\") as root_span:\n",
        "                return self._do_chat(query, system_prompt, root_span)\n",
        "        else:\n",
        "            return self._do_chat(query, system_prompt)\n",
        "\n",
        "    def _build_query(self, query: str, history_lines: List[str]) -> str:\n",
        "        \"\"\"\n",
        "        Builds final query prompt including history and user question.\n",
        "        `history_lines` is a list of formatted strings: [\"User: ...\", \"Assistant: ...\"]\n",
        "        \"\"\"\n",
        "        prompt_parts = []\n",
        "        if history_lines:\n",
        "            trimmed_history = history_lines[-2 * self.max_history_pairs:]\n",
        "            history_section = \"\\n\".join(trimmed_history)\n",
        "            prompt_parts.append(\"--- Conversation History ---\\n\" + history_section)\n",
        "\n",
        "        prompt_parts.append(\"--- Current User Question ---\\n\" + query.strip())\n",
        "        return \"\\n\\n\".join(prompt_parts)\n",
        "\n",
        "    def _do_chat(self, query: str, system_prompt: Optional[str] = None,\n",
        "                 root_span: Optional[object] = None) -> str:\n",
        "        try:\n",
        "            # Prompt injection check\n",
        "            flagged = self._check_prompt_injection(query, root_span)\n",
        "            if flagged:\n",
        "                if root_span:\n",
        "                    with self.langfuse_client.start_as_current_span(\n",
        "                        name=\"prompt_injection_check.flagged\"\n",
        "                    ) as flagged_span:\n",
        "                        flagged_span.update(\n",
        "                            input={\"prompt\": query},\n",
        "                            output={\"flagged_response\": flagged},\n",
        "                            status_message='warning'\n",
        "                        )\n",
        "\n",
        "                        root_span.update(\n",
        "                            input={\"query\": query},\n",
        "                            output={\"final_response\": flagged},\n",
        "                            status_message=\"warning\",\n",
        "                            tags={\"prompt_injection\": \"flagged\"}\n",
        "                        )\n",
        "                return flagged\n",
        "\n",
        "            # Search cache prompts\n",
        "            cached_response = self.semantic_cache.get_cached_response(query)\n",
        "            if cached_response:\n",
        "                if root_span:\n",
        "                    with self.langfuse_client.start_as_current_span(\n",
        "                        name=\"semantic_cache.hit\"\n",
        "                    ) as cache_hit_span:\n",
        "                        cache_hit_span.update(\n",
        "                            input={\"query\": query},\n",
        "                            output={\"cached_response\": cached_response},\n",
        "                            status_message=\"success\"\n",
        "                        )\n",
        "\n",
        "                    root_span.update(\n",
        "                        input={\"query\": query},\n",
        "                        output={\"final_response\": cached_response},\n",
        "                        status_message=\"success\",\n",
        "                        tags={\"semantic_cache\": \"hit\"}\n",
        "                    )\n",
        "                return cached_response\n",
        "\n",
        "            # Compress query\n",
        "            compressed_query = self._compress_data(query) if self.enable_compression else query\n",
        "            self.history.add_user_message(compressed_query)\n",
        "\n",
        "            # System promt + Rerank contexts\n",
        "            final_contexts = self.rag_pipeline._build_context(self._enhanced_search_with_reranking(query, root_span))\n",
        "            self.system_prompt = system_prompt or self._load_default_system_prompt()\n",
        "\n",
        "            # Query + History chat\n",
        "            final_query = self._build_query(query, self._get_history_texts())\n",
        "\n",
        "            # Get response\n",
        "            response = self.rag_pipeline.generate_response(final_query, final_contexts, self.system_prompt, self.langfuse_client)\n",
        "\n",
        "            compressed_response = self._compress_data(response) if self.enable_compression else response\n",
        "            self.history.add_ai_message(compressed_response)\n",
        "            self._trim_history()\n",
        "\n",
        "            if root_span:\n",
        "                root_span.update(\n",
        "                    input={\"query\": query, \"context\": final_contexts},\n",
        "                    output={\"final_response\": response},\n",
        "                    status_message=\"success\",\n",
        "                )\n",
        "\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            error_msg = f\"An error occurred while processing your request: {str(e)}\"\n",
        "            self.logger.error(error_msg, exc_info=True)\n",
        "            try:\n",
        "                compressed_error = self._compress_data(error_msg) if self.enable_compression else error_msg\n",
        "                self.history.add_ai_message(compressed_error)\n",
        "            except:\n",
        "                pass\n",
        "            if root_span:\n",
        "                root_span.update(status_message=\"error\", output={\"error\": str(e)})\n",
        "            return error_msg\n",
        "\n",
        "    def get_conversation_history(self) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            messages = self.history.messages\n",
        "            return [{\n",
        "                \"type\": \"human\" if isinstance(msg, HumanMessage) else \"ai\",\n",
        "                \"content\": self._decompress_data(msg.content) if self.enable_compression else msg.content,\n",
        "                \"timestamp\": getattr(msg, 'timestamp', None)\n",
        "            } for msg in messages]\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error getting conversation history: {e}\")\n",
        "            return []\n",
        "\n",
        "    def clear_history(self) -> bool:\n",
        "        try:\n",
        "            self.history.clear()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error clearing history: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_session_stats(self) -> Dict[str, Any]:\n",
        "        try:\n",
        "            return {\n",
        "                \"session_id\": self.session_id,\n",
        "                \"total_messages\": len(self.history.messages),\n",
        "                \"compression_enabled\": self.enable_compression,\n",
        "                \"reranker_available\": self.reranker is not None,\n",
        "                \"max_history_pairs\": self.max_history_pairs\n",
        "            }\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error getting session stats: {e}\")\n",
        "            return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a309tOuCCVPW"
      },
      "outputs": [],
      "source": [
        "print(f'Loading reranker model {config.reranker_model_name}...')\n",
        "reranker_model = SentenceTransformer(config.reranker_model_name)\n",
        "print(reranker_model)\n",
        "print(f'Loading embedding {config.embedding_model_name}...')\n",
        "embedding_model = SentenceTransformer(config.embedding_model_name)\n",
        "print(embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viiGbWAdCXWC"
      },
      "outputs": [],
      "source": [
        "# Config Langfuse\n",
        "langfuse_client = Langfuse(\n",
        "    public_key=config.langfuse_public_key,\n",
        "    secret_key=config.langfuse_secret_key,\n",
        "    host=config.langfuse_host\n",
        ")\n",
        "\n",
        "# Initialize LLM\n",
        "gemini = GeminiLLM(\n",
        "    model_name=config.gemini_model_name,\n",
        "    api_key=config.gemini_api_key,\n",
        "    llm_config=config\n",
        ")\n",
        "\n",
        "# Config Qdrant database\n",
        "qdrant_client = QdrantClient(\n",
        "    url=config.qdrant_url,\n",
        "    api_key=config.qdrant_api_key\n",
        ")\n",
        "\n",
        "# Config main RAG pipeline\n",
        "rag_pipeline = RAGSingleVectorSearch(\n",
        "    llm=gemini,\n",
        "    embedding_model=embedding_model,\n",
        "    qdrant_client=qdrant_client,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Config reranker\n",
        "reranker = HistoryGuidedReranker(\n",
        "    embedding_model=reranker_model,\n",
        "    similarity_weight=0.7,\n",
        "    history_weight=0.3,\n",
        "    max_final_contexts=5,\n",
        ")\n",
        "\n",
        "# Semantic cache\n",
        "semantic_cache = SemanticPromptCache(\n",
        "    redis_url=config.redis_url,\n",
        "    embedding_model=embedding_model\n",
        ")\n",
        "\n",
        "# Test chatbot\n",
        "chatbot = ConversationalRAGChatbot(\n",
        "    rag_pipeline=rag_pipeline,\n",
        "    reranker=reranker,\n",
        "    langfuse_client=langfuse_client,\n",
        "    config=config,\n",
        "    session_id=\"thuan phat\",\n",
        "    semantic_cache=semantic_cache\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3Cx1f_1irQV",
        "outputId": "1d4a4350-4d9c-4320-b0e3-38862ea92e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Hello\n",
            "Assistant:  Xin chào bạn! Tôi là chatbot assistant của trang The Doms, tôi có thể giúp gì cho bạn?\n",
            "\n",
            "\n",
            "\n",
            "User: Hãy nêu một số điểm mới của LLMs DeepSeekR1 so với các LLMs khác hiện tại như GPT hay Gemini\n",
            "Assistant:  Dạ, nội dung chính là: DeepSeek đã nâng cấp LLM DeepSeek-R1, đạt hiệu suất cạnh tranh với OpenAI o3 và Google Gemini 2.5 Pro. Bản cập nhật DeepSeek-R1-0528 cải thiện đáng kể ở các nhiệm vụ toán học, lập trình và logic. DeepSeek-R1-0528-Qwen3-8B có kích thước nhỏ hơn, có thể chạy trên một GPU duy nhất với VRAM chỉ 40GB. DeepSeek tuyên bố cải thiện khả năng suy luận, quản lý các tác vụ phức tạp, viết và chỉnh sửa văn bản dài, đồng thời giảm 50% ảo giác khi viết lại và tóm tắt. Mã nguồn và trọng số của DeepSeek-R1 được cấp phép tự do cho mục đích thương mại và cá nhân.\n",
            "\n",
            "\n",
            "\n",
            "User: Đạt hiệu suất cạnh tranh như thế nào? Có số liệu thực tế không?\n",
            "Assistant:  Dạ, theo nội dung thì GPT-3 chỉ đạt độ chính xác 68% khi trả lời các câu hỏi đố vui trong one-shot TriviaQA, trong khi mô hình mới đạt 75%, một kết quả hiện đại. Ngoài ra, việc tăng hiệu quả tính toán có nghĩa là chi phí năng lượng thấp hơn, giúp các kỹ sư dễ dàng đào tạo các mô hình hiện đại hơn.\n",
            "\n",
            "\n",
            "\n",
            "User: bye\n",
            "Assistant: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Test chat\n",
        "while True:\n",
        "  try:\n",
        "      query = input(\"User: \")\n",
        "      if query.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "          print(\"Assistant: Goodbye!\")\n",
        "          break\n",
        "      else:\n",
        "          print(\"Assistant: \", chatbot.chat(query))\n",
        "          print('\\n\\n')\n",
        "  except Exception as e:\n",
        "      raise e"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
