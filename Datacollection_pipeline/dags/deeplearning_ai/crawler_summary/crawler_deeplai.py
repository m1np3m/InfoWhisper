import asyncio
import re
import json
import os
from urllib.parse import urljoin, urlparse
from datetime import datetime
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from bs4 import BeautifulSoup
import nest_asyncio
from pymongo import MongoClient
from dotenv import load_dotenv
import time
import logging
import pytz
from dateutil import parser
from deeplearning_ai.crawler_summary.summary_gemini import ArticleSummarizer

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables from .env file
load_dotenv()
nest_asyncio.apply()

class DeepLearningAICrawler:
    def __init__(self, base_url="https://www.deeplearning.ai/the-batch/tag/research/",
                 category_name="research", max_articles=50, min_articles_threshold=30,
                 mongo_uri=None, db_name=None, max_load_more_clicks=10, lf=None, handler =None):
        self.base_url = base_url
        self.category_name = category_name
        self.max_articles = max_articles
        self.min_articles_threshold = min_articles_threshold  # S·ªë b√†i t·ªëi thi·ªÉu ƒë·ªÉ wait_for
        self.max_load_more_clicks = max_load_more_clicks  # S·ªë l·∫ßn click Load More t·ªëi ƒëa
        self.articles = []
        self.mongo_uri = mongo_uri or os.getenv('MONGO_URI','mongodb+srv://vinhthuanly210:Vinhthuanly123@cluster0.mznyroo.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0')
        self.db_name = db_name or os.getenv('MONGO_DB_NAME_DEEP', 'deeplearning_ai_news')
        self.mongo_client = None
        self.db = None
        self.lf= lf
        self.handler = handler
        self._connect_mongodb()
        self.summarizer = ArticleSummarizer(lf=self.lf,handler=self.handler)

    def _connect_mongodb(self):
        """Thi·∫øt l·∫≠p k·∫øt n·ªëi MongoDB"""
        try:
            if self.mongo_uri:
                self.mongo_client = MongoClient(self.mongo_uri)
                self.db = self.mongo_client[self.db_name]
                logger.info(f"ƒê√£ k·∫øt n·ªëi th√†nh c√¥ng v·ªõi MongoDB: {self.db_name}")
        except Exception as e:
            logger.error(f"L·ªói k·∫øt n·ªëi MongoDB: {str(e)}")
            self.mongo_client = None
            self.db = None

    def _close_mongodb(self):
        """ƒê√≥ng k·∫øt n·ªëi MongoDB"""
        if self.mongo_client is not None:
            self.mongo_client.close()
            self.mongo_client = None
            self.db = None

    async def get_article_links_with_load_more(self):
        """Crawl trang v·ªõi t·ª± ƒë·ªông nh·∫•n Load More ƒë·ªÉ l·∫•y t·∫•t c·∫£ b√†i vi·∫øt"""
        
        # JavaScript ƒë∆∞·ª£c t·ªëi ∆∞u d·ª±a tr√™n code ho·∫°t ƒë·ªông
        js_code = f"""
        (async () => {{
            const maxClicks = {self.max_load_more_clicks};
            let clickCount = 0;

            function getLoadMoreButton() {{
                // Th·ª≠ selector ch√≠nh x√°c ƒë·∫ßu ti√™n
                let btn = document.querySelector('div[class*="buttons_secondary"][class*="text-center"]');
                return btn;
            }}

            function countValidArticles() {{
                const links = Array.from(document.querySelectorAll('a[href*="/the-batch/"]'));
                const valid = links.filter(a =>
                    !a.href.includes('/tag/') &&
                    !a.href.includes('/category/') &&
                    !a.href.includes('/author/') &&
                    !a.href.includes('/page/') &&
                    !a.href.endsWith('/the-batch/') &&
                    !a.href.endsWith('/the-batch') &&
                    !a.href.includes('#') &&
                    !a.href.includes('mailto:') &&
                    !a.href.includes('tel:')
                );
                return valid.length;
            }}

            console.log('üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh load b√†i vi·∫øt...');
            let initialCount = countValidArticles();
            console.log(`üìä S·ªë b√†i vi·∫øt ban ƒë·∫ßu: ${{initialCount}}`);

            for (let i = 0; i < maxClicks; i++) {{
                const btn = getLoadMoreButton();
                
                if (!btn || btn.offsetParent === null) {{
                    console.log(`‚ùå Kh√¥ng t√¨m th·∫•y n√∫t Load More sau ${{i}} l·∫ßn click`);
                    break;
                }}

                // Scroll ƒë·∫øn n√∫t v√† click
                btn.scrollIntoView({{ behavior: 'smooth', block: 'center' }});
                
                try {{
                    btn.click();
                    clickCount++;
                    console.log(`‚úÖ Clicked Load More l·∫ßn th·ª© ${{clickCount}}`);
                }} catch (e) {{
                    console.log("‚ùå L·ªói khi click:", e);
                    break;
                }}

                // Scroll xu·ªëng m·ªôt ch√∫t ƒë·ªÉ trigger loading
                window.scrollBy(0, 500);
                
                // ƒê·ª£i content load
                await new Promise(r => setTimeout(r, 500));
                
                const currentCount = countValidArticles();
                console.log(`üìä S·ªë b√†i vi·∫øt hi·ªán t·∫°i: ${{currentCount}}`);
                
                // N·∫øu kh√¥ng tƒÉng th√™m b√†i vi·∫øt, c√≥ th·ªÉ ƒë√£ h·∫øt
                if (currentCount === initialCount && i > 0) {{
                    console.log('‚ö†Ô∏è Kh√¥ng c√≥ b√†i vi·∫øt m·ªõi, c√≥ th·ªÉ ƒë√£ load h·∫øt');
                    break;
                }}
                
                initialCount = currentCount;
            }}

            const finalCount = countValidArticles();
            console.log(`üèÜ Ho√†n th√†nh! T·ªïng s·ªë b√†i vi·∫øt: ${{finalCount}}, ƒê√£ click: ${{clickCount}} l·∫ßn`);
            
            // Scroll l√™n ƒë·∫ßu ƒë·ªÉ chu·∫©n b·ªã crawl
            window.scrollTo(0, 0);

            return {{ 
                success: true, 
                totalArticles: finalCount, 
                clickCount: clickCount 
            }};
        }})();
        """

        # Wait for condition v·ªõi threshold c√≥ th·ªÉ config
        wait_for_condition = f"""js:() => {{
            const links = Array.from(document.querySelectorAll('a[href*="/the-batch/"]'));
            const valid = links.filter(a =>
                !a.href.includes('/tag/') &&
                !a.href.includes('/category/') &&
                !a.href.includes('/author/') &&
                !a.href.includes('/about') &&
                !a.href.includes('/page/') &&
                !a.href.endsWith('/the-batch/') &&
                !a.href.endsWith('/the-batch') &&
                !a.href.includes('#') &&
                !a.href.includes('mailto:') &&
                !a.href.includes('tel:')
            );
            console.log(`Wait for check: ${{valid.length}} valid articles found`);
            return valid.length >= {self.min_articles_threshold};
        }}"""

        config = CrawlerRunConfig(
            js_code=js_code,
            wait_for=wait_for_condition,
            delay_before_return_html=2,
            page_timeout=30000,  # 30s
        )

        async with AsyncWebCrawler(headless=True, verbose=True) as crawler:

            logger.info(f"üåê ƒêang truy c·∫≠p: {self.base_url}")
            logger.info(f"‚öôÔ∏è C·∫•u h√¨nh: max_articles={self.max_articles}, min_threshold={self.min_articles_threshold}, max_clicks={self.max_load_more_clicks}")

            result = await crawler.arun(url=self.base_url, config=config)
            
            logger.info(f"üìã JavaScript execution result: {result.js_execution_result}")

            # Parse HTML ƒë·ªÉ l·∫•y links
            soup = BeautifulSoup(result.html, 'html.parser')
            all_links = set()

            # T√¨m t·∫•t c·∫£ links theo c√°ch ƒë√£ ƒë∆∞·ª£c test
            for a in soup.find_all("a", href=True):
                href = a["href"]
                if (
                    "/the-batch/" in href
                    and not any(x in href for x in ["/tag/", "/category/", "/author/","/about", "/page/", "#", "mailto:", "tel:"])
                    and href.strip() != "/the-batch/"
                    and not href.rstrip("/").endswith("/the-batch")
                ):
                    full_url = urljoin("https://www.deeplearning.ai", href)
                    all_links.add(full_url)

            # S·∫Øp x·∫øp v√† log k·∫øt qu·∫£
            links = sorted(all_links)
            logger.info(f"üéØ T·ªïng c·ªông t√¨m th·∫•y {len(links)} link b√†i vi·∫øt unique")

            # Log m·ªôt v√†i URL ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra
            if links:
                logger.info("üîç M·ªôt v√†i URL ƒë·∫ßu ti√™n:")
                for i, link in enumerate(links[:5]):
                    logger.info(f"  {i+1}. {link}")

            # Tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng theo max_articles
            final_links = links[:self.max_articles] if self.max_articles else links
            logger.info(f"üìä S·∫Ω crawl {len(final_links)} b√†i vi·∫øt")

            return final_links

    async def crawl_article(self, url):
        """Crawl m·ªôt b√†i vi·∫øt c·ª• th·ªÉ v·ªõi c·∫£i thi·ªán"""
        try:
            config = CrawlerRunConfig(
                # headers={
                #     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                # },
                page_timeout=30000
            )

            async with AsyncWebCrawler(
                headless=True,
                verbose=False
            ) as crawler:
                
                result = await crawler.arun(url=url, config=config)
                soup = BeautifulSoup(result.html, 'html.parser')

                def get_text(selectors, default=""):
                    """Th·ª≠ nhi·ªÅu selector ƒë·ªÉ t√¨m text"""
                    if isinstance(selectors, str):
                        selectors = [selectors]

                    for selector in selectors:
                        element = soup.select_one(selector)
                        if element:
                            text = element.get_text(strip=True)
                            if text:
                                return text
                    return default

                # C·∫£i thi·ªán c√°ch l·∫•y title
                    # C·∫£i thi·ªán c√°ch l·∫•y title v√† subtitle ri√™ng bi·ªát
                title = ""
                subtitle = ""

                h1_element = soup.select_one('h1')
                if h1_element:
                    # L·∫•y subtitle t·ª´ span b√™n trong h1
                    span_element = h1_element.find('span')
                    if span_element:
                        subtitle = span_element.get_text(strip=True)
                        # X√≥a span ƒë·ªÉ l·∫•y title ch√≠nh
                        span_element.decompose()
                        title = h1_element.get_text(strip=True)
                    else:
                        # N·∫øu kh√¥ng c√≥ span, fallback v·ªÅ c√°ch c≈©
                        full_title = h1_element.get_text(strip=True)
                        
                        # T√°ch title ƒë∆°n gi·∫£n: l·∫•y t·ª´ sau "reveals" tr·ªü ƒëi
                        if "reveals" in full_title.lower():
                            parts = full_title.split("reveals", 1)
                            if len(parts) == 2:
                                title = ("reveals" + parts[1]).strip()
                            else:
                                title = full_title
                        else:
                            title = full_title

                # N·∫øu v·∫´n kh√¥ng c√≥ title, th·ª≠ c√°c selector kh√°c
                if not title:
                    title = get_text([
                        'h1.post-title',
                        'h1[class*="title"]',
                        '.entry-title',
                        '.title',
                        '[class*="headline"]'
                    ], "Kh√¥ng t√¨m th·∫•y ti√™u ƒë·ªÅ")

                # L√†m s·∫°ch title v√† subtitle
                title = re.sub(r'\s+', ' ', title).strip()
                subtitle = re.sub(r'\s+', ' ', subtitle).strip()
                print(f"Title: {title}")
                print(f"Subtitle: {subtitle}")
                
                # C·∫£i thi·ªán c√°ch l·∫•y author
                author = get_text([
                    '.author-name',
                    '.byline .author',
                    '[class*="author"]',
                    '.post-author',
                    '.entry-author'
                ], "Kh√¥ng c√≥ th√¥ng tin t√°c gi·∫£")

                # C·∫£i thi·ªán c√°ch l·∫•y publish date
                publish_date = None

                # Th·ª≠ c√°c meta tag
                for meta_attr in ['article:published_time', 'datePublished', 'publishdate']:
                    meta_date = soup.select_one(f'meta[property="{meta_attr}"], meta[name="{meta_attr}"]')
                    if meta_date:
                        publish_date = meta_date.get('content')
                        break

                if not publish_date:
                    # Th·ª≠ t√¨m trong time tag
                    time_elem = soup.select_one('time[datetime]')
                    if time_elem:
                        publish_date = time_elem.get('datetime')

                if not publish_date:
                    # Th·ª≠ c√°c class th∆∞·ªùng d√πng cho date
                    publish_date = get_text([
                        '.publish-date',
                        '.post-date',
                        '.entry-date',
                        '[class*="date"]',
                        '.meta-date'
                    ])

                # C·∫£i thi·ªán c√°ch l·∫•y description
                description = ""

                # Th·ª≠ meta description
                meta_desc = soup.select_one('meta[name="description"], meta[property="og:description"]')
                if meta_desc:
                    description = meta_desc.get('content', '').strip()

                if not description:
                    description = get_text([
                        '.post-excerpt',
                        '.entry-summary',
                        '.excerpt',
                        '[class*="summary"]',
                        '.lead'
                    ])

                # L·∫•y main image
                main_image = None

                # Th·ª≠ og:image
                meta_image = soup.select_one('meta[property="og:image"]')
                if meta_image:
                    main_image = meta_image.get('content')

                if not main_image:
                    # Th·ª≠ featured image
                    img_elem = soup.select_one('.featured-image img, .post-image img, .entry-image img, img')
                    if img_elem:
                        main_image = img_elem.get('src') or img_elem.get('data-src')

                if main_image and main_image.startswith('/'):
                    main_image = urljoin("https://www.deeplearning.ai", main_image)

                # C·∫£i thi·ªán c√°ch l·∫•y content
                content = "Kh√¥ng t√¨m th·∫•y n·ªôi dung chi ti·∫øt"

                # Th·ª≠ c√°c selector kh√°c nhau cho content
                content_selectors = [
                    '.post-content',
                    '.entry-content',
                    '.article-content',
                    '[class*="content"]',
                    '.post-body',
                    'article .content',
                    'main article'
                ]

                for selector in content_selectors:
                    content_div = soup.select_one(selector)
                    if content_div:
                        # Lo·∫°i b·ªè c√°c element kh√¥ng c·∫ßn thi·∫øt
                        for unwanted in content_div.select('.ad, .advertisement, .social, .share, script, style, .related, .comments'):
                            unwanted.decompose()

                        # L·∫•y text t·ª´ paragraphs
                        paragraphs = content_div.select('p')
                        if paragraphs:
                            content_parts = []
                            for p in paragraphs:
                                p_text = p.get_text(strip=True)
                                if p_text and len(p_text) > 20:  # Ch·ªâ l·∫•y ƒëo·∫°n c√≥ √Ω nghƒ©a
                                    content_parts.append(p_text)

                            if content_parts:
                                content = '\n\n'.join(content_parts)
                                break
                        else:
                            # N·∫øu kh√¥ng c√≥ p tag, l·∫•y to√†n b·ªô text
                            content_text = content_div.get_text(separator='\n\n', strip=True)
                            if content_text and len(content_text) > 100:
                                content = content_text
                                break

                # Parse publish date
                if publish_date:
                    try:
                        if isinstance(publish_date, str):
                            publish_date = parser.parse(publish_date).replace(tzinfo=None)
                    except:
                        logger.warning(f"Kh√¥ng th·ªÉ parse ng√†y th√°ng: {publish_date}")
                        publish_date = None

                # T·∫°o unique ID t·ª´ URL
                parsed_url = urlparse(url)
                article_id = parsed_url.path.split('/')[-1] or parsed_url.path.split('/')[-2]
                if not article_id:
                    article_id = str(hash(url))

                combined_text = f"{title.strip()}\n\n{content.strip()}"
                summary = self.summarizer.create_bullet_summary(combined_text) if combined_text else "Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt"

                return {
                    '_id': article_id,
                    'title': title,
                    'author': author,
                    'publish_date': publish_date,
                    'description': description,
                    'main_image': main_image,
                    'content': content,
                    'url': url,
                    'summary': summary,
                    'category': self.category_name,
                    'crawled_at': datetime.now(pytz.timezone("Asia/Ho_Chi_Minh")).isoformat(),
                    'source': 'deeplearning.ai',
                    "summary": summary

                }

        except Exception as e:
            logger.error(f"L·ªói khi crawl {url}: {str(e)}")
            return None

    def filter_existing_urls(self, urls):
        """L·ªçc b·ªè c√°c URL ƒë√£ t·ªìn t·∫°i trong database"""
        if self.db is None:
            logger.warning("Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi MongoDB ƒë·ªÉ l·ªçc URLs")
            return urls

        collection = self.db[self.category_name]
        existing_urls = set()

        try:
            existing_docs = collection.find({}, {"url": 1})
            existing_urls = {doc["url"] for doc in existing_docs}
        except Exception as e:
            logger.error(f"L·ªói khi truy v·∫•n existing URLs: {str(e)}")
            return urls

        filtered_urls = [url for url in urls if url not in existing_urls]
        logger.info(f"ƒê√£ l·ªçc: {len(urls)} URLs ban ƒë·∫ßu -> {len(filtered_urls)} URLs m·ªõi")
        return filtered_urls

    async def crawl_all_articles(self):
        """Crawl t·∫•t c·∫£ b√†i vi·∫øt"""
        start_time = time.time()

        logger.info("üöÄ ƒêang b·∫Øt ƒë·∫ßu qu√° tr√¨nh crawl v·ªõi Load More...")
        article_links = await self.get_article_links_with_load_more()

        if not article_links:
            logger.warning("‚ùå Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o!")
            return

        # L·ªçc c√°c URL ƒë√£ t·ªìn t·∫°i
        filtered_links = self.filter_existing_urls(article_links)

        if not filtered_links:
            logger.info(f"‚ÑπÔ∏è Kh√¥ng c√≥ b√†i vi·∫øt m·ªõi n√†o trong danh m·ª•c {self.category_name} c·∫ßn crawl.")
            return

        logger.info(f"üìù ƒêang crawl {len(filtered_links)} b√†i vi·∫øt m·ªõi...")

        # Crawl t·ª´ng b√†i vi·∫øt v·ªõi concurrency control
        semaphore = asyncio.Semaphore(3)  # Gi·ªØ 3 ƒë·ªÉ ·ªïn ƒë·ªãnh

        async def crawl_single_article(url, index):
            async with semaphore:
                logger.info(f"[{index}/{len(filtered_links)}] üîÑ ƒêang crawl: {url}")
                article_data = await self.crawl_article(url)
                if article_data:
                    self.articles.append(article_data)
                    logger.info(f"    ‚úÖ {article_data['title'][:60]}...")
                else:
                    logger.warning(f"    ‚ùå Kh√¥ng th·ªÉ crawl: {url}")

                # Delay gi·ªØa c√°c request
                await asyncio.sleep(1)
                return article_data

        # Ch·∫°y crawl parallel
        tasks = [crawl_single_article(url, i+1) for i, url in enumerate(filtered_links)]
        await asyncio.gather(*tasks, return_exceptions=True)

        elapsed = time.time() - start_time
        logger.info(f"‚è±Ô∏è T·ªïng th·ªùi gian crawl: {elapsed:.2f} gi√¢y")
        logger.info(f"üìä Th√†nh c√¥ng crawl: {len(self.articles)}/{len(filtered_links)} b√†i vi·∫øt")

    def save_to_mongodb(self):
        """L∆∞u c√°c b√†i vi·∫øt v√†o MongoDB"""
        try:
            if self.db is None:
                self._connect_mongodb()
                if self.db is None:
                    logger.error("Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi MongoDB ƒë·ªÉ l∆∞u d·ªØ li·ªáu")
                    return False

            collection = self.db[self.category_name]

            if self.articles:
                saved_count = 0
                for article in self.articles:
                    try:
                        result = collection.replace_one(
                            {"_id": article["_id"]},
                            article,
                            upsert=True
                        )
                        if result.upserted_id or result.modified_count > 0:
                            saved_count += 1
                    except Exception as e:
                        logger.error(f"L·ªói khi l∆∞u b√†i vi·∫øt {article.get('title', 'N/A')}: {str(e)}")

                logger.info(f"üíæ ƒê√£ l∆∞u th√†nh c√¥ng {saved_count}/{len(self.articles)} b√†i vi·∫øt v√†o collection '{self.category_name}'")
            else:
                logger.info(f"‚ÑπÔ∏è Kh√¥ng c√≥ b√†i vi·∫øt ƒë·ªÉ l∆∞u v√†o collection '{self.category_name}'")

            return True

        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u d·ªØ li·ªáu v√†o MongoDB: {str(e)}")
            return False
        finally:
            self._close_mongodb()

    def save_results(self, file_path=None):
        """L∆∞u k·∫øt qu·∫£ ra file JSON (n·∫øu c·∫ßn) v√† v√†o MongoDB"""
        # L∆∞u v√†o MongoDB
        mongo_result = self.save_to_mongodb()

        # N·∫øu c√≥ y√™u c·∫ßu l∆∞u file JSON
        if file_path:
            try:
                with open(file_path, "w", encoding="utf-8") as f:
                    json.dump(self.articles, f, ensure_ascii=False, indent=2, default=str)
                logger.info(f"üìÑ ƒê√£ l∆∞u {len(self.articles)} b√†i vi·∫øt v√†o {file_path}")
            except Exception as e:
                logger.error(f"Kh√¥ng th·ªÉ l∆∞u file: {str(e)}")

        return mongo_result
