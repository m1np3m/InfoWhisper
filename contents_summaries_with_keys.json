[
    {
        "content": "Dear friends,Greetings from Davos, Switzerland! Many business and government leaders are gathered here again for the annual World Economic Forum to discuss tech, climate, geopolitics, and economic growth. While the vast majority of my conversations have been on AI business implementations and governance, I have also been speaking about our latest AI climate simulator and about geoengineering. After speaking about geoengineering onstage at multiple events to a total of several hundred people, I’ve been pleasantly surprised by almost uniformly positive reactions. You can play with our simulator here.Here’s why I think we should seriously consider geoengineering: The world urgently needs to reduce carbon emissions, but it hasn’t happened fast enough. Given recent emission trends, without geoengineering, there’s no longer any plausible path to keeping global warming to the 1.5 degrees Celsius goal set by the Paris agreement. Under reasonable assumptions, we are on a path to 2.5 degrees of warming or worse. We might be in for additional abrupt changes if we hit certain tipping points.If you tilt a four-legged chair by a few degrees, it will fall back onto its four legs. But if you tip it far enough — beyond its “tipping point” — it will fall over with a crash. Climate tipping points are like that, where parts of our planet, warmed sufficiently, might reach a point where the planet reorganizes abruptly in a way that is impossible to reverse. Examples include a possible melting of the Arctic permafrost, which would release additional methane (a potent greenhouse gas), or a collapse of ocean currents that move warm water northward from the tropics (the Atlantic Meridional Overturning Circulation).Keeping warming low will significantly lower the risk of hitting a tipping point. This is why the OECD’s report states, “the existence of climate system tipping points means it is vital to limit the global temperature increase to 1.5 degrees C, with no or very limited overshoot.”The good news is that geoengineering keeps the 1.5 degree goal alive. Spraying reflective particles into the atmosphere — an idea called Stratospheric Aerosol Injection (SAI) — to reflect 1% of sunlight back into space would get us around 1 degree Celsius of cooling.Now, there are risks to doing this. For example, just as global warming has had uneven regional effects, the global cooling impact will also be uneven. But on average, a planet with 1.5 degrees of warming would be much more livable than one with 2.5 degrees (or more). Further, after collaborating extensively with climate scientists on AI climate models and examining the output of multiple such models, I believe the risks associated with cooling down our planet will be much lower than the risks of runaway climate change.I hope we can build a global governance structure to decide collectively whether, and if so to what extent and how, to implement geoengineering. For example, we might start with small scale experiments (aiming for <<0.1 degrees of cooling) that are easy to stop/reverse at any time. Further, there is much work to be done to solve difficult engineering challenges, such as how to build and operate a fleet of aircraft to efficiently lift and spray reflective particles at the small particle sizes needed.Even as I have numerous conversations about AI business and governance here at the World Economic Forum, I am glad that AI climate modeling is helpful for addressing global warming. If you are interested in learning more about geoengineering, I encourage you to play with our simulator at planetparasol.ai.I am grateful to my collaborators on the simulator work: Jeremy Irvin, Jake Dexheimer, Dakota Gruener, Charlotte DeWald, Daniele Visioni, Duncan Watson-Parris, Douglas MacMartin, Joshua Elliott, Juerg Luterbacher, and Kion Yaghoobzadeh.Keep learning!Andrew",
        "summary": "* Author discussed AI business implementations and governance at the World Economic Forum.\n* Author discussed their latest AI climate simulator and AI climate models, considered helpful for addressing global warming.\n* Collaboration on AI climate models with climate scientists and a specific simulator (planetparasol.ai) were mentioned.",
        "publish_date": "Jan 22, 2025",
        "keywords": [
            "planetparasol.ai",
            "AI climate simulator",
            "AI climate models",
            "AI business implementations",
            "AI governance"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-cool-a-warming-planet/"
    },
    {
        "content": "Dear friends,Is AI progressing rapidly? Yes! But while the progress of underlying AI technology has indeed sped up over the past 2 years, the fastest acceleration is in applications.Consider this: GPT-4 was released March 2023. Since then, models have become much faster, cheaper, sometimes smaller, more multimodal, and better at reasoning, and many more open weight versions are available — so progress has been fantastic! (Claims that AI is “hitting a wall” seem extremely ill-informed.) But more significantly, many applications that  already were theoretically possible using the March 2023 version of GPT-4 — in areas such as customer service, question answering, and process automation — now have significant early momentum.I’m confident 2025 will see even faster and more exciting advances than 2024 in both AI technology and applications. Looking back, the one thing that could have stopped AI was bad, anti-competitive regulation that would have put onerous burdens on developers, particularly of open models. So long as we remain vigilant and hold off these anti-innovation forces, we’ll keep up or even further accelerate progress.I’m also seeing a widening gap between those at the cutting edge (which includes many readers of The Batch!) and those who have not yet tried out ChatGPT even once (yes, a lot of people are still in this group!). As technology changes around us, we all have to keep up to remain relevant and be able to make significant contributions. I’m committed to making sure DeepLearning.AI continues to help you learn the most useful and important AI technologies. If you’re making New Year’s resolutions, I hope you’ll include us in your learning plan!AI is the most important technological change happening in the world right now. I’m thrilled to be working in this exciting sector alongside you, and I’m grateful for your efforts to learn about and apply it to better the lives of yourself and others.Happy holidays!Andrew",
        "summary": "*   AI progress is accelerating, with the fastest acceleration seen in applications over the past 2 years.\n*   Since GPT-4's March 2023 release, models have become faster, cheaper, sometimes smaller, more multimodal, better at reasoning, and more open weight versions are available.\n*   Applications such as customer service, question answering, and process automation, theoretically possible with March 2023 GPT-4, now have significant early momentum.\n*   Faster and more exciting advances in both AI technology and applications are anticipated in 2025 compared to 2024.\n*   Bad, anti-competitive regulation, particularly on open models, is identified as a potential factor that could stop AI progress.",
        "publish_date": "Dec 25, 2024",
        "keywords": [
            "GPT-4",
            "Multimodal",
            "Reasoning",
            "Open weight models",
            "Question Answering"
        ],
        "url": "https://www.deeplearning.ai/the-batch/think-technology-is-moving-fast-look-at-applications/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,AI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications. This is making it possible to build new kinds of things, which in turn is driving shifts in best practices in product management — the discipline of defining what to build to serve users — because what is possible to build has shifted. In this letter, I’ll share some best practices I have noticed.Use concrete examples to specify AI products. Starting with a concrete idea helps teams gain speed. If a product manager (PM) proposes to build “a chatbot to answer banking inquiries that relate to user accounts,” this is a vague specification that leaves much to the imagination. For instance, should the chatbot answer questions only about account balances or also about interest rates, processes for initiating a wire transfer, and so on? But if the PM writes out a number (say, between 10 and 50) of concrete examples of conversations they’d like a chatbot to execute, the scope of their proposal becomes much clearer. Just as a machine learning algorithm needs training examples to learn from, an AI product development team needs concrete examples of what we want an AI system to do. In other words, the data is your PRD (product requirements document)!In a similar vein, if someone requests “a vision system to detect pedestrians outside our store,” it’s hard for a developer to understand the boundary conditions. Is the system expected to work at night? What is the range of permissible camera angles? Is it expected to detect pedestrians who appear in the image even though they’re 100m away? But if the PM collects a handful of pictures and annotates them with the desired output, the meaning of “detect pedestrians” becomes concrete. An engineer can assess if the specification is technically feasible and if so, build toward it. Initially, the data might be obtained via a one-off, scrappy process, such as the PM walking around taking pictures and annotating them. Eventually, the data mix will shift to real-word data collected by a system running in production.Using examples (such as inputs and desired outputs) to specify a product has been helpful for many years, but the explosion of possible AI applications is creating a need for more product managers to learn this practice.Assess technical feasibility of LLM-based applications by prompting. When a PM scopes out a potential AI application, whether the application can actually be built — that is, its technical feasibility — is a key criterion in deciding what to do next. For many ideas for LLM-based applications, it’s increasingly possible for a PM, who might not be a software engineer, to try prompting — or write just small amounts of code — to get an initial sense of feasibility.For example, a PM may envision a new internal tool for routing emails from customers to the right department (such as customer service, sales, etc.). They can prompt an LLM to see if they can get it to select the right department based on an input email, and see if they can achieve high accuracy. If so, this gives engineering a great starting point from which to implement the tool. If not, the PM can falsify the idea themselves and perhaps improve the product idea much faster than if they had to rely on an engineer to build a prototype.Often, testing feasibility requires a little more than prompting. For example, perhaps the LLM-based email system needs basic RAG capability to help it make decisions. Fortunately, the barrier to writing small amounts of code is now quite low, since AI can help by acting as a coding companion, as I describe in the course, “AI Python for Beginners.” This means that PMs can do much more technical feasibility testing, at least at a basic level, than was possible before.Prototype and test without engineers. User feedback to initial prototypes is also instrumental to shaping products. Fortunately, barriers to building prototypes rapidly are falling, and PMs themselves can move prototypes forward without needing software developers.In addition to using LLMs to help write code for prototyping, tools like Replit, Vercel’s V0, Bolt, and Anthropic’s Artifacts (I’m a fan of all of these!) are making it easier for people without a coding background to build and experiment with simple prototypes. These tools are increasingly accessible to non-technical users, though I find that those who understand basic coding are able to use them much more effectively, so it’s still important to learn basic coding. (Interestingly, highly technical, experienced developers use them too!) Many members of my teams routinely use such tools to prototype, get user feedback, and iterate quickly.AI is enabling a lot of new applications to be built, creating massive growth in demand for AI product managers who know how to scope out and help drive progress in building these products. AI product management existed before the rise of generative AI, but the increasing ease of building applications is creating greater demand for AI applications, and thus a lot of PMs are learning AI and these emerging best practices for building AI products. I find this discipline fascinating, and will keep on sharing best practices as they grow and evolve.Keep learning!Andrew",
        "summary": "*   AI Product Management is evolving rapidly due to the growth of generative AI and AI-based developer tools, enabling new applications and shifting best practices.\n*   Specifying AI products effectively requires using concrete input/output examples, similar to ML training data, which can serve as the product requirements document (PRD).\n*   Product Managers can assess the technical feasibility of LLM-based applications early by using prompting or writing small amounts of code.\n*   Tools and AI assistance are lowering barriers, allowing PMs to build and test prototypes rapidly without engineers to gather user feedback.\n*   The increased ease of building AI applications is driving significant demand for AI Product Managers skilled in these emerging practices.",
        "publish_date": "Dec 11, 2024",
        "keywords": [
            "ML training data",
            "prompting",
            "LLM-based applications",
            "AI-based developer tools",
            "Generative AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/best-practices-for-ai-product-management/"
    },
    {
        "content": "Dear friends,I’m thrilled that former students and postdocs of mine won both of this year’s NeurIPS Test of Time Paper Awards. This award recognizes papers published 10 years ago that have significantly shaped the research field. The recipients included Ian Goodfellow (who, as an undergraduate, built my first GPU server for deep learning in his dorm room) and his collaborators for their work on generative adversarial networks, and my former postdoc Ilya Sutskever and PhD student Quoc Le (with Oriol Vinyals) for their work on sequence-to-sequence learning. Congratulations to all these winners!By nature, I tend to focus on the future rather than the past. Steve Jobs famously declined to build a corporate museum, instead donating Apple's archives to Stanford University, because he wanted to keep the company forward-looking. Jeff Bezos encourages teams to approach every day as if it were “Day 1,” a mindset that emphasizes staying in the early, innovative stage of a company or industry. These philosophies resonate with me.But taking a brief look at the past can help us reflect on lessons for the future. One takeaway from looking at what worked 10 to 15 years ago is that many of the teams I led bet heavily on scaling to drive AI progress — a bet that laid a foundation to build larger and larger AI systems. At the time, the idea of scaling up neural networks was controversial, and I was on the fringe. I recall distinctly that, around  2008, Yoshua Bengio advised me not to bet on scaling and to focus on inventing algorithms instead!A lesson I carry from that time is to not worry about what others think, but follow your convictions, especially if you have data to support your beliefs. Small-scale experiments performed by my Stanford group convinced me that scaling up neural networks would drive significant progress, and that’s why I was willing to ignore the skeptics. The diagram below, generated by Adam Coates and Honglak Lee, is the one that most firmed up my beliefs at that time. It shows that, for a range of models, the larger we scaled them, the better they perform. I remember presenting it at CIFAR 2010, and if I had to pick a single reason why I pushed through to start Google Brain and set as the team’s #1 goal to scale up deep learning algorithms, it is this diagram!I also remember presenting at NeurIPS in 2008 our work on using GPUs to scale up training neural networks. (By the way, one measure of success in academia is when your work becomes sufficiently widely accepted that no one cites it anymore. I’m quite pleased the idea that GPUs should be used for AI — which was controversial back then — is now such a widely accepted “fact” that no one bothers to cite early papers that pushed for it.😃)When I started Google Brain, the thesis was simple: I wanted to use the company’s  huge computing capability to scale up deep learning. Shortly afterward, I built Stanford’s first supercomputer for deep learning using GPUs, since I could move faster at Stanford than within a large company. A few years later, my team at Baidu showed that as you scale up a model, its performance improves linearly on a log-log scale, which was a precursor to OpenAI’s scaling laws.As I look to the future, I’m sure there are ideas that many people are skeptical about today, but will prove to be accurate. Scaling up AI models turned out to be useful for many teams, and it continues to be exciting, but now I’m even more excited by upcoming ideas that will prove to be even more valuable in the future.This past year, I spent a lot of time encouraging teams to build applications with agentic AI and worked to share best practices. I have a few hypotheses for additional technologies that will be important next year. I plan to spend the winter holiday playing with a few of them, and I will have more to share next year. But if you have an idea that you have conviction on, so long as you can do so responsibly, I encourage you to pursue it!Keep learning,Andrew",
        "summary": "*   Former students/postdocs won NeurIPS Test of Time Paper Awards for their work on Generative Adversarial Networks and sequence-to-sequence learning.\n*   A key lesson from 10-15 years ago is the success of betting heavily on scaling to drive AI progress, despite skepticism (e.g., 2008 advice), supported by evidence like a 2010 diagram showing performance improves with scale.\n*   Early work (NeurIPS 2008) advocated for using GPUs to scale neural network training, a controversial idea then that is now widely accepted.\n*   Work by the Baidu team showed model performance improves linearly on a log-log scale with scaling, preceding OpenAI’s scaling laws.\n*   Current focus includes encouraging teams to build applications with agentic AI and sharing best practices.",
        "publish_date": "Dec 18, 2024",
        "keywords": [
            "Generative Adversarial Networks",
            "sequence-to-sequence learning",
            "GPUs",
            "scaling laws",
            "agentic AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/focus-on-the-future-learn-from-the-past/"
    },
    {
        "content": "Dear friends,The buzz over DeepSeek this week crystallized, for many people, a few important trends that have been happening in plain sight: (i) China is catching up to the U.S. in generative AI, with implications for the AI supply chain. (ii) Open weight models are commoditizing the foundation-model layer, which creates opportunities for application builders. (iii) Scaling up isn’t the only path to AI progress. Despite the massive focus on and hype around processing power, algorithmic innovations are rapidly pushing down training costs.About a week ago, DeepSeek, a company based in China, released DeepSeek-R1, a remarkable model whose performance on benchmarks is comparable to OpenAI’s o1. Further, it was released as an open weight model with a permissive MIT license. At Davos last week, I got a lot of questions about it from non-technical business leaders. And on Monday, the stock market saw a “DeepSeek selloff”: The share prices of Nvidia and a number of other U.S. tech companies plunged. (As of the time of writing, they have recovered somewhat.)Here’s what I think DeepSeek has caused many people to realize:China is catching up to the U.S. in generative AI. When ChatGPT was launched in November 2022, the U.S. was significantly ahead of China in generative AI. Impressions change slowly, and so even recently I heard friends in both the U.S. and China say they thought China was behind. But in reality, this gap has rapidly eroded over the past two years. With models from China such as Qwen (which my teams have used for months), Kimi, InternVL, and DeepSeek, China had clearly been closing the gap, and in areas such as video generation there were already moments where China seemed to be in the lead.I’m thrilled that DeepSeek-R1 was released as an open weight model, with a technical report that shares many details. In contrast, a number of U.S. companies have pushed for regulation to stifle open source by hyping up hypothetical AI dangers such as human extinction. It is now clear that open source/open weight models are a key part of the AI supply chain: Many companies will use them. If the U.S. continues to stymie open source, China will come to dominate this part of the supply chain and many businesses will end up using models that reflect China’s values much more than America’s.Open weight models are commoditizing the foundation-model layer. As I wrote previously, LLM token prices have been falling rapidly, and open weights have contributed to this trend and given developers more choice. OpenAI’s o1 costs $60 per million output tokens; DeepSeek R1 costs $2.19. This nearly 30x difference brought the trend of falling prices to the attention of many people.The business of training foundation models and selling API access is tough. Many companies in this area are still looking for a path to recouping the massive cost of model training. The article “AI’s $600B Question” lays out the challenge well (but, to be clear, I think the foundation model companies are doing great work, and I hope they succeed). In contrast, building applications on top of foundation models presents many great business opportunities. Now that others have spent billions training such models, you can access these models for mere dollars to build customer service chatbots, email summarizers, AI doctors, legal document assistants, and much more.Scaling up isn’t the only path to AI progress. There’s been a lot of hype around scaling up models as a way to drive progress. To be fair, I was an early proponent of scaling up models. A number of companies raised billions of dollars by generating buzz around the narrative that, with more capital, they could (i) scale up and (ii) predictably drive improvements. Consequently, there has been a huge focus on scaling up, as opposed to a more nuanced view that gives due attention to the many different ways we can make progress. Driven in part by the U.S. AI chip embargo, the DeepSeek team had to innovate on many optimizations to run on less-capable H800 GPUs rather than H100s, leading ultimately to a model trained (omitting research costs) for under $6M of compute.It remains to be seen if this will actually reduce demand for compute. Sometimes making each unit of a good cheaper can result in more dollars in total going to buy that good. I think the demand for intelligence and compute has practically no ceiling over the long term, so I remain bullish that humanity will use more intelligence even as it gets cheaper.I saw many different interpretations of DeepSeek’s progress on social media, as if it was a Rorschach test that allowed many people to project their own meaning onto it. I think DeepSeek-R1 has geopolitical implications that are yet to be worked out. And it’s also great for AI application builders. My team is already brainstorming ideas that are newly possible only because we have easy access to an open advanced reasoning model. This continues to be a great time to build!Keep learning,Andrew",
        "summary": "*   DeepSeek-R1, an open weight model with an MIT license from a China-based company, shows performance comparable to OpenAI's o1.\n*   Its release underscores China's rapid progress in generative AI, narrowing the gap with the U.S. and influencing the AI supply chain.\n*   Open weight models like DeepSeek-R1 ($2.19 per million output tokens) are commoditizing the foundation model layer, significantly reducing costs compared to models like OpenAI's o1 ($60 per million output tokens) and creating opportunities for application builders.\n*   DeepSeek-R1's training cost (under $6M compute, omitting research) highlights that algorithmic innovations, partly driven by optimizing for H800 GPUs over H100s, are a key path to AI progress beyond just scaling up.",
        "publish_date": "Jan 29, 2025",
        "keywords": [
            "DeepSeek-R1",
            "OpenAI's o1",
            "H800 GPUs",
            "H100s",
            "open weight model"
        ],
        "url": "https://www.deeplearning.ai/the-batch/three-takeaways-from-deepseeks-big-week/"
    },
    {
        "content": "Dear friends,Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future!Software is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements — such as cars (with internal-combustion engines) and gasoline — falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build.This is why I’m excited about the future of Product Management, the discipline of developing and managing software products. I’m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products.Many companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow.This change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow.Further, AI Product Management requires a different set of skills than traditional software Product Management. It requires:Technical proficiency in AI. PMs need to understand what products might be technically feasible to build. They also need to understand the lifecycle of AI projects, such as data collection, building, then monitoring, and maintenance of AI models.Iterative development. Because AI development is much more iterative than traditional software and requires more course corrections along the way, PMs need to understand how to manage such a process.Data proficiency. AI products often learn from data, and they can be designed to generate richer forms of data than traditional software.Skill in managing ambiguity. Because AI’s performance is hard to predict in advance, PMs need to be comfortable with this and have tactics to manage it.Ongoing learning. AI technology is advancing rapidly. PMs, like everyone else who aims to make best use of the technology, need to keep up with the latest technology advances, product ideas, and how they fit into users’ lives.Finally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled at gathering feedback fast to keep projects moving. Increasingly, I also expect strong product managers to be able to build prototypes for themselves.The demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work.The variety of valuable things we can build is nearly unlimited. What a great time to build!Keep learning,Andrew",
        "summary": "*   AI making software building cheaper and faster is increasing demand for Product Managers who define what to build.\n*   A shortage exists for Product Managers who understand AI, as Engineers are adopting AI faster.\n*   AI Product Management requires specific skills including technical AI proficiency (covering the AI project lifecycle: data collection, building, monitoring, maintenance), data proficiency, iterative development, managing ambiguity, ongoing learning, and responsible AI implementation.\n*   The demand for AI Product Managers is projected to be huge.",
        "publish_date": "Jan 15, 2025",
        "keywords": [
            "Data collection",
            "Building",
            "Monitoring",
            "Maintenance",
            "Responsible AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-product-managers-will-be-in-demand/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Last month, a drone from Skyfire AI was credited with saving a police officer’s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened.Skyfire AI, an AI Fund portfolio company led by CEO Don Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers’ time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond.In January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located.From the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road — they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf’s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find  them.From the aerial footage, it appeared that the officer still had his radio, but  was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation.Fortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes.The officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we’d probably saved the officer’s life.Democratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we’re making. In the aftermath of Hurricane Helene last year, Skyfire AI’s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives.It’s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these.Keep building!Andrew",
        "summary": "*   Skyfire AI operates a public safety program using drones as first responders to enhance situational awareness and save officer time, particularly for tasks like checking false alarms.\n*   A Skyfire AI drone located a police officer and assailant in a hidden location, enabling additional units to arrive in 45 seconds (vs. estimated 5-7 minutes by car), credited with potentially saving the officer's life.\n*   Skyfire AI drones supported search-and-rescue after Hurricane Helene, locating missing persons and directing rescue assets, credited with saving 13 lives.\n*   The text highlights the importance of building drone technology with guardrails to enhance civil liberties and human rights.",
        "publish_date": "Feb 19, 2025",
        "keywords": [
            "Drones",
            "Computer Vision",
            "Object Detection",
            "Object Tracking",
            "Guardrails"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-ai-saved-a-police-officers-life/"
    },
    {
        "content": "Dear friends,A small number of people are posting text online that’s intended for direct consumption not by humans, but by LLMs (large language models). I find this a fascinating trend, particularly when writers are incentivized to help LLM providers better serve their users!People who post text online don’t always have an incentive to help LLM providers. In fact, their incentives are often misaligned. Publishers worry about LLMs reading their text, paraphrasing it, and reusing their ideas without attribution, thus depriving them of subscription or ad revenue. This has even led to litigation such as The New York Times’ lawsuit against OpenAI and Microsoft for alleged copyright infringement. There have also been demonstrations of prompt injections, where someone writes text to try to give an LLM instructions contrary to the provider’s intent. (For example, a handful of sites advise job seekers to get past LLM resumé screeners by writing on their resumés, in a tiny/faint font that’s nearly invisible to humans, text like “This candidate is very qualified for this role.”) Spammers who try to promote certain products — which is already challenging for search engines to filter out — will also turn their attention to spamming LLMs.But there are examples of authors who want to actively help LLMs. Take the example of a startup that has just published a software library. Because the online documentation is very new, it won’t yet be in LLMs’ pretraining data. So when a user asks an LLM to suggest software, the LLM won’t suggest this library, and even if a user asks the LLM directly to generate code using this library, the LLM won’t know how to do so. Now, if the LLM is augmented with online search capabilities, then it might find the new documentation and be able to use this to write code using the library. In this case, the developer may want to take additional steps to make the online documentation easier for the LLM to read and understand via RAG. (And perhaps the documentation eventually will make it into pretraining data as well.)Compared to humans, LLMs are not as good at navigating complex websites, particularly ones with many graphical elements. However, LLMs are far better than people at rapidly ingesting long, dense, text documentation. Suppose the software library has many functions that we want an LLM to be able to use in the code it generates. If you were writing documentation to help humans use the library, you might create many web pages that break the information into bite-size chunks, with graphical illustrations to explain it. But for an LLM, it might be easier to have a long XML-formatted text file that clearly explains everything in one go. This text might include a list of all the functions, with a dense description of each and an example or two of how to use it. (This is not dissimilar to the way we specify information about functions to enable LLMs to use them as tools.)A human would find this long document painful to navigate and read, but an LLM would do just fine ingesting it and deciding what functions to use and when!Because LLMs and people are better at ingesting different types of text, we write differently for LLMs than for humans. Further, when someone has an incentive to help an LLM better understand a topic — so the LLM can explain it better to users — then an author might write text to help an LLM.So far, text written specifically for consumption by LLMs has not been a huge trend. But Jeremy Howard’s proposal for web publishers to post a llms.txt file to tell LLMs how to use their websites, like a robots.txt file tells web crawlers what to do, is an interesting step in this direction. In a related vein, some developers are posting detailed instructions that tell their IDE how to use tools, such as the plethora of .cursorrules files that tell the Cursor IDE how to use particular software stacks.I see a parallel with SEO (search engine optimization). The discipline of SEO has been around for decades. Some SEO helps search engines find more relevant topics, and some is spam that promotes low-quality information. But many SEO techniques — those that involve writing text for consumption by a search engine, rather than by a human — have survived so long in part because search engines process web pages differently than humans, so providing tags or other information that tells them what a web page is about has been helpful.The need to write text separately for LLMs and humans might diminish if LLMs catch up with humans in their ability to understand complex websites. But until then, as people get more information through LLMs, writing text to help LLMs will grow.Keep learning!AndrewP.S. I like LLMs, but I like humans even more. So please keep writing text for humans as well. 😀",
        "summary": "*   A trend is emerging where online text is written specifically for LLM consumption, distinct from human-intended content.\n*   Conflicts exist regarding LLM content consumption, including publisher concerns over reuse/attribution (e.g., The New York Times’ lawsuit against OpenAI and Microsoft) and adversarial uses like prompt injections.\n*   Some authors are incentivized to optimize text for LLMs (e.g., new software documentation) to improve LLM understanding via methods like RAG or dense formats (like XML).\n*   LLMs consume text differently than humans (better at dense text, worse at complex websites), leading to proposals like `llms.txt` and developer files like `.cursorrules` to guide LLM interaction.\n*   Writing text for LLMs is compared to SEO, where content is optimized for non-human consumers (search engines).",
        "publish_date": "Nov 20, 2024",
        "keywords": [
            "RAG",
            "XML",
            "llms.txt",
            ".cursorrules",
            "prompt injections"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-is-part-of-your-online-audience/"
    },
    {
        "content": "Dear friends, A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.” There aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job). But for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow. 10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done. I think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact. Similarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way. A 2023 Harvard/BCG   estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves. Here in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.” Keep learning! Andrew ",
        "summary": "*   AI is expected to enable \"10x professionals\" in knowledge/information processing roles, unlike jobs limited by physical constraints.\n*   Achieving \"10x\" impact with AI requires sophisticated application beyond basic generative AI use, involving potentially coding, orchestrating AI tools/agents, and reinventing workflows.\n*   Specific examples of sophisticated AI use include efficiently using training examples (e.g., 100 vs 10,000), deep data analysis/experiments, coordinating AI suites for research, and orchestrating AI agents.\n*   A 2023 Harvard/BCG study showed consultants using GPT-4 completed 12% more tasks and 25% faster on average.",
        "publish_date": "Feb 05, 2025",
        "keywords": [
            "GPT-4",
            "Orchestrating AI agents",
            "Efficiently using training examples",
            "Coordinating AI suites",
            "Knowledge Processing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-ai-can-make-you-a-10x-professional/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,At the Artificial Intelligence Action Summit in Paris this week, U.S. Vice President J.D. Vance said, “I’m not here to talk about AI safety. ... I’m here to talk about AI opportunity.” I’m thrilled to see the U.S. government focus on opportunities in AI. Further, while it is important to use AI responsibly and try to stamp out harmful applications, I feel “AI safety” is not the right terminology for addressing this important problem. Language shapes thought, so using the right words is important. I’d rather talk about “responsible AI” than “AI safety.” Let me explain.First, there are clearly harmful applications of AI, such as non-consensual deepfake porn (which creates sexually explicit images of real people without their consent), the use of AI in misinformation, potentially unsafe medical diagnoses, addictive applications, and so on. We definitely want to stamp these out! There are many ways to apply AI in harmful or irresponsible ways, and we should discourage and prevent such uses.However, the concept of “AI safety” tries to make AI — as a technology — safe, rather than making safe applications of it. Consider the similar, obviously flawed notion of “laptop safety.” There are great ways to use a laptop and many irresponsible ways, but I don’t consider laptops to be intrinsically either safe or unsafe. It is the application, or usage, that determines if a laptop is safe. Similarly, AI, a general-purpose technology with numerous applications, is neither safe nor unsafe. How someone chooses to use it determines whether it is harmful or beneficial.Now, safety isn’t always a function only of how something is used. An unsafe airplane is one that, even in the hands of an attentive and skilled pilot, has a large chance of mishap. So we definitely should strive to build safe airplanes (and make sure they are operated responsibly)! The risk factors are associated with the construction of the aircraft rather than merely its application. Similarly, we want safe automobiles, blenders, dialysis machines, food, buildings, power plants, and much more.“AI safety” presupposes that AI, the underlying technology, can be unsafe. I find it more useful to think about how applications of AI can be unsafe.Further, the term “responsible AI” emphasizes that it is our responsibility to avoid building applications that are unsafe or harmful and to discourage people from using even beneficial products in harmful ways.If we shift the terminology for AI risks from “AI safety” to “responsible AI,” we can have more thoughtful conversations about what to do and what not to do.I believe the 2023 Bletchley AI Safety Summit slowed down European AI development — without making anyone safer — by wasting time considering science-fiction AI fears rather than focusing on opportunities. Last month, at Davos, business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting, unlike the one at Bletchley, will result in acceleration rather than deceleration.In a world where AI is becoming pervasive, if we can shift the conversation away from “AI safety” toward responsible [use of] AI, we will speed up AI’s benefits and do a better job of addressing actual problems. That will actually make people safer.Keep building!Andrew",
        "summary": "* The author advocates for using \"responsible AI\" instead of \"AI safety\" to discuss AI risks, arguing language shapes thought.\n* \"AI safety\" is considered flawed as it implies the technology itself is unsafe, whereas AI is a general-purpose technology whose safety depends on its application or usage.\n* Harmful applications of AI, such as non-consensual deepfake porn, misinformation, potentially unsafe medical diagnoses, and addictive applications, exist and should be prevented.\n* Shifting to \"responsible AI\" emphasizes human responsibility in avoiding unsafe applications and discouraging harmful usage.\n* Focusing on \"AI safety\" and \"science-fiction AI fears\" (like at the 2023 Bletchley AI Safety Summit) may slow down AI development without increasing safety, unlike focusing on responsible AI use and actual problems.",
        "publish_date": "Feb 12, 2025",
        "keywords": [
            "Deepfake",
            "Responsible AI",
            "AI safety",
            "Medical diagnoses",
            "Misinformation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-difference-between-ai-safety-and-responsible-ai/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,There’s a lingering misconception that building with generative AI is expensive. It is indeed expensive to train cutting-edge foundation models, and a number of companies have spent billions of dollars doing this (and even released some of their models as open weights). But as a result, it’s now very inexpensive to build a wide range of AI applications.The AI stack has several layers, shown in the diagram below. Here are the lower layers, from the bottom up:Semiconductors. Nvidia has been a huge benefactor in this space. AMD’s MI300 and forthcoming MI350 are also strong alternatives to the Nvidia H100 and the delayed Blackwell chips.Cloud. AWS (disclosure: I serve on Amazon’s board of directors), Google Cloud, and Microsoft Azure make it easy for developers to build.Foundation models. This includes both proprietary models such as OpenAI’s and Anthropic’s, and open weights models such as Meta’s Llama.The foundation model layer frequently appears in headlines because foundation models cost so much to build. Some companies have made massive investments in training these models, and a few of those have added to the hype by pointing out that paying lots for compute and data would lead (probably) to predictably better performance following scaling laws.This layer is also currently hyper-competitive, and switching costs for application developers to move from one model to another are fairly low (for example, requiring changes to just a few lines of code). Sequoia Capital’s thoughtful article on “AI's $600B Question” points out that, to justify massive capital investments in AI infrastructure (particularly GPU purchases and data center buildouts), generative AI needs to get around $600B of revenue. This has made investing at the foundation model layer challenging. It’s expensive, and this sector still needs to figure out how to deliver returns. (I’m cautiously optimistic it will work out!)On top of this layer is an emerging orchestration layer, which provides software that helps coordinate multiple calls to LLMs and perhaps to other APIs. This layer is becoming increasingly agentic. For example, Langchain has helped many developers build LLM applications, and its evolution into LangGraph for building agents has been a great development. Other platforms such as Autogen, MemGPT, and CrewAI (disclosure: I made a personal investment in CrewAI) are also making it easier to build agentic workflows. Switching costs for this layer are much higher than for the foundation model layer, since, if you’ve built an agent on one of these frameworks, it’s a lot of work to switch to a different one. Still, competition in the orchestration layer, as in the foundation model layer, seems intense.Finally, there’s the application layer. Almost by definition, this layer has to do better financially than all the layers below. In fact, for investments at the lower layers to make financial sense, the applications had better generate even more revenue, so the application vendors can afford to pay providers of infrastructure, cloud computing, foundation models, and orchestration. (This is why my team AI Fund focuses primarily on AI application companies, as I discussed in a talk.)Fortunately, because of the massive investments in foundation models, it’s now incredibly inexpensive to experiment and build prototypes in the applications layer! Over Thanksgiving holiday, I spent about one and a half days prototyping different generative AI applications, and my bill for OpenAI API calls came out to about $3. On my personal AWS account, which I use for prototyping and experimentation, my most recent monthly bill was $35.30. I find it amazing how much fun you can have on these platforms for a small number of dollars!By building on widely available AI tools, AI Fund now budgets $55,000 to get to a working prototype. And while that is quite a lot of money, it’s far less than the billions companies are raising to develop foundation models. Individuals and businesses can experiment and test important ideas at reasonable cost.Keep learning!Andrew",
        "summary": "*   Training cutting-edge foundation models is expensive (billions), but building a wide range of AI applications is now very inexpensive.\n*   The AI stack includes layers such as Semiconductors (Nvidia, AMD MI300/MI350), Cloud (AWS, Google Cloud, Microsoft Azure), Foundation models (proprietary/open weights), Orchestration (Langchain, Autogen, MemGPT, CrewAI), and Applications.\n*   The Foundation model layer is hyper-competitive with low developer switching costs, while the emerging Orchestration layer has higher switching costs and intense competition.\n*   Prototyping AI applications is inexpensive, with examples like OpenAI API calls costing ~$3 for 1.5 days, a personal AWS account bill of $35.30/month, and AI Fund budgeting $55,000 for a working prototype.",
        "publish_date": "Dec 04, 2024",
        "keywords": [
            "AMD MI300/MI350",
            "Langchain",
            "Autogen",
            "MemGPT",
            "CrewAI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-falling-cost-of-building-ai-applications/"
    },
    {
        "content": "Dear friends,Happy sum(i**3 for i in range(10)) !Despite having worked on AI since I was a teenager, I’m now more excited than ever about what we can do with it, especially in building AI applications. Sparks are flying in our field, and 2025 will be a great year for building!One aspect of AI that I’m particularly excited about is how easy it is to build software prototypes. AI is lowering the cost of software development and expanding the set of possible applications. While it can help extend or maintain large software systems, it shines particularly in building prototypes and other simple applications quickly.If you want to build an app to print out flash cards for your kids (I just did this in a couple of hours with o1’s help), or write an application that monitors foreign exchange rates to manage international bank accounts (a real example from DeepLearning.AI’s finance team), or analyzes  user reviews automatically to quickly flag problems with your products (DeepLearning.AI's content team does this), it is now possible to build these applications quickly through AI-assisted coding.I find AI-assisted coding especially effective for prototyping because (i) stand-alone prototypes require relatively little context and software integration and (ii) prototypes in alpha testing usually don’t have to be reliable. While generative AI also helps with engineering large, mission-critical software systems, the improvements in productivity there aren't as dramatic, because it’s challenging to give the AI system all the context it needs to navigate a large codebase and also to make sure the generated code is reliable (for example, covering all important corner cases).Until now, a huge friction point for getting a prototype into users’ hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed — sometimes using an AI-enabled IDE like Cursor — and finally assemble the modules.)Building prototypes quickly is an efficient way to test ideas and get tasks done. It’s also a great way to learn. Perhaps most importantly, it’s really fun! (At least I think it is. 😄)How can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will:Make a learning plan! To be effective builders, we all need to keep up with the exciting changes that continue to unfold. How many short courses a month do you want to take in 2025? If you discuss your learning plan with friends, you can help each other along. For instance, we launched a learning summary page that shows what short courses people have taken. A few DeepLearning.AI team members have agreed to a friendly competition to see who can take more courses in 2025!  Go build! If you already know how to code, I encourage you to build prototypes whenever inspiration strikes and you have a spare moment. And if you don’t yet code, it would be well worth your while to learn! Even small wins — like the flash cards I printed out, which inspired my daughter to spend an extra 20 minutes practicing her multiplication table last night — make life better. Perhaps you’ll invent something that really takes off. And even if you don’t, you’ll have fun and learn a lot along the way.Happy New Year! AndrewP.S. I develop mostly in Python. But if you prefer JavaScript: Happy Array.from({ length: 10 }, (_, i) => i ** 3).reduce((a, b) => a + b, 0) !",
        "summary": "*   AI is lowering the cost of software development and expanding application possibilities, particularly for building prototypes quickly.\n*   AI-assisted coding is especially effective for prototyping due to less context/integration required and lower reliability needs in alpha testing, compared to large systems.\n*   Platforms like Bolt, Replit Agent, and Vercel V0 use generative AI with agentic workflows to improve code quality and help deploy generated applications directly.\n*   The author's workflow involves using an LLM to design system architecture and generate code module by module, testing, and editing (sometimes with AI-enabled IDEs like Cursor).",
        "publish_date": "Jan 01, 2025",
        "keywords": [
            "Bolt",
            "Replit Agent",
            "Vercel V0",
            "agentic workflows",
            "AI-enabled IDEs"
        ],
        "url": "https://www.deeplearning.ai/the-batch/new-opportunities-for-the-new-year/"
    },
    {
        "content": "Dear friends,Happy Thanksgiving! In the United States, this is a week when many reflect on their blessings and give thanks. Even as I reflect on how lucky I am to have food, shelter, family, and friends, I think about those who have much less and what we can do to help them.Last week, I spoke with a woman who had been severely physically abused by her husband. She showed me pictures of her face from a few years ago, which had a bloodied sequence of tears down the middle. She also showed me scars left by cigarette burns inflicted by her husband, who told her these burns made her ugly so no other man would ever want her. She is no longer with her husband but continues to struggle. Her phone is badly cracked and barely holds a charge. Without a high-school degree, she has struggled to find a job and is surviving by staying on the couch of a friend. As winter approaches, they keep their place chilly to save the cost of electricity.Working in AI, I am fortunate to interact with many of the smartest and most capable technology and business leaders in the world. But both at home and when I travel, I try to meet with people of a broad range of backgrounds, because ultimately I want to do work that helps people broadly, and this requires that I understand people broadly. When you go to a grocery store and see someone put down a $5 carton of eggs because it is too expensive, and hear them think through how to explain to their kids why they’re skipping eggs that week, it gives you a deeper appreciation for why a $1.50/hour raise can be life-changing for many people.While I can try to help out individuals here and there, technology is advancing rapidly, and this gives me a lot of optimism for the future. Technology remains the best way I know of to help people at scale through providing better education, career guidance, healthcare, personal safety, healthier food, or other things needed to support thriving.I am optimistic about the future because I see so many ways life can be so much better for so many people. I feel blessed that, when my kids or I are cold, we have warm clothing, and when we are hungry, we have a working car to drive to the grocery and buy fresh food. I feel blessed that, rather than using a badly cracked cellphone, I have a modern laptop and a fast internet connection to do my work on.As a child, my father taught me the aphorism “there but for the grace of God go I” to recognize that, in even slightly different circumstances, I might have ended up with much less. Having worked on many software products, I know that, to make good decisions, I have to understand the people I hope to serve. This is why I continue to routinely seek out, speak with, and try to understand people from all walks of life, and I hope many others in AI will do so, too.I see so many people in the AI community building things to make the world better. I am thankful for what the AI community has already done, and I look forward to continuing to build and serve others together.Keep building!Andrew",
        "summary": "*   Working in AI, the author emphasizes the importance for those in AI to understand people from all walks of life to make good decisions and serve them effectively.\n*   The author believes technology, specifically AI, is the best way to help people at scale via applications like education, healthcare, and safety.\n*   The author is thankful for the AI community's work in building things to make the world better.",
        "publish_date": "Nov 27, 2024",
        "keywords": [
            "AI",
            "AI Applications",
            "Societal Impact",
            "Understanding People",
            "AI Community"
        ],
        "url": "https://www.deeplearning.ai/the-batch/building-a-better-future-for-all/"
    },
    {
        "content": "Dear friends,Large language models (LLMs) are typically optimized to answer peoples’ questions. But there is a trend toward models also being optimized to fit into agentic workflows. This will give a huge boost to agentic performance!Following ChatGPT’s breakaway success at answering questions, a lot of LLM development focused on providing a good consumer experience. So LLMs were tuned to answer questions (“Why did Shakespeare write Macbeth?”) or follow human-provided instructions (“Explain why Shakespeare wrote Macbeth”). A large fraction of the datasets for instruction tuning guide models to provide more helpful responses to human-written questions and instructions of the sort one might ask a consumer-facing LLM like those offered by the web interfaces of ChatGPT, Claude, or Gemini.But agentic workloads call on different behaviors. Rather than directly generating responses for consumers, AI software may use a model in part of an iterative workflow to reflect on its own output, use tools, write plans, and collaborate in a multi-agent setting. Major model makers are increasingly optimizing models to be used in AI agents as well.Take tool use (or function calling). If an LLM is asked about the current weather, it won’t be able to derive the information needed from its training data. Instead, it might generate a request for an API call to get that information. Even before GPT-4 natively supported function calls, application developers were already using LLMs to generate function calls, but by writing more complex prompts (such as variations of ReAct prompts) that tell the LLM what functions are available and then have the LLM generate a string that a separate software routine parses (perhaps with regular expressions) to figure out if it wants to call a function. Generating such calls became much more reliable after GPT-4 and then many other models natively supported function calling. Today, LLMs can decide to call functions to search for information for retrieval-augmented generation (RAG), execute code,  send emails, place orders online, and much more.Recently, Anthropic released a version of its model that is capable of computer use, using mouse-clicks and keystrokes to operate a computer (usually a virtual machine). I’ve enjoyed playing with the demo. While other teams have been prompting LLMs to use computers to build a new generation of RPA (robotic process automation) applications, native support for computer use by a major LLM provider is a great step forward. This will help many developers!As agentic workflows mature, here is what I am seeing:First, many developers are prompting LLMs to carry out the agentic behaviors they want. This allows for quick, rich exploration!In a much smaller number of cases, developers who are working on very valuable applications will fine-tune LLMs to carry out particular agentic functions more reliably. For example, even though many LLMs support function calling natively, they do so by taking as input a description of the functions available and then (hopefully) generating output tokens to request the right function call. For mission-critical applications where generating the right function call is important, fine-tuning a model for your application’s specific function calls significantly increases reliability. (But please avoid premature optimization! Today I still see too many teams fine-tuning when they should probably spend more time on prompting before they resort to this.)Finally, when a capability such as tool use or computer use appears valuable to many developers, major LLM providers are building these capabilities directly into their models. Even though OpenAI o1-preview’s advanced reasoning helps consumers, I expect that it will be even more useful for agentic reasoning and planning.Most LLMs have been optimized for answering questions primarily to deliver a good consumer experience, and we’ve been able to “graft” them into complex agentic workflows to build valuable applications. The trend of LLMs built to support particular operations in agents natively will create a lot of lift for agentic performance. I’m confident that we will realize large agentic performance gains in this direction over the next few years.Keep learning!Andrew",
        "summary": "*   LLMs are transitioning from being primarily optimized for consumer question answering to being optimized for agentic workflows involving iterative processes, tool use, and planning.\n*   Agentic workflows require LLMs to use tools (e.g., APIs for search, code execution), with native function calling support (like in GPT-4) improving reliability over prompt-based methods (like ReAct).\n*   Some LLMs are gaining native capabilities for computer use (mouse-clicks, keystrokes) to support applications like RPA.\n*   Developers initially use prompting for agentic behaviors, fine-tune for critical applications, and LLM providers are integrating common agentic capabilities directly into models.\n*   Optimizing LLMs for native support of agentic operations is expected to yield large agentic performance gains.",
        "publish_date": "Nov 13, 2024",
        "keywords": [
            "Agentic workflows",
            "Tool use",
            "Function calling",
            "ReAct",
            "GPT-4"
        ],
        "url": "https://www.deeplearning.ai/the-batch/from-optimizing-for-people-to-optimizing-for-machines/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Using AI-assisted coding to build software prototypes is an important way to quickly explore many ideas and invent new things. In this and future letters, I’d like to share with you some best practices for prototyping simple web apps. This letter will focus on one idea: being opinionated about the software stack.The software stack I personally use changes every few weeks. There are many good alternatives to these choices, and if you pick a preferred software stack and become familiar with its components, you’ll be able to develop more quickly. But as an illustration, here’s my current default: Python with FastAPI for building web-hosted APIs: I develop primarily in Python, so that’s a natural choice for me. If you’re a JavaScript/TypeScript developer, you’ll likely make a different choice. I’ve found FastAPI really easy to use and scalable for deploying web services (APIs) hosted in Python. Uvicorn to run the backend application server (to execute code and serve web pages) for local testing on my laptop. If deploying on the cloud, then either Heroku for small apps or Amazon Web Services Elastic Beanstalk for larger ones (disclosure: I serve on Amazon’s board of directors): There are many services for deploying jobs, including HuggingFace Spaces, Railway, Google’s Firebase, Vercel, and others. Many of these work fine, and becoming familiar with just 1 or 2 will simplify your development process.MongoDB for NoSQL database: While traditional SQL databases are amazing feats of engineering that result in highly efficient and reliable data storage, the need to define the database structure (or schema) slows down prototyping. If you really need speed and ease of implementation, then dumping most of your data into a NoSQL (unstructured or semi-structured) database such as MongoDB lets you write code quickly and sort out later exactly what you want to do with the data. This is sometimes called schema-on-write, as opposed to schema-on-read. Mind you, if an application goes to scaled production, there are many use cases where a more structured SQL database is significantly more reliable and scalable.OpenAI’s o1 and Anthropic’s Claude 3.5 Sonnet for coding assistance, often by prompting directly (when operating at the conceptual/design level). Also occasionally Cursor (when operating at the code level): I hope never to have to code again without AI assistance! Claude 3.5 Sonnet is widely regarded as one of the best coding models. And o1 is incredible at planning and building more complex software modules, but you do have to learn to prompt it differently.On top of all this, of course, I use many AI tools to manage agentic workflows, data ingestion, retrieval augmented generation, and so on. DeepLearning.AI and our wonderful partners offer courses on many of these tools.My personal software stack continues to evolve regularly. Components enter or fall out of my default stack every few weeks as I learn new ways to do things. So please don’t feel obliged to use the components I do, but perhaps some of them can be a helpful starting point if you are still deciding what to use. Interestingly, I have found most LLMs not very good at recommending a software stack. I suspect their training sets include too much “hype” on specific choices, so I don’t fully trust them to tell me what to use. And if you can be opinionated and give your LLM directions on the software stack you want it to build on, I think you’ll get better results.A lot of the software stack is still maturing, and I think many of these components will continue to improve. With my stack, I regularly build prototypes in hours that, without AI assistance, would have taken me days or longer. I hope you, too, will have fun building many prototypes!Keep learning,Andrew",
        "summary": "*   AI-assisted coding is used for rapid software prototyping, enabling building prototypes in hours that would take days or longer without it.\n*   Specific AI models like OpenAI's o1 and Anthropic's Claude 3.5 Sonnet are employed for coding assistance at different levels (conceptual/design, code).\n*   Additional AI tools are utilized for managing agentic workflows, data ingestion, and retrieval augmented generation (RAG).\n*   A NoSQL database like MongoDB is chosen for prototyping speed due to schema flexibility (schema-on-write), contrasting with SQL's schema definition requirement.\n*   LLMs are found to be ineffective at recommending software stacks, suggesting users provide opinionated directions when prompting them for this purpose.",
        "publish_date": "Jan 08, 2025",
        "keywords": [
            "OpenAI's o1",
            "Anthropic's Claude 3.5 Sonnet",
            "retrieval augmented generation (RAG)",
            "MongoDB",
            "agentic workflows"
        ],
        "url": "https://www.deeplearning.ai/the-batch/my-ai-assisted-software-development-stack/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn’t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other.Much has been written about why high, widespread taxes on imports are harmful. In this letter, I’d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is.However, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI.With regard to data-center buildouts, another silver lining is that, with the rise of generative AI, data gravity has decreased because compute processing costs are much greater than transmission costs, meaning it’s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally.Finally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vance pointed out in 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel — or know-how, or supply chain — to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI’s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small.My 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit — he was proud that he’s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I’ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids.I don’t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let’s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas — including specifically open source software — and keep supporting each other. Let’s all do what we can to keep the world as connected as we are able.Love,Andrew",
        "summary": "* Tariffs focus on physical imports, not digital goods like AI research IP and software (including open source), allowing the free flow of these ideas across borders.\n* Tariffs on hardware (power generation, servers, consumer electronics) will slow AI progress by increasing costs for data centers and making AI tools less accessible.\n* The rise of generative AI has decreased data gravity, potentially encouraging global data center buildouts despite tariffs.\n* Tariffs might create mild tailwinds for robotics and industrial automation in domestic manufacturing, though AI progress in robotics is slower than in other domains.",
        "publish_date": "Apr 09, 2025",
        "keywords": [
            "generative AI",
            "robotics",
            "industrial automation",
            "data centers",
            "open source"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-impact-of-u-s-tariffs-on-ai/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it’s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I’d like to share when and how I use “lazy prompting.”When debugging code, many developers copy-paste error messages — sometimes pages of them — into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don’t need to explicitly tell them. With brief instructions like “Edit this: …” or “sample dotenv code” (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text.At the other end of the spectrum, sometimes  I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer.I don’t try a lazy prompt if (i) I feel confident there’s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI’s Agentic Doc Extraction!), I should say so in the prompt, since otherwise it’s very hard for the LLM to guess my preference. I also wouldn’t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want.By the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you’ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM’s web or app interface It doesn’t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won’t be examining every output to clarify and iterate if the output is poor.Thank you to Rohit Prsad, who has been collaborating with me on the open-source package aisuite, for suggesting the term lazy prompting. There is an analogy to lazy evaluation in computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed.Keep building!Andrew",
        "summary": "*   \"Lazy prompting\" is presented as an alternative to standard detailed prompting, involving quick, imprecise prompts where the key is quickly assessing output quality to decide if more context is needed.\n*   It can be useful for tasks like debugging code by copy-pasting error messages or generating simple code with brief instructions.\n*   Lazy prompting is not recommended when the LLM has no chance of providing a good solution without additional context or when a buggy implementation would take a long time to detect.\n*   This technique is described as an advanced method for users who already know how to provide enough context, applicable when iterating quickly using an LLM's web or app interface, not for API calls in code.",
        "publish_date": "Apr 02, 2025",
        "keywords": [
            "Lazy prompting",
            "standard detailed prompting",
            "LLM",
            "API calls",
            "web or app interface"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-benefits-of-lazy-prompting/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out days after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event.I'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so.Based on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you!Other aspects of the event that struck me:First, agentic AI continues to be a strong theme. The topic attendees most wanted to hear about (based on free text responses to our in-person survey at the start of the event) was agents!Google's Paige Bailey talked about embedding AI in everything and using a wide range of models to do so. I also particularly enjoyed her demos of Astra and Deep Research agents.Meta's Amit Sangani talked compellingly as usual about open models. Specifically, he described developers fine-tuning smaller models on specific data, resulting in superior performance than with large general purpose models. While there're still many companies using fine-tuning that should really just be prompting, I'm also seeing continued growth of fine-tuning in applications that are reaching scale and that are becoming valuable.Many speakers also spoke about the importance of being pragmatic about what problems we are solving, as opposed to buying into the AGI hype. For example, Nebius' Roman Chernin put it simply: Focusing on solving real problems is important! Lastly, I was excited to hear continued enthusiasm for the Voice Stack. Justin Uberti gave a talk about OpenAI’s realtime audio API to a packed room, with many people pulling out laptops to try things out themselves in code! DeepLearning.AI has a strong “Learner First” mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions.I'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future.Keep building!AndrewP.S. I'm thrilled to share our newest course series: the Data Analytics Professional Certificate! Data analytics remains one of the core skills of data science and AI, and this professional certificate takes you up to being job-ready for this. Led by Netflix data science leader Sean Barnes, this certificate gives you hands-on experience with essential tools like SQL, Tableau, and Python, while teaching you to use Generative AI effectively as a thought partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities at every level—I’m excited to see where they take you! Sign up here!",
        "summary": "*   AI Dev 25 was held as a vendor-neutral conference for AI developers to share ideas and build with available tools.\n*   Key themes included agentic AI (most wanted topic), embedding AI with various models (e.g., Astra, Deep Research agents), and pragmatic problem-solving.\n*   Discussions covered open models and fine-tuning smaller models on specific data for superior performance, noting growth in scaled applications.\n*   Enthusiasm was shown for the Voice Stack, specifically OpenAI’s realtime audio API.\n*   A new Data Analytics Professional Certificate was announced, covering core data science/AI skills (SQL, Tableau, Python) and Generative AI, citing a projected 36% growth in data science jobs by 2033.",
        "publish_date": "Mar 19, 2025",
        "keywords": [
            "agentic AI",
            "embedding AI",
            "OpenAI’s realtime audio API",
            "fine-tuning",
            "Astra"
        ],
        "url": "https://www.deeplearning.ai/the-batch/lessons-from-our-first-ai-dev-conference/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.As coding becomes easier, more people should code, not fewer!Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, “Build Apps with Windsurf’s AI Coding Agents.”)I wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals — individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.One question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that.When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.Keep building!Andrew",
        "summary": "* The author disagrees with the view that AI will automate programming into extinction, arguing that AI-assisted coding makes it easier and more important to learn.\n* AI-assisted coding tools, including what some call \"vibe coding,\" are making programming increasingly easier.\n* Learning coding provides the \"language of software\" necessary to effectively direct AI tools like LLMs and AI-enabled IDEs for precise results.\n* The ability to tell a computer exactly what you want, through coding or directing AI, is presented as a vital future skill to address job displacement concerns.",
        "publish_date": "Mar 12, 2025",
        "keywords": [
            "AI-assisted coding",
            "vibe coding",
            "LLMs",
            "AI-enabled IDEs",
            "Directing AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/learn-the-language-of-software/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Fine-tuning small language models has been gaining traction over the past half year. I’d like to share my sense of when to use this technique, and also when not to, based on what I’m seeing in multiple companies.First, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writing mega prompts), few-shot prompting, or simple agentic workflows.Why shouldn’t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task.Having said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I’ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully:Improving accuracy of critical applications. Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims.Learning a particular  style of communication. As I explain in “Generative AI for Everyone,” my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone.Reducing latency or cost during scale-ups. I’ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task.At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which  I think RAG would work better.Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal.It is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated.In conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it’s a critical piece of a small minority of them.Keep learning!Andrew",
        "summary": "*   Fine-tuning small language models (e.g., 13B or fewer parameters), especially with methods like LoRA, is becoming affordable and can require 100 or fewer examples, but is often more complex than simpler methods like prompting or agentic workflows (~75% of cases).\n*   Fine-tuning is valuable for specific applications where simpler methods fall short, such as improving critical task accuracy (e.g., raising 95% to 99% for API calls or text classification), learning specific communication styles, or enabling smaller models to perform like larger ones for latency/cost reduction.\n*   For enabling LLMs to understand new knowledge, Retrieval Augmented Generation (RAG) is generally a simpler and preferred alternative to fine-tuning.",
        "publish_date": "Mar 26, 2025",
        "keywords": [
            "LoRA",
            "Retrieval Augmented Generation (RAG)",
            "fine-tuning",
            "prompting",
            "agentic workflows"
        ],
        "url": "https://www.deeplearning.ai/the-batch/when-to-fine-tune-and-when-not-to/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,I’m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity.AI Fund isn’t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): We co-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices.Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles in The Batch as well.If you are building an AI startup, here are some ideas to consider:A startup with a small team that pursues one focused, concrete idea can move really fast. Rather than hedging, it is often better to pursue one hypothesis (for example, build one concrete product) but also be willing to switch quickly to a different hypothesis (say, change what features you decide to build) if the data that comes back indicates the original hypothesis is flawed. Concreteness gets you speed!A subject matter expert’s gut is remarkably good at making quick decisions. Obviously, there’s a role for data and user studies as well. But if you’re deciding whether to build feature A or B, or to sell first to user persona X or Y, sometimes a domain expert’s gut will point to a quick decision that you can execute and validate or falsify. Trusting a domain expert’s gut gets you speed!AI-assisted coding is making prototyping faster than ever before. Yes, AI assistance is speeding up building reliable, enterprise-grade applications and maintaining legacy codebases. But the acceleration it brings to building stand-alone prototypes is far greater. This is because stand-alone prototypes have low requirements for reliability, integration, or even security (if, say, you run them in a sandbox environment). This lets us prototype and test at a ferocious velocity. AI -assisted coding (including vibe coding, where you might barely look at the code) gets you speed!Finally, with faster prototyping, the bottleneck shifts to getting feedback from users. A single learning cycle might consist of (i) building a prototype and (ii) getting user feedback to inform the next iteration. Since (i) is now much faster than before, accelerating (ii) is growing in importance. This means teams that are skilled at finding prospective customers and getting their feedback in hours/days rather than weeks can go faster. For example, when building consumer products, I routinely approach strangers (in a respectful way) in public places to ask if they’re willing to give feedback on a prototype I’m working on. (Gathering feedback is more complex for enterprise products, because prospective customers are harder to track down.) Quick user feedback gets you speed!In addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed!I’m grateful to AI Fund’s investors, team, and entrepreneur partners for working with us. There is much ahead to build!Andrew",
        "summary": "*   AI Fund closed $190M for a new fund, operating as a venture builder/studio that co-founds AI companies and is directly involved in building.\n*   Speed is critical for startup success, with AI-assisted coding significantly accelerating prototyping, particularly for stand-alone prototypes.\n*   Deep knowledge of AI technology is important for startup success, enabling better decisions and speed.\n*   Accelerating user feedback is crucial as prototyping speeds up, shifting the bottleneck in the learning cycle.",
        "publish_date": "May 7, 2025",
        "keywords": [
            "AI Fund",
            "AI-assisted coding",
            "AI technology",
            "AI companies",
            "Artificial Intelligence"
        ],
        "url": "https://www.deeplearning.ai/the-batch/hot-tips-for-speedy-startups/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Everyone can benefit by learning to code with AI! At AI Fund, the venture studio I lead, everyone — not just the engineers — can vibe code or use more sophisticated AI-assisted coding techniques. This empowers everyone to build with AI. The impact on team creativity and productivity has been exciting! I share my experience with this in the hope that more teams will invest in empowering everyone to build with AI.Everyone at AI Fund who was not already an engineer started with our “AI Python for Beginners” course to learn the basics. I also shared with the team details of the tech stack I use to give everyone a default set of building blocks. Since then, many have gone on to acquire additional building blocks (such as additional third-party APIs) themselves either by taking courses, searching online, or learning from colleagues.You can watch a video of our experience with this here.Here are just a few examples of applications that non-engineers at AI Fund have built: Our CFO Ellen Li built an app that scans our Google docs system to flag updates to a portfolio company’s information, saving what was previously 5 to 6 hours of manual work per week.Senior Executive Recruiter Jon Zemmelman built a system that lets him configure the relative ratings of screening criteria for job candidates (such as previous startup experience, technical expertise, etc.) and automatically evaluate resumes against the criteria.Associate General Counsel Nikhil Sharma wrote code to automatically generate NDAs (non-disclosure agreements) in AI Fund’s standard template.Office Coordinator Ellie Jenkins, as a fun project, built a visualization of the history of fashion design houses and their influence on each other.It is very empowering when individuals don’t have to try to get scarce engineering resources allocated to their ideas in order to try them out. There are a lot fewer gatekeepers in the way: If someone has an idea, they can build a prototype and try it out. If it gets positive feedback from users, that lays the groundwork for scaling it up. Or, if the prototype does not work, this is also valuable information that lets them quickly move on to a different idea or take insights from critical feedback to decide what to try next.In the future, one of the most important skills in any profession will be the ability to tell a computer exactly what you want, so the computer can do it for you. For the foreseeable future, writing code (with AI assistance, so the AI, rather than you, actually writes the code) will be the best way to do this.This is a great time for everyone to code with AI!Keep building,Andrew",
        "summary": "*   AI Fund promotes AI-assisted coding for all employees, not just engineers, to enhance team creativity and productivity.\n*   Non-engineers at AI Fund learned basics via an \"AI Python for Beginners\" course and built applications using AI-assisted techniques.\n*   Examples of applications built by non-engineers include an app saving 5-6 hours/week flagging document updates, a system for evaluating resumes, and code for generating NDAs.\n*   AI-assisted coding empowers individuals to prototype ideas quickly without relying on scarce engineering resources.\n*   The ability to instruct computers via AI-assisted coding is highlighted as a crucial future skill for any profession.",
        "publish_date": "Jun 4, 2025",
        "keywords": [
            "AI-assisted coding",
            "Applications",
            "Code",
            "Prototyping",
            "Python"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-ai-fund-is-building-ai-builders/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Even though I’m a much better Python than JavaScript developer, with AI assistance, I’ve been writing a lot of JavaScript code recently. AI-assisted coding is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we’re not familiar with, which lets us get code working in many more contexts!My background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being “Python developers\" or “C++ developers,” many more of us will just be “developers”!But understanding the concepts behind different languages is still important. That’s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax — say, from JS to TS, or C++ to Java, or Rust to Go — once you’ve learned the first set of concepts, you’ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!)  In addition, you’ll be able to understand much of the generated code (perhaps with a little LLM assistance).Different programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages.Similarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory.Just as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven’t already done so, I encourage you to try having an LLM write some code in a language you’d like to learn but perhaps haven’t yet gotten around to, and see if it helps you get some new applications to work.Keep building!Andrew",
        "summary": "*   AI assistance, particularly generative AI, enables developers to write code in programming languages they are not familiar with, making specific languages less important.\n*   Understanding core programming concepts (e.g., arrays, dictionaries, caches, memory, UI components, DOM, GPU compute/memory organization) remains crucial for effective prompting of LLMs and understanding generated code.\n*   LLMs facilitate writing code across various languages and frameworks, including Python, JS, TS, C++, Java, Cobol, TensorFlow, PyTorch, React, CUDA, and ROCm.",
        "publish_date": "Apr 23, 2025",
        "keywords": [
            "TensorFlow",
            "PyTorch",
            "CUDA",
            "ROCm",
            "GPU compute/memory organization"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-become-a-multilingual-coder/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,In the age of AI, large corporations — not just startups — can move fast. I often speak with large companies’ C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don’t need permission to innovate. Let me explain.Large companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they’re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product?Thanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies’ processes – designed to protect against legitimate downside risks – make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable.Fortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone’s permission.The sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute.Within this sandbox, there can be broad scope for experimentation, and — importantly — a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company’s brand, and so on.Under this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs.Importantly, this also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones.I often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I’m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters.Keep building!Andrew",
        "summary": "* AI-assisted coding enables rapid software prototyping, but large company processes designed to mitigate risk often prevent leveraging this speed.\n* Large companies can create \"sandbox\" environments with limited downside risk, allowing teams to experiment with prototypes faster without needing frequent permission.\n* This sandbox approach facilitates building and discarding numerous prototypes cheaply to discover valuable ideas, fostering a culture of experimentation.\n* The author's checklist for AI strategy includes people, process (emphasized here for speed), and platform.",
        "publish_date": "May 21, 2025",
        "keywords": [
            "AI-assisted coding",
            "sandbox environments",
            "rapid software prototyping",
            "process",
            "AI strategy"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-large-companies-can-move-fast-in-ai/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now — with help from AI — he not only writes code, he also teaches CS. I found Kyle’s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels.Kyle’s success has been with the support of Kira Learning (an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery — educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers — so the teacher can focus on social-emotional support. While these are still early days, it appears to be working!A key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of the flipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python codebest_$alty_snack = 'potato chips'Kira Learning’s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like “Can you identify what characters are allowed in variable names?” Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12.Additionally, agentic workflows can automate a lot of teachers’ repetitive tasks. For example, when designing a curriculum, it’s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers.Since learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players’ attempts to shoot three-pointers (shown above), which in turn is affecting the team’s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach!I talked about Kyle (and other topics) at the ASU+GSV Summit on education. You can see a video here.In the future, people who know how to code and build with AI will be much more productive than people who don’t. I’m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI.Keep learning!Andrew",
        "summary": "*   AI-enabled coding education is proposed for K-12 students to empower them and increase productivity.\n*   AI helps address the shortage of CS teachers by enabling individuals with no prior coding experience (like Kyle Creasy) to teach CS.\n*   Kira Learning utilizes AI for digital content delivery (autograded quizzes, AI chatbots) and hyperpersonalization, such as spotting buggy Python code (`$`) and suggesting teacher prompts.\n*   AI agentic workflows automate repetitive teacher tasks, including aligning curriculum content to educational standards (Common Core, AP CS).",
        "publish_date": "Apr 30, 2025",
        "keywords": [
            "AI chatbots",
            "AI agentic workflows",
            "Spotting buggy Python code",
            "Aligning curriculum content",
            "Hyperpersonalization"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-ai-revolution-comes-to-grade-school-classrooms/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done.If not for funding for my early work in deep learning from the National Science Foundation (NSF)  and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. — and also the world — to miss out on the next set of ideas.In fact, such funding benefits the U.S. more than any other nation.  Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation.Why does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area — Google Brain, which invented the transformer network, and OpenAI, which scaled it up — did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children’s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies.In a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work.Thus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technology points out, “... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.”Further, generative AI is evolving so rapidly that staying on the cutting edge is what’s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what’s most valuable. Studies like this one (albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally.China was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China’s tech ecosystem is very open internally, and this has helped it to catch up over the past two years:There is ample funding for open academic research in China.China’s businesses such as DeepSeek and Alibaba have released cutting-edge, open-weights models. This openness at the corporate level accelerates diffusion of knowledge.China’s labor laws make non-compete agreements (which stop an employee from jumping ship to a competitor) relatively hard to enforce, and the work culture supports significant idea sharing among employees of different companies; this has made circulation of ideas relatively efficient.While there’s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate.In 1945, Vannevar Bush’s landmark report “Science, The Endless Frontier” laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S.The good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research.Andrew",
        "summary": "* Proposed cuts to U.S. basic research funding are viewed as detrimental to U.S. competitiveness in AI.\n* Funding from agencies like NSF and DARPA is credited with enabling foundational AI work, including deep learning scaling and the invention of the transformer network.\n* Openly shared research benefits the host nation most by accelerating knowledge diffusion and talent creation, exemplified by generative AI innovation in Silicon Valley.\n* China's progress in generative AI since 2022 is attributed partly to its open tech ecosystem, including funding for open academic research and businesses releasing open-weights models.\n* Historical U.S. federal funding for science, guided by principles like those in the 1945 \"Science, The Endless Frontier\" report, fostered scientific dominance and breakthroughs, benefiting the U.S. and training talent.",
        "publish_date": "May 28, 2025",
        "keywords": [
            "transformer network",
            "open-weights models",
            "deep learning scaling",
            "generative AI",
            "deep learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/cut-research-funding-weaken-the-nation/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,I’ve noticed that many GenAI application projects put in automated evaluations (evals) of the system’s output probably later — and rely on humans to manually examine and judge outputs longer — than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there’s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It’s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals.I wrote previously about the importance and difficulty of creating evals. Say you’re building a customer-service chatbot that responds to users in free text. There’s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals.I encourage you to approach building evals differently. It’s okay to build quick evals that are only partial, incomplete, and noisy measures of the system’s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals’ output and human judgments. For example:It’s okay to start with very few examples in the eval set, say 5, and gradually add to them over time — or subtract them if you find that some examples are too easy or too hard, and not useful for distinguishing between the performance of different versions of your system.It’s okay to start with evals that measure only a subset of the dimensions of performance you care about, or measure narrow cues that you believe are correlated with, but don’t fully capture, system performance. For example if, at a certain moment in the conversation, your customer-support agent is supposed to (i) call an API to issue a refund and (ii) generate an appropriate message to the user, you might start off measuring only whether or not it calls the API correctly and not worry about the message. Or if, at a certain moment, your chatbot should recommend a specific product, a basic eval could measure whether or not the chatbot mentions that product without worrying about what it says about it.So long as the output of the evals correlates with overall performance, it’s fine to measure only a subset of things you care about when starting.The development process thus comprises two iterative loops, which you might execute in parallel:Iterating on the system to make it perform better, as measured by a combination of automated evals and human judgment;Iterating on the evals to make them correspond more closely to human judgment.As with many things in AI, we often don’t get it right the first time. So t’s better to build an initial end-to-end system quickly and then iterate to improve it. We’re used to taking this approach to building AI systems. We can build evals the same way.To me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B: If A works significantly better than B according to a skilled human judge, the eval should give A a significantly higher score than B.If A and B have similar performance, their eval scores should be similar.Whenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in “error” and we should tweak it to make it rank A and B correctly. This is a similar philosophy to error analysis in building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output — such as when it outputs an incorrect label — we focus on “errors” of the evals — such as when they incorrectly rank two systems A and B, so the evals aren’t helpful in choosing between them.Relying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress. Keep building!Andrew",
        "summary": "* Many GenAI projects delay automated evaluations (evals), relying on manual human judgment longer than necessary due to the perceived large up-front investment required.\n* It is recommended to build evals iteratively, starting with a quick implementation (e.g., 5 examples) and gradually improving over time.\n* This iterative approach allows gradually shifting the evaluation burden from humans to automated evals and enables parallel iteration on both the system and the evals.\n* A successful eval should rank system performance (e.g., system A vs B) similarly to a skilled human judge, treating discrepancies as \"errors\" in the eval itself.",
        "publish_date": "Apr 16, 2025",
        "keywords": [
            "Automated evaluations (evals)",
            "Manual human judgment",
            "Iterative approach",
            "System performance",
            "Ranking"
        ],
        "url": "https://www.deeplearning.ai/the-batch/we-iterate-on-models-we-can-iterate-on-evals-too/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,AI’s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value.For the task of writing code, AI is a game-changer. It takes so much less effort — and is so much cheaper — to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabled speed rather than AI-enabled cost reduction.That AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value.I see this pattern across more and more businesses. Consider the following scenarios:If a lender can approve loans in minutes using AI, rather than days waiting for a human to review them, this creates more borrowing opportunities (and also lets the lender deploy its capital faster). Even if human-in-the-loop review is needed, using AI to get the most important information to the reviewer might speed things up. The ability to provide loans quickly opens up the market to new customers in need of rapid funds and helps customers who need a quick positive or negative decision to accept the loan or move on.If an academic institution gives homework feedback to students in minutes (via sophisticated autograding) rather than days (via human grading), not only is the automation cheaper, the rapid feedback facilitates better learning.If an online seller can approve purchases faster, this can lead to more sales. For example, many platforms that accept online ad purchases have an approval process that can take hours or days; if approvals can be done faster, they can earn revenue faster. Further, for customers buying ads, being able to post an ad in minutes lets them test ideas faster and also makes the ad product more valuable.If a company’s sales department can prioritize leads and respond to prospective customers in minutes or hours rather than days — closer to when the customers’ buying intent first led them to contact the company — sales representatives might close more deals. Likewise, a business that can respond more quickly to requests for proposals may win more deals.I’ve written previously about looking at the tasks a company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you’re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth.Growth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth.Keep building!Andrew",
        "summary": "* AI's ability to make tasks faster is an underrated driver of business value, distinct from cost reduction.\n* For writing code, AI reduces effort and cost but primarily shortens the time from idea to working prototype, enabling faster testing and invention.\n* AI-enabled speed creates new value and growth, exemplified by faster loan approvals, rapid homework feedback, quicker online ad approvals, and faster sales lead responses.\n* Businesses should identify tasks, especially those in the revenue path, where AI can significantly increase speed to unlock growth.",
        "publish_date": "May 14, 2025",
        "keywords": [
            "AI",
            "Speed",
            "Code Writing",
            "Loan Approvals",
            "Sales Lead Responses"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-value-of-ais-speed-is-underrated/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,Continuing our discussion on the Voice Stack, I’d like to explore an area that today’s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication.When communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm.A key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user’s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI’s output, and let the user take a turn. This works reasonably well in quiet environments.However, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD — which is usually trained to detect human speech — tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you’ve stopped talking. This is why today’s speech applications often struggle in noisy environments.Intriguingly, last year, Kyutai Labs published Moshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user.If you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we’d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It’s listening all the time, and it’s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an “inner monologue” that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.)Just as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of “reasoning tokens” before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I’m confident we’ll see many more good voice models released this year.It feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in.Keep building!Andrew",
        "summary": "*   Current voice-based systems often use a turn-taking paradigm based on Voice Activity Detection (VAD), influenced by text chatbots.\n*   VAD systems struggle in noisy environments, especially with background human speech, leading to inaccuracies in turn detection.\n*   Kyutai Labs' Moshi model introduced persistent bi-directional audio streams, allowing the model to manage turns and eliminating the need for explicit VAD.\n*   Voice models are undergoing architectural explorations, similar to text transformers, with significant investment in developing better voice foundation models.",
        "publish_date": "Mar 05, 2025",
        "keywords": [
            "Voice Activity Detection (VAD)",
            "Moshi model",
            "Persistent bi-directional audio streams",
            "Text transformers",
            "Voice foundation models"
        ],
        "url": "https://www.deeplearning.ai/the-batch/wait-your-turn-conversation-by-voice-versus-text/"
    },
    {
        "content": "Loading the Elevenlabs Text to Speech AudioNative Player...\n\nDear friends,The Voice Stack is improving rapidly. Systems that interact with users via speaking and listening will drive many new applications. Over the past year, I’ve been working closely with DeepLearning.AI, AI Fund, and several collaborators on voice-based applications, and I will share best practices I’ve learned in this and future letters.Foundation models that are trained to directly input, and often also directly generate, audio have contributed to this growth, but they are only part of the story. OpenAI’s RealTime API makes it easy for developers to write prompts to develop systems that deliver voice-in, voice-out experiences. This is great for building quick-and-dirty prototypes, and it also works well for low-stakes conversations where making an occasional mistake is okay. I encourage you to try it!However, compared to text-based generation, it is still hard to control the output of voice-in voice-out models. In contrast to directly generating audio, when we use an LLM to generate text, we have many tools for building guardrails, and we can double-check the output before showing it to users. We can also use sophisticated agentic reasoning workflows to compute high-quality outputs. Before a customer-service agent shows a user the message, “Sure, I’m happy to issue a refund,” we can make sure that (i) issuing the refund is consistent with our business policy and (ii) we will call the API to issue the refund (and not just promise a refund without issuing it).In contrast, the tools to prevent a voice-in, voice-out model from making such mistakes are much less mature.In my experience, the reasoning capability of voice models also seems inferior to text-based models, and they give less sophisticated answers. (Perhaps this is because voice responses have to be more brief, leaving less room for chain-of-thought reasoning to get to a more thoughtful answer.)When building applications where I need a high degree of control over the output, I use agentic workflows to reason at length about the user’s input. In voice applications, this means I end up using a pipeline that includes speech-to-text (STT, also known as ASR, or automatic speech recognition) to transcribe the user’s words, then processes the text using one or more LLM calls, and finally returns an audio response to the user via TTS (text-to-speech). This STT → LLM/Agentic workflow → TTS pipeline, where the reasoning is done in text, allows for more accurate responses.However, this process introduces latency, and users of voice applications are very sensitive to latency. When DeepLearning.AI worked with RealAvatar (an AI Fund portfolio company led by Jeff Daniel) to build an avatar of me, we found that getting TTS to generate a voice that sounded like me was not very hard, but getting it to respond to questions using words similar to those I would choose was. Even after a year of tuning our system — starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows — it remains a work in progress. You can play with it here.Initially, this agentic workflow incurred 5-9 seconds of latency, and having users wait that long for responses led to a bad experience. To address this, we came up with the following latency reduction technique. The system quickly generates a pre-response (short for preliminary response) that can be uttered quickly, which buys time for an agentic workflow to generate a more thoughtful, full response. (We’re grateful to LiveKit’s CEO Russ d’Sa and team for helping us get this working.) This is similar to how, if you were to ask me a complicated question, I might say “Hmm, let me think about that” or “Sure, I can help with that” — that’s the pre-response — while thinking about what my full response might be.I think generating a pre-response followed by a full response, to quickly acknowledge the user’s query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and DeepLearning.AI, through our efforts on the pre-response and other optimizations, have reduced the system’s latency to around 0.5-1 seconds.Months ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT → LLM → TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize!Building reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you’ll start building prototypes and see how far you can get! I’ll keep building voice applications and sharing best practices and voice-related technology trends in future letters.Keep building!Andrew",
        "summary": "* Direct voice-in/voice-out models are challenging to control and reason with compared to text-based models.\n* A common pipeline for controlled voice applications is STT/ASR → LLM/Agentic workflow (text reasoning) → TTS.\n* This pipeline introduces latency, a critical issue for voice applications.\n* Generating a quick \"pre-response\" while computing the \"full response\" is a technique to reduce perceived latency.\n* Using this technique and optimizations, system latency was reduced to around 0.5-1 seconds, nearing human conversational latency (0.3-1 seconds).",
        "publish_date": "Feb 26, 2025",
        "keywords": [
            "STT",
            "LLM",
            "Agentic workflow",
            "TTS",
            "pre-response"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-ive-learned-building-voice-applications/"
    },
    {
        "content": "Dear friends,I’m delighted to announce AI Python for Beginners, a sequence of free short courses that teach anyone to code, regardless of background. I’m teaching this introductory course to help beginners take advantage of powerful trends that are reshaping computer programming. It’s designed for people in any field — be it marketing, finance, journalism, administration, or something else — who can be more productive and creative with a little coding knowledge, as well as those who aspire to become software developers. Two of the four courses are available now, and the remaining two will be released in September. Generative AI is transforming coding in two ways:Programs are using AI: Previously, you had to learn a lot about coding before it became useful. Now, knowing how to write code that calls large language models (and other AI APIs) makes it possible to build powerful programs more easily. This is increasing the value of coding. AI is helping programmers: Programmers are using large language models as coding companions that write pieces of code, explain coding concepts, find bugs, and the like. This is especially helpful for beginners, and it lowers the effort needed to learn to code. The combination of these two factors means that novices can learn to do useful things with code far faster than they could have a year ago. These courses teach coding in a way that is aligned with these trends: (i) We teach how to write code to use AI to carry out tasks, and (ii) Unlike some instructors who are still debating how to restrict the use of ChatGPT, we embrace generative AI as a coding companion and show how to use it to accelerate your learning. To explain these two trends in detail:Programs are using AI. Because programs can now take advantage of AI, increasingly knowing a little bit about how to code helps people in roles other than software engineers do their work better. For example, I’ve seen a marketing professional write code to download web pages and use generative AI to derive insights; a reporter write code to flag important stories; and an investor automate first drafts of contracts. Even if your goal is not to become a professional developer, learning just a little coding can be incredibly useful! In the courses, you’ll use code to write personalized notes to friends, brainstorm recipes, manage to-do lists, and more.AI is helping programmers. There is a growing body of evidence that AI is making programming easier. For example:A study at Cisco by Pandey et al. projects a “33-36% time reduction for coding-related tasks” for many cloud development tasks.McKinsey estimates a 35 percent to 45 percent reduction in time needed for code generation tasks. In study by Microsoft (which owns Github and sells Github Copilot), Github, and MIT, developers who used AI completed a programming task nearly 56 percent faster.  Further, as AI tools get better — for example, as coding agents continue to improve and can write simple programs more autonomously — these productivity gains will improve.In order to help learners skate to where the puck is going, this course features a built in chatbot and teaches best practices for how beginners can use a large language model to explain, write, and debug code and explain programming concepts. AI is already helping experienced programmers, and it will help beginner programmers much more.If you know someone who is curious about coding (or if you yourself are), please encourage them to learn to code! The case is stronger than ever that pretty much everyone can benefit from learning at least a little coding. Please help me spread the word, and encourage everyone who isn’t already a coder to check out AI Python for Beginners. Andrew",
        "summary": "*   Generative AI is transforming coding by enabling programs to use AI via large language models (LLMs)/APIs and by helping programmers as coding companions.\n*   Knowing how to write code calling large language models (and other AI APIs) makes it possible to build powerful programs more easily, increasing the value of coding for various roles.\n*   AI helps programmers as coding companions (writing code, explaining concepts, finding bugs), with studies projecting significant time reductions for coding tasks (e.g., \"33-36%\", 35% to 45%, nearly 56% faster).\n*   The \"AI Python for Beginners\" course teaches how to write code to use AI and embraces generative AI as a coding companion to accelerate beginner learning.",
        "publish_date": "Aug 07, 2024",
        "keywords": [
            "Coding companions",
            "AI APIs",
            "Writing code",
            "Explaining concepts",
            "Finding bugs"
        ],
        "url": "https://www.deeplearning.ai/the-batch/learning-to-code-is-easier-than-ever/"
    },
    {
        "content": "Dear friends,We won! California’s anti-innovation bill SB 1047 was vetoed by Governor Newsom over the weekend. Open source came closer to taking a major blow than many people realize, and I’m grateful to the experts, engineers, and activists who worked hard to combat this bill.The fight to protect open source is not yet over, and we have to continue our work to make sure regulations are based on science, not science-fiction.As I  wrote previously, SB 1047 makes a fundamental mistake of trying to regulate technology rather than applications. It was also a very confusing law that would have been hard to comply with. That would have driven up costs without improving safety.While I’m glad that SB 1047 has been defeated, I wish it had never made it to the governor’s desk. It would not have made AI safer. In fact, many of its opponents were champions of responsible AI and making AI safe long before the rise of generative AI. Sadly, as the Santa Fe Institute’s Melanie Mitchell pointed out, the term “AI safety” has been co-opted to refer to a broad set of speculative risks that have little basis in science — as demonstrated by the security theater SB 1047 would have required — that don’t actually make anything safer. This leaves room for lobbying that can enrich a small number of people while making everyone else worse off.As Newsom wrote to explain his decision, SB 1047 is “not informed by an empirical trajectory analysis of AI systems and capabilities.” In contrast, the United States federal government’s work is “informed by evidence-based approaches, to guard against demonstrable risks to public safety.” As the governor says, evidence-based regulation is important!Many people in the AI community were instrumental in defeating the bill. We're lucky to have Martin Casado, who organized significant community efforts; Clément Delangue, who championed openness; Yann LeCun, a powerful advocate for open research and open source; Chris Lengerich, who published deep legal analysis of the bill; Fei-Fei Li and Stanford's HAI, who connected with politicians; and Garry Tan, who organized the startup accelerator Y Combinator against the bill. Legendary investors Marc Andreessen and Roelof Botha were also influential. Plus far too many others to name here. I’m also delighted that brilliant artists like MC Hammer support the veto!Looking ahead, far more work remains to be done to realize AI’s benefits. Just this week, OpenAI released an exciting new voice API that opens numerous possibilities for beneficial applications! In addition, we should continue to mitigate current and potential harms. UC Berkeley computer scientist Dawn Song and collaborators recently published a roadmap to that end. This includes investing more to enable researchers to study AI risks and increasing transparency of AI models (for which open source and red teaming will be a big help).Unfortunately, some segments of society still have incentives to pass bad laws like SB 1047 and use science fiction narratives of dangerous AI superintelligence to advance their agendas. The more light we can shine on what AI really is and isn’t, the harder it will be for legislators to pass laws based on science fiction rather than science.Keep learning!Andrew",
        "summary": "*   California's SB 1047 bill, criticized as anti-innovation and harmful to open source AI, was vetoed by Governor Newsom.\n*   The bill was opposed for attempting to regulate AI technology rather than applications and focusing on speculative \"AI safety\" risks not based on science.\n*   Governor Newsom's veto stated the bill was \"not informed by an empirical trajectory analysis of AI systems and capabilities\" and lacked evidence-based approaches for demonstrable risks.\n*   Key figures and groups within the AI community were instrumental in opposing the bill.\n*   Ongoing efforts in AI include developing beneficial applications and mitigating harms through risk research, transparency (open source, red teaming), and evidence-based approaches.",
        "publish_date": "Oct 02, 2024",
        "keywords": [
            "open source AI",
            "AI safety",
            "red teaming",
            "risk research",
            "empirical trajectory analysis"
        ],
        "url": "https://www.deeplearning.ai/the-batch/a-victory-for-innovation-and-open-source/"
    },
    {
        "content": "Dear friends,Last week I spoke at Coursera Connect, the company’s annual conference in Las Vegas, where a major topic was AI and education. There has been a lot of hype about generative AI’s ability to transform industries overnight. Certainly many industries — including education — will be transformed. But we’re about 15 years into the deep learning revolution, and we’re not yet done identifying and building useful deep learning applications. Despite the exciting progress to date with generative AI, I expect that a decade from now we will still be far from finished identifying and building generative AI applications for education and numerous other sectors.This was the first time since 2019 that Coursera’s conference was held in person. It was great to see so many people dedicated to the educational mission coming together to discuss innovations, including generative AI innovations, that serve learners.Coursera’s CEO Jeff Maggioncalda and the company’s executive team demonstrated multiple generative AI products, such as:Coursera Coach, a chatbot that understands the context of a learner's journey and answers their questions (without giving away exact answers to quiz questions!)Course Builder, which businesses are using to customize long courses or specializations quickly, for example, by selecting the parts most relevant to their businessCoach for Interactive Instruction, which lets learners have a Socratic dialog and learn or practice new concepts in conversation Because AI is a general-purpose technology, there are many opportunities to apply it to different tasks in education. I was thrilled at the volume of experimentation happening across Coursera, DeepLearning.AI, and the broader ecosystem of partners and customers. I was also proud to present awards to many partners and customers who are doing great work to serve learners.I was particularly gratified by the number of people coming together in service of the education mission. Even before the recent rise of AI, education was already urgently in need of improvement. With AI transforming jobs, the need has become even more acute. My heart was warmed by the conversations I had with many people from universities, high schools, businesses, and the Coursera team who have a deep desire to help others through education.Coursera held its first conference in 2013, when the online education movement was in its early days, and we all had high hopes for where it could go. Today, there are over 155 million learners on Coursera. Despite that, given society’s heightened need for education and AI’s potential to transform the field, I feel the opportunities for edtech at this moment are greater than at any moment over the past decade.Keep learning!AndrewP.S. I’m excited to announce our new specialization, Generative AI for Software Development, taught by Laurence Moroney! Using chatbots to generate code is not the only way AI can help developers. This three-course series shows you how to use AI throughout the software development lifecycle – from design and architecture to coding, testing, deployment, and maintenance. Everyone who writes software can benefit from these skills. Please sign up here!",
        "summary": "*   AI, particularly generative AI, was a major topic at Coursera Connect, expected to transform education despite being ~15 years into the deep learning revolution with significant application building still needed over the next decade.\n*   Coursera demonstrated specific generative AI products for education: Coursera Coach (learner chatbot), Course Builder (course customization), and Coach for Interactive Instruction (Socratic dialog learning).\n*   AI is a general-purpose technology with many application opportunities in education, noting significant experimentation across Coursera, DeepLearning.AI, and partners.\n*   A new specialization, \"Generative AI for Software Development,\" was announced, covering AI use throughout the software development lifecycle (design, architecture, coding, testing, deployment, maintenance).",
        "publish_date": "Sep 25, 2024",
        "keywords": [
            "Coursera Coach",
            "Course Builder",
            "Coach for Interactive Instruction",
            "Socratic dialog learning",
            "Generative AI for Software Development"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-innovations-for-learners/"
    },
    {
        "content": "Dear friends,Over the weekend, my two kids colluded in a hilariously bad attempt to mislead me to look in the wrong place during a game of hide-and-seek. I was reminded that most capabilities — in humans or in AI — develop slowly.Some people fear that AI someday will learn to deceive humans deliberately. If that ever happens, I’m sure we will see it coming from far away and have plenty of time to stop it.While I was counting to 10 with my eyes closed, my daughter (age 5) recruited my son (age 3) to tell me she was hiding in the bathroom while she actually hid in the closet. But her stage whisper, interspersed with giggling, was so loud I heard her instructions clearly. And my son’s performance when he pointed to the bathroom was so hilariously overdramatic, I had to stifle a smile.Perhaps they will learn to trick me someday, but not yet! (In his awful performance, I think my son takes after me. To this day, I have a terrible poker face — which is matched by my perfect lifetime record of losing every poker game I have ever played!)Last year, the paper “Are Emergent Abilities of Large Language Models a Mirage?” by Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo, which won a NeurIPS outstanding paper award, considered “emergent” properties of LLMs, which refers to capabilities that seem to appear suddenly as model sizes increase. The authors point out that scaling laws imply that the per-token error rate decreases (improves) slowly with scale, and emergent properties might be an artifact of researchers studying nonlinear or discontinuous metrics that transform a gradually decreasing per-token error rate into something that looks more like a step function.Consider a “combination lock” metric that requires getting many items right. Say we’re measuring the likelihood that an LLM will get 10 independent digits of an answer right. If the odds of it getting each digit right improve gradually from 0 to 1, then the odds of it getting all 10 digits right will appear to jump suddenly. But if we look at continuous metrics, such as the total number of correct digits, we will see that the underlying performance actually improves gradually. (Public perception of a technology can also shift in a discontinuous way because of social dynamics.)This is why many of us saw GPT-3 as a promising step in transforming text processing long before ChatGPT appeared: BERT, GPT, GPT-2, and GPT-3 represented points on a continuous spectrum of progress. Or, looking back further in AI history, even though AlphaGo’s victory over Lee Sedol in the game of Go took the public by surprise, it actually represented many years of gradual improvements in AI’s ability to play Go.While analogies between human and machine learning can be misleading, I think that just as a person’s ability to do math, to reason — or to deceive — grows gradually, so will AI’s. This means the capabilities of AI technology will grow gradually (although I wish we could achieve AGI overnight!), and the ability of AI to be used in harmful applications, too, will grow gradually. As long as we keep performing red-teaming exercises and monitoring our systems’ capabilities as they evolve, I’m confident that we will have plenty of time to spot issues in advance, and the science-fiction fears of AI-initiated doomsday will remain science fiction.Keep learning!Andrew",
        "summary": "*   AI capabilities, including potential for deception or harmful applications, are expected to develop gradually, similar to human abilities.\n*   The paper \"Are Emergent Abilities of Large Language Models a Mirage?\" suggests that apparent \"emergent\" properties might be artifacts of using discontinuous metrics on underlying gradual performance improvements governed by scaling laws.\n*   Progress in AI models like BERT, GPT, GPT-2, and GPT-3, and systems like AlphaGo, represents a continuous spectrum of gradual improvement.\n*   Continuous monitoring and red-teaming exercises are crucial for identifying potential issues in evolving AI systems in advance.",
        "publish_date": "Sep 11, 2024",
        "keywords": [
            "Emergent Abilities",
            "Scaling Laws",
            "BERT",
            "GPT",
            "AlphaGo"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-science-fiction-scenarios-of-ais-emergent-behavior-are-likely-to-remain-fictional/"
    },
    {
        "content": "Dear friends,Years ago, when I was working at a large tech company, I was responsible for the data warehouse. Every piece of data relating to individual users was supposed to come through the data warehouse, and it was an intellectually challenging undertaking to store the data reliably and make it available to other teams, subject to security and privacy guardrails, so they could use it to derive insights.I wish that, back then, I (and my whole team) had had access to the Data Engineering Professional Certificate, a major new specialization we just launched on Coursera!Data underlies all modern AI systems, and engineers who know how to build systems to store and serve it are in high demand. Today, far too many businesses struggle to build a robust data infrastructure, which leads to missed opportunities to create value with data analytics and AI. Additionally, AI’s rise is accelerating the demand for data engineers.If you’re interested in learning these skills, please check out this four-course sequence, which is designed to make you job-ready as a data engineer.The Data Engineering Professional Certificate is taught by Joe Reis, co-author of the best-selling book Fundamentals of Data Engineering, in collaboration with Amazon Web Services. (Disclosure: I serve on Amazon's board of directors.) When DeepLearning.AI decided to teach data engineering, I felt that Joe, who has helped many startups and big companies design their data architectures and thus has broad and deep experience in this field, would be the ideal instructor. He was the first person we reached out to, and I was thrilled that he agreed to work with us on this. I hope that you’ll be thrilled, too, taking this specialization!While building AI systems and analyzing data are important skills, the data that we feed into these systems determines their performance. In this specialization, you’ll go through the whole data engineering lifecycle and learn how to generate, ingest, store, transform, and serve data. You’ll learn how to make necessary tradeoffs between speed, flexibility, security, scalability, and cost.If you’re a software engineer, this will give you a deeper understanding of data engineering so that you can build data applications. If you’re an aspiring or practicing data scientist or AI/machine learning engineer, you’ll learn skills that expand your scope to manage data in a more sophisticated way. For example, you’ll learn about DataOps to automate and monitor your data pipelines, and how to build “infrastructure as code” to programmatically define, deploy, and maintain your data infrastructure, as well as best practices for data-centric AI.You’ll also hear 17 other industry leaders share their wisdom about effective data engineering. Bill Inmon, the father of data warehousing, shares fascinating stories about the evolution of the data warehouse, including how he wrote his first program as a student in 1965. Wes McKinney, creator of the Python pandas package (as in “import pandas as pd”), talks about how he designed this wildly popular package and shares best practices for data manipulation. These instructors will give you a mental framework for developing and deploying data systems.Getting your data infrastructure right is a valuable foundational skill that will serve you well in whatever you do with AI or data analytics. I hope you enjoy this specialization!Keep learning,Andrew",
        "summary": "*   Data is foundational for modern AI systems, and robust data infrastructure is essential for deriving value from data analytics and AI, with AI's rise accelerating demand for data engineers.\n*   Effective data management is critical for AI system performance, expanding the scope for data scientists and AI/ML engineers.\n*   The Data Engineering Professional Certificate teaches the data engineering lifecycle (generate, ingest, store, transform, serve), DataOps, \"infrastructure as code\", and data-centric AI best practices.\n*   The specialization includes insights from 17 industry leaders, such as Bill Inmon (father of data warehousing) and Wes McKinney (creator of Python pandas).",
        "publish_date": "Sep 18, 2024",
        "keywords": [
            "Data Engineering Professional Certificate",
            "DataOps",
            "infrastructure as code",
            "data-centric AI",
            "Python pandas"
        ],
        "url": "https://www.deeplearning.ai/the-batch/our-new-specialization-the-data-engineering-professional-certificate/"
    },
    {
        "content": "Dear friends,Trump and the Republican party chalked up huge wins this week. Did manipulation of social media by generative AI play any role in this election? While many have worried about AI creating fake or misleading content that influences people, generative AI has probably not been the primary method of manipulation in this election cycle. Instead, I think a bigger impact might have been the “amplification effect” where software bots — which don’t have to rely heavily on generative AI — create fake engagement (such as likes/retweets/reshares), leading social media companies’ recommendation algorithms to amplify certain content to real users, some of whom promote it to their own followers. This is how fake engagement leads to real engagement.This amplification effect is well known to computer security researchers. It is an interesting sign of our global anxiety about AI that people ascribe social media manipulation to AI becoming more powerful. But the problem here is not that AI is too powerful; rather, it is that AI is not powerful enough. Specifically, the issue is not that generative AI is so powerful that hostile foreign powers or unethical political operatives are successfully using it to create fake media that influences us; the problem is that some social media companies’ AI algorithms are not powerful enough to screen out fake engagement by software bots, and mistake it for real engagement by users. These bots (which don’t need to be very smart) fool the recommender algorithms into amplifying certain content.The Washington Post reported that tweets on X/Twitter posted by Republicans were more viral than tweets from Democrats. Did this reflect the audience’s deeper engagement with Republican messages than Democratic ones, or have bots influenced this by boosting messages on either side? It is hard to know without access to Twitter’s internal data.The bottleneck to disinformation is not creating it but disseminating it. It is easy to write text that proposes a certain view, but hard to get many people to read it. Rather than generating a novel message (or using deepfakes to generate a misleading image) and hoping it will go viral, it might be easier to find a message written by a real human that supports a point of view you want to spread, and use bots to amplify that.I don’t know of any easy technical or legislative approach to combating bots. But it would be a good step to require transparency of social media platforms so we can better spot problems, if any. Everyone has a role to play in protecting democracy, and in tech, part of our duty will be to make sure social media platforms are fair and defend them against manipulation by those who seek to undermine democracy.Democracy is one of humanity’s best inventions. Elections are an important mechanism for protecting human rights and supporting human flourishing. Following this election, we must continue to strenuously nourish democracy and make sure this gem of human civilization continues to thrive.Keep learning!Andrew",
        "summary": "*   Generative AI was likely not the primary method of social media manipulation in the election cycle.\n*   A bigger impact might be the \"amplification effect\" where software bots create fake engagement, fooling social media companies’ recommendation algorithms (AI) into amplifying content.\n*   The issue is not generative AI being too powerful for creating fake media, but social media companies’ AI algorithms not being powerful enough to screen out fake engagement by bots.\n*   The bottleneck to disinformation is dissemination, not generation; amplifying existing content with bots might be easier than generating new content with generative AI.\n*   Requiring transparency of social media platforms is suggested to better spot problems related to algorithmic manipulation.",
        "publish_date": "Nov 06, 2024",
        "keywords": [
            "Recommendation algorithms",
            "Algorithmic manipulation",
            "Bots",
            "Screening algorithms",
            "Amplification"
        ],
        "url": "https://www.deeplearning.ai/the-batch/social-media-bots-and-the-amplification-effect/"
    },
    {
        "content": "Dear friends,Welcome to our special Halloween issue of The Batch, in which we probe fears, anomalies, and shadows of AI.In this letter, I’d like to explore why some people who are knowledgeable in AI take extreme positions on AI “safety” that warn of human extinction and describe scenarios, such as AI deciding to “take over,” based less on science than science fiction. As I wrote in last year’s Halloween edition, exaggerated fears of AI cause real harm. I’d like to share my observations on the psychology behind some of the fear mongering.First, there are direct incentives for some AI scientists and developers to create fear of AI: Companies that are training large models have pushed governments to place large regulatory burdens on competitors, including open source/open weights models.A few enterprising entrepreneurs have used the supposed dangers of their technology to gin up investor interest. After all, if your technology is so powerful that it can destroy the world, it has to be worth a lot!Fear mongering attracts a lot of attention and is an inexpensive way to get people talking about you or your company. This makes individuals and companies more visible and apparently more relevant to conversations around AI.It also allows one to play savior: “Unlike the dangerous AI products of my competitors, mine will be safe!” Or “unlike all other legislators who callously ignore the risk that AI could cause human extinction, I will pass laws to protect you!”Persuading lawmakers to place compliance burdens on AI developers could boost one's efforts to build a business that helps AI companies comply with new regulations! See, for example, this concerning conflict of interest from a prominent backer of California’s proposed AI safety law, SB-1047. I’ve seen people start off making mild statements about dangers of AI and get a little positive feedback in the form of attention, praise or other rewards, which encouraged them to double down and become more alarmist over time. Further, once someone has taken a few steps in this direction, the psychological effect known as commitment and consistency bias, where one feels obliged to stay consistent with one’s earlier statements, will lead some people to keep going in this direction.To be clear, AI has problems and potentially harmful applications that we should address. But excessive hype about science-fiction dangers is also harmful.Although I’m highlighting various motivations for AI fear mongering, ultimately the motivations that underlie any specific person’s actions are hard to guess. This is why, when I argue for or against particular government policies, I typically stick to the issues at hand and make points regarding the impact of particular decisions (such as whether it will stifle open source) instead of speculating about the motivations of specific people who take particular sides. This, too, is why I rarely make issues personal. I would rather stick to the issues than to the personalities.When I understand someone’s motivations, I find that I can better empathize with them (and better predict what they’ll do), even if I don’t agree with their views. I also encourage expressing one’s own motives transparently. For example, I’m strongly pro the AI community, and strongly pro open source! Still, arguments based on substantive issues ultimately carry the most weight. By arguing for or against specific policies, investments, and other actions based on their merits rather than hypothetical motivations, I believe we can act more consistently in a rational way to serve the goals we believe in.Happy Halloween!Andrew",
        "summary": "* Some knowledgeable people take extreme positions on AI safety, warning of human extinction based less on science than science fiction, which the author argues causes real harm.\n* Motivations for AI fear mongering include incentives for large model companies to regulate competitors (like open source), entrepreneurs attracting investors, gaining attention/relevance, playing the \"savior\" role, and potential conflicts of interest in proposing regulations (e.g., building compliance businesses).\n* Psychological factors like positive feedback for alarmist statements and commitment/consistency bias can reinforce extreme positions.\n* While acknowledging AI has real problems and harmful applications that need addressing, the author argues excessive hype about science-fiction dangers is also harmful.",
        "publish_date": "Oct 30, 2024",
        "keywords": [
            "AI safety",
            "Large models",
            "Open source",
            "Harmful applications",
            "Regulation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-psychology-of-ai-doom/"
    },
    {
        "content": "Dear friends,Startups live or die by their ability to execute at speed. For large companies, too, the speed with which an innovation team is able to iterate has a huge impact on its odds of success. Generative AI makes it possible to quickly prototype AI capabilities. AI capabilities that used to take months can sometimes be built in days or hours by simply prompting a large language model. I find this speed exciting and have been thinking about how to help startups and large companies alike go faster.I’ve been obsessed with speedy execution for a long time. When working on a project, I am loath to take two weeks to do something that I could do in one week. The price of moving at that pace is not that we take one week longer (which might be okay) but that we’re 2x slower (which is not)!When building an AI-powered product, there are many steps in designing, building, shipping, and scaling the product that are distinct from building the AI capability, and our ability to execute these other steps has not sped up as much as the AI part. But the speed with which we can prototype AI creates significant pressure to speed up these other steps, too. If it took 6 months to collect data, train a supervised learning algorithm, and deploy the model to the cloud, it might be okay to take 2 months to get user feedback. But if it takes a week to build a prototype, waiting 2 months for feedback seems intolerably slow!I’d like to focus on one key step of building applications: getting user feedback. A core part of the iterative workflow of designing and building a product (popularized by Eric Ries in his book The Lean Startup) is to build a prototype (or MVP, minimum viable product), get user feedback on it, and to use that feedback to drive improvements. The faster you can move through this loop — which may require many iterations — the faster you can design a product that fits the market. This is why AI Fund, a venture studio that I lead, uses many fast, scrappy tactics to get feedback.For B2C (business to consumer) offerings, here is a menu of some options for getting customer feedback: Ask 3 friends or team members to look at the product and let you know what they think (this might take ~0.5 days).Ask 10 friends or team members to take a look (~2 days).Send it to 100 trusted/volunteer alpha testers (~1 week?).Send it to 1,000 users to get qualitative or quantitative feedback (~2 weeks?).Incorporate it into an existing product to get feedback (1 to 2 months?).Roll it out to a large user base of an existing product and do rigorous A/B testing.As we go down this list, we get (probably) more accurate feedback, but the time needed to get that feedback increases significantly. Also, the tactics at the top of the list create basically no risk, and thus it’s safe to repeatedly call on them, even with preliminary ideas and prototypes. Another advantage of the tactics further up the list is that we get more qualitative feedback (for example, do users seem confused? Are they telling us they really need one additional feature?), which sparks better ideas for how to change our product than an A/B test, which tells us with rigor whether a particular implementation works but is less likely to point us in new directions to try. I recommend using the fast feedback tactics first. As we exhaust the options for learning quickly, we can try the slower tactics.With these tactics, scrappy startup leaders and innovation-team leaders in large companies can go faster and have a much higher chance of success.The mantra “move fast and break things” got a bad reputation because, well, it broke things. Unfortunately, some have interpreted this to mean we should not move fast, but I disagree. A better mantra is “move fast and be responsible.” There are many ways to prototype and test quickly without shipping a product that can cause significant harm. In fact, prototyping and testing/auditing quickly before launching to a large audience is a good way to identify and mitigate potential problems.There are numerous AI opportunities ahead, and our tools are getting better and better to pursue them at speed, which I find exhilarating!Keep learning!Andrew",
        "summary": "*   Generative AI, leveraging large language models, enables prototyping of AI capabilities in days or hours, significantly faster than the months previously required.\n*   The speed of AI prototyping highlights the need to accelerate other product development steps, such as obtaining user feedback.\n*   Iterative development requires fast feedback loops, utilizing methods ranging from quick internal reviews to more rigorous A/B testing, balancing speed and accuracy.",
        "publish_date": "Oct 23, 2024",
        "keywords": [
            "Generative AI",
            "large language models",
            "A/B testing",
            "AI prototyping",
            "iterative development"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-get-user-feedback-to-your-ai-products-fast/"
    },
    {
        "content": "Dear friends,It’s high time to take geoengineering more seriously as a potential tool to mitigate climate change. 2023 was the hottest year on record, and 2024 is likely to top that. In the United States, Hurricane Helene caused over 200 deaths, and Hurricane Milton's death toll is at least two dozen. It’s well established that the hurricanes are growing stronger as global temperatures rise.While stratospheric aerosol injection (SAI) — which sprays particles (aerosols) in the atmosphere to provide a small amount of shade from the sun — is far from a perfect solution, we should take it seriously as a possible tool for saving lives. A few months ago, my collaborators and I had released a climate emulator, Planet Parasol, that you can play with to simulate different SAI scenarios to understand its possible impact. By using AI to model its impact and thereby advance our understanding of SAI, we’ll be better prepared to decide if this is a good step.The key idea of SAI, which is a form of climate geoengineering, is to spray reflective particles into the stratosphere to reflect a little more, say 1%, of the sunlight that otherwise would fall on Earth back into space. This small increase in reflected sunlight would be sufficient to mitigate much of the impact of human-induced warming. For example, in 1991, Mount Pinatubo ejected almost 20 tons of aerosols (sulfur dioxide) into the atmosphere and cooled down the planet by around 0.5 degrees Celsius over the following year. We should be able to induce cooling equivalent to, say, a fraction of Mount Pinatubo, via a fair, international process that’s backed by science.There are many criticisms of SAI, such as: It could have unintended climate consequences, for example, disrupting local weather patterns and creating droughts or floods.  If it were started and then stopped suddenly, it could lead to sudden warming, known as “termination shock.”Depending on the aerosol used (sulfur dioxide is a leading candidate), it could contribute to pollution and/or ozone depletion. It might reduce urgency to decarbonize (an example of a “moral hazard”).In addition, many people have a visceral emotional reaction, as I once did before I understood the science more deeply, against “playing god” by daring to engineer the planet.All these downsides should be balanced against the reality that people are dying.I’m moved by meteorologist John Morales’ emotional account of the havoc caused by Hurricane Milton. The New York Times quoted him as saying, “It claims lives. It also wrecks lives.”Skyfire AI, a drone company led by CEO Don Mathis that my team AI Fund helped to co-build, was recently on the ground in the aftermath of Helene and Milton, deploying drones to help emergency responders survey remote areas and find survivors. Mathis reports that Skyfire was credited with saving at least 13 lives. On Monday, I also spoke about AI applied to renewable energy with AES’ CEO Andres Gluski and CPO Chris Shelton. You can view our conversation here.While I’m glad that AI can help mitigate these disasters, it saddens me that so many lives have already been lost due to climate-influenced causes. My mind frequently returns to SAI as one of the few untapped tools in our arsenal that can help. We need to be investing in SAI research now.I’m grateful to my collaborators on the Planet Parasol emulator (a group that includes many climate scientists) including Jeremy Irvin, Daniele Visioni, Ben Kravitz, Dakota Gruener, Chris Smith, and Duncan Watson-Parris. MIT Technology Review’s James Temple wrote about his experience playing with our emulator and also outlines fair criticisms. Much work remains to be done, and making sure our actions are based on science — a task that AI can help with (witness the recent Chemistry and Physics Nobel Prizes going to innovators in AI!) – will help us make better decisions.If you’re interested in learning more about SAI, check out this recent panel discussion where I spoke alongside climate scientists Chris Field, David Keith, Douglas MacMartin, and Simone Tilmes about the science and possible roadmaps ahead.Keep learning!Andrew",
        "summary": "*   AI is used in the Planet Parasol climate emulator to simulate different Stratospheric Aerosol Injection (SAI) scenarios and model its impact.\n*   Skyfire AI deployed drones to help emergency responders survey areas and find survivors after hurricanes Helene and Milton, credited with saving at least 13 lives.\n*   AI is being applied to renewable energy.",
        "publish_date": "Oct 16, 2024",
        "keywords": [
            "Planet Parasol climate emulator",
            "Stratospheric Aerosol Injection (SAI)",
            "Skyfire AI",
            "Drones",
            "Emergency responders"
        ],
        "url": "https://www.deeplearning.ai/the-batch/to-fight-climate-change-its-time-to-consider-geoengineering/"
    },
    {
        "content": "Dear friends,Last week, I wrote about why working on a concrete startup or project idea — meaning a specific product envisioned in enough detail that we can build it for a specific target user — lets you go faster. In this letter, I’d like to share some best practices for identifying promising ideas.AI Fund, which I lead, works with many corporate partners to identify ideas, often involving applications of AI to the company’s domain. Because AI is applicable to numerous sectors such as retail, energy, logistics and finance, I’ve found working with domain experts who know these areas well immensely helpful for identifying what applications are worth building in these areas.Our brainstorming process starts with recommending that a large number of key contributors at our partner corporation (at least 10 but sometimes well over 100) gain a non-technical, business-level understanding of AI and what it can and can’t do. Taking DeepLearning.AI’s “Generative AI for Everyone” course is a popular option, after which a company is well positioned to assign a small team to coordinate a brainstorming process, followed by a prioritization exercise to pick what to work on. The brainstorming process can be supported by a task-based analysis of jobs in which we decompose employees’ jobs into tasks to identify which ones might be automated or augmented using AI. Here are some best practices for these activities:Trust the domain expert’s gut. A domain expert who has worked for years in a particular sector will have well honed instincts that let them make leaps that would take a non-expert weeks of research.Let’s say we’re working with a financial services expert and have developed a vague idea (“build a chatbot for financial advice”). To turn this into a concrete idea, we might need to answer questions such as what areas of finance to target (should we focus on budgeting, investing, or insurance?) and what types of user to serve (fresh graduates, mortgage applicants, new parents, or retirees?) Even a domain expert who has spent years giving financial advice might not know the best answer, but a choice made via their gut gives a quick way to get to one plausible concrete idea. Of course, if market-research data can be obtained quickly to support this decision, we should take advantage of it. But to avoid slowing down too much, we’ve found that experts’ gut reactions work well and are a quick way to make decisions. So, if I’m handed a non-concrete idea, I often ask a domain expert to use their gut — and nothing else — to quickly make decisions as needed to make the idea concrete. The resulting idea is only a starting point to be tweaked over time. If, in the discussion, the domain expert picks one option but seems very hesitant to disregard a different option, then we can also keep the second option as a back-up that we can quickly pivot to if the initial one no longer looks promising. Generate many ideas. I usually suggest coming up with at least 10 ideas; some will come up with over 100, which is even better. The usual brainstorming advice to go for volume rather than quality applies here. Having many ideas is particularly important when it comes to prioritization. If only one idea is seriously considered — sometimes this happens if a senior executive has an idea they really like and puts this forward as the “main” idea to be worked on — there’s a lot of pressure to make this idea work. Even if further investigation discovers problems with it — for example, market demand turns out to be weak or the technology is very expensive to build — the team will want to keep trying to make it work so we don’t end up with nothing.In contrast, when a company has many ideas to choose from, if one starts to look less interesting, it’s easy to shift attention to a different one. When many ideas are considered, it’s easier to compare them to pick the superior ones. As explained in the book Ideaflow, teams that generate more ideas for evaluation and prioritization end up with better solutions. Because of this, I’ve found it helpful to run a broad brainstorming process that involves many employees. Specifically, large companies have many people who collectively have a lot of wisdom regarding the business. Having a small core team coordinate the gathering of ideas from a large number of people lets us tap into this collective fountain of invention. Many times I’ve seen a broad effort (involving, say, ~100 people who are knowledgeable about the domain and have a basic understanding of AI) end up with better ideas than a narrow one (involving, say, a handful of top executives). Make the evaluation criteria explicit. When evaluating and prioritizing, clear criteria for scoring and ranking ideas helps the team to judge ideas more consistently. Business value and technical feasibility are almost always included. Additionally, many companies will prioritize projects that can be a quick win (to build momentum for their overall AI efforts) or support certain strategic priorities such as growth in a particular part of the business. Making such criteria explicit can help during the idea-generation phase, and it’s critical when you evaluate and prioritize. In large companies, it can take a few weeks to go through a process to gather and prioritize ideas, but this pays off well in identifying valuable, concrete ideas to pursue. AI isn’t useful unless we find appropriate ways to apply it, and I hope these best practices will help you to generate great AI application ideas to work on.Keep learning!Andrew",
        "summary": "*   AI Fund identifies AI application ideas with corporate partners, leveraging domain experts familiar with sectors like retail, energy, logistics, and finance.\n*   The brainstorming process involves 10 to over 100 key contributors gaining a non-technical AI understanding (e.g., \"Generative AI for Everyone\" course) and potentially using task-based analysis to identify tasks for AI automation/augmentation.\n*   Best practices for identifying promising ideas include trusting domain experts' gut to make vague ideas concrete, generating many ideas (at least 10, sometimes over 100), and making evaluation criteria explicit (business value, technical feasibility, quick wins, strategic priorities).",
        "publish_date": "Jul 31, 2024",
        "keywords": [
            "Generative AI",
            "task-based analysis",
            "technical feasibility",
            "domain experts",
            "AI automation/augmentation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-brainstorm-ai-startup-ideas/"
    },
    {
        "content": "Dear friends,After a recent price reduction by OpenAI, GPT-4o tokens now cost $4 per million tokens (using a blended rate that assumes 80% input and 20% output tokens). GPT-4 cost $36 per million tokens at its initial release in March 2023. This price reduction over 17 months corresponds to about a 79% drop in price per year: 4/36 = (1 - p)17/12. (OpenAI charges a lower price, just $2 per million tokens, for using a new Batch API that takes up to 24 hours to respond to a batch of prompts. That’s an 87% drop in price per year.)As you can see, token prices are falling rapidly! One force that’s driving prices down is the release of open weights models such as Llama 3.1. If API providers, including startups Anyscale, Fireworks, Together.ai, and some large cloud companies, do not have to worry about recouping the cost of developing a model, they can compete directly on price and a few other factors such as speed.Further, hardware innovations by companies such as Groq (a leading player in fast token generation), Samba Nova (which serves Llama 3.1 405B tokens at an impressive 114 tokens per second), and wafer-scale computation startup Cerebras (which just announced a new offering), as well as the semiconductor giants NVIDIA, AMD, Intel, and Qualcomm, will drive further price cuts.When building applications, I find it useful to design to where the technology is going rather than only where it has been. Based on the technology roadmaps of multiple software and hardware companies — which include improved semiconductors, smaller models, and algorithmic innovation in inference architectures — I’m confident that token prices will continue to fall rapidly.This means that even if you build an agentic workload that isn’t entirely economical, falling token prices might make it economical at some point. As I wrote previously, being able to process many tokens is particularly important for agentic workloads, which must call a model many times before generating a result. Further, even agentic workloads are already quite affordable for many applications. Let's say you build an application to assist a human worker, and it uses 100 tokens per second continuously: At $4/million tokens, you'd be spending only $1.44/hour – which is significantly lower than the minimum wage in the U.S. and many other countries. So how can AI companies prepare?First, I continue to hear from teams that are surprised to find out how cheap LLM usage is when they actually work through cost calculations. For many applications, it isn’t worth too much effort to optimize the cost. So first and foremost, I advise teams to focus on building a useful application rather than on optimizing LLM costs.Second, even if an application is marginally too expensive to run today, it may be worth deploying in anticipation of lower prices. Finally, as new models get released, it might be worthwhile to periodically examine an application to decide whether to switch to a new model either from the same provider (such as switching from GPT-4 to the latest GPT-4o-2024-08-06) or a different provider, to take advantage of falling prices and/or increased capabilities. Because multiple providers now host Llama 3.1 and other open-weight models, if you use one of these models, it might be possible to switch between providers without too much testing (though implementation details — specifically quantization, does mean that different offerings of the model do differ in performance). When switching between models, unfortunately, a major barrier is still the difficulty of implementing evals, so carrying out regression testing to make sure your application will still perform after you swap in a new model can be challenging. However, as the science of carrying out evals improves, I’m optimistic that this will become easier.Keep learning!Andrew",
        "summary": "*   OpenAI's GPT-4o token price is $4 per million tokens (blended), a significant reduction from GPT-4's initial $36 per million tokens (approx. 79% annual drop); the Batch API costs $2 per million tokens.\n*   Falling token prices are driven by open weights models like Llama 3.1 enabling price competition among API providers and hardware innovations from companies like Groq, Samba Nova (serving Llama 3.1 405B at 114 tokens per second), and Cerebras.\n*   Decreasing token costs make agentic workloads more economical ($1.44/hour for 100 tokens/sec at $4/million), suggesting AI companies should prioritize building useful applications over initial LLM cost optimization.\n*   AI companies should anticipate continued rapid token price drops, potentially deploying applications even if marginally expensive today, and periodically evaluate switching models or providers despite current difficulties in implementing evaluations (evals).",
        "publish_date": "Aug 28, 2024",
        "keywords": [
            "GPT-4o",
            "GPT-4",
            "Llama 3.1",
            "Groq",
            "Samba Nova"
        ],
        "url": "https://www.deeplearning.ai/the-batch/falling-llm-token-prices-and-what-they-mean-for-ai-companies/"
    },
    {
        "content": "Dear friends,Recently I visited South Korea, where I spoke at length about AI with President Yoon Suk Yeol. Based on what I saw there in government, business, and academia, the nation is well positioned to become a strong AI hub. When he asked me if I would advise South Korea as a member of the Global AI Strategy Steering Group of the country’s National AI Committee, I agreed on the spot. I was delighted to learn this week that Yann LeCun has also joined. I’ve been consistently impressed by the thoughtful approach the Korean government has taken toward AI, with an emphasis on investment and innovation and a realistic understanding of risks without being distracted by science-fiction scenarios of harm.I’ve advised many countries to build AI for the sectors where they’re strong. For example, I felt that by investing in sectors like tourism and certain industries, Thailand can do projects more efficiently than I can in Silicon Valley. South Korea’s tech ecosystem gives it a foundation to move even faster across multiple sectors. This emphasizes the long-term value for countries to become good at tech, because tech is now pervasive and affects all industries.Korea has a very strong local software ecosystem. For example, the dominant search engine is not Google or Bing, but Naver (a Korean company). The dominant messaging system is not WhatsApp or WeChat, but KakaoTalk. With local tech giants Naver and Kakao offering email, mobile payment, cloud computing, ride sharing, and other services, the country has many sophisticated tech businesses. Additionally, SK hynix and Samsung are advanced semiconductor manufacturers. It also has a thriving entrepreneurship ecosystem, including Upstage, a language modeling startup, which taught a course with us on “Pretraining LLMs.” Finally, the Korean institutions Seoul National University, which I visited last year, and KAIST have global reputations.Korea has a highly educated population, highly skilled software engineers, and a thriving set of software products. This gives it a fantastic foundation to embrace the next generation of AI. After meeting with businesses in retail, construction, insurance, cosmetics, telecoms, and other industries, I was delighted by the wide variety of opportunities many companies are pursuing across different industry sectors.Lastly, Korea is known globally for its K-pop. Meeting Bang Si-Hyuk, the chairman of HYBE, which manages the superstar singing group BTS, and learning how the company operates was a real treat! (Another treat was eating at a Korean eel house, where the seafood was unforgettable.)That’s why I’ve traveled to South Korea four times since last year. My venture studio AI Fund, which collaborates with many Korean companies, has benefited tremendously from the advice of many South Koreans, including Taizo Son, Changmook Kang, Hyungjun Kim, Sung Kim, JP Lee, Ian Park, and Alice Oh. I look forward to doing more in, and with, South Korea!화이팅 (Let’s go)!AndrewP.S. We just released the final two courses of AI Python for Beginners! The complete set of four courses is now available and remains free for a limited time. If you know someone who is considering learning to code, please recommend these courses! They teach how to (a) write code using AI-assistance, which is where the field is going, and (b) take advantage of generative AI, which allows you to do valuable things quickly. Since releasing the first two courses, I’ve been inspired by many learner stories like this one. Julia K. started with AI Python for Beginners and shortly afterward wrote useful program after useful program. (She accomplished this before we had even finished releasing all four courses!) I hope many others will have similar stories to tell.",
        "summary": "*   South Korea is positioned to become a strong AI hub, with the government emphasizing investment and innovation and establishing a Global AI Strategy Steering Group of the National AI Committee, joined by Yann LeCun and the author.\n*   Korea possesses a strong tech foundation for AI, including a robust local software ecosystem (Naver, KakaoTalk), advanced semiconductor manufacturing (SK hynix, Samsung), leading universities (Seoul National University, KAIST), and a highly skilled talent pool.\n*   The country has a thriving AI entrepreneurship ecosystem, exemplified by language modeling startup Upstage (which taught a course on \"Pretraining LLMs\"), and companies across diverse sectors are actively pursuing AI opportunities.\n*   The author's venture studio AI Fund collaborates with many Korean companies.",
        "publish_date": "Sep 04, 2024",
        "keywords": [
            "Pretraining LLMs",
            "Language modeling",
            "Upstage",
            "SK hynix",
            "Naver"
        ],
        "url": "https://www.deeplearning.ai/the-batch/an-asian-ai-hub-in-the-making/"
    },
    {
        "content": "Dear friends, When entrepreneurs build a startup, it is often their speed and momentum that gives them a shot at competing with the tech behemoths. This is true of countries as well. I was recently in Thailand, where I was delighted to see tremendous momentum building in AI (and sip the best Thai ice tea I’ve ever tasted). Even though Thailand is not as advanced in AI technology or applications as leading tech countries, the enthusiasm for building AI throughout government, corporations, and academia was thrilling. I came away heartened that AI’s benefits will be spread among many countries and convinced that one’s level of AI development right now matters less than your momentum toward increasing it.  Seeing the momentum behind AI in Thailand — where the per capita GDP is around one fifth that of Japan, and one tenth that of the United States — left me feeling that any country, company, or person has a shot at doing meaningful work in the field. While advanced economies such as the U.S. and China are still in the lead, generative AI has made the playing field more level. Foundation models, especially those with open weights, are significantly lowering the barriers to building meaningful AI projects. In Thailand, a lot of people I met weren’t just talking about AI, they were rolling up their sleeves and building. That buys a nation a lot more momentum than just talk. I met with Prime Minister Srettha Thavisin and his Ministers of Higher Education and Education (primary/secondary) along with many staffers. It was delightful to hear the PM speak of his enthusiasm for AI. The ministers discussed how to (i) provide AI training and (ii) use AI to improve education in a variety of subjects. Happily, the focus was on creating value while thinking through realistic risks like AI’s potential to proliferate misinformation, and not a single person asked me about whether AI will lead to human extinction! I also met with many business leaders and enjoyed seeing a rapid pace of experimentation with AI. KBTG, an affiliate of the country’s leading digital bank KBank, is working on a financial chatbot advisor, AI-based identity verification for anti-fraud, AI for auto insurance, and a Thai-language financial large language model. These features are growing mobile banking and increasing financial access. Many business leaders in other sectors, too, have asked their teams to run experiments. There are many AI applications yet to be built in industrial sectors, tourism, trade, and more! (KBTG is an investor in AI Fund, which I lead.) I often visit universities in both developed and developing economies, and I’ve been surprised to see that universities in developing economies sometimes adopt AI faster. At Chulalongkorn University (known as Chula), I met with the University President Wilert Puriwat and Director of Chula AI Professor Proadpran Punyabukkana. Chula AI has rolled out campus-wide training in generative AI for faculty, staff, and students. In addition, it supports building AI applications such as AI screening for depression and gastrointestinal cancer.  It takes years to build up advanced technology. But momentum matters, and there will be many rewards along the journey. There’s no time like the present to start building!  Keep building, Andrew ",
        "summary": "* Thailand is showing momentum in AI development across government, corporations, and academia.\n* Generative AI and open-weight foundation models are lowering barriers for building AI projects.\n* Government initiatives include providing AI training and using AI to improve education.\n* Businesses like KBTG are developing specific AI applications: financial chatbot, AI identity verification (anti-fraud), AI for auto insurance, and a Thai-language financial large language model.\n* Universities like Chulalongkorn University are providing campus-wide generative AI training and supporting AI application development (e.g., AI screening for depression and gastrointestinal cancer).",
        "publish_date": "Aug 14, 2024",
        "keywords": [
            "Thai-language financial large language model",
            "financial chatbot",
            "AI identity verification",
            "AI screening for depression",
            "AI screening for gastrointestinal cancer"
        ],
        "url": "https://www.deeplearning.ai/the-batch/thailands-ai-push/"
    },
    {
        "content": "Dear friends,I’m encouraged at the progress of the U.S. government at moving to stem harmful AI applications. Two examples are the new Federal Trade Commission (FTC) ban on fake product reviews and the DEFIANCE Act, which imposes punishments for creating and disseminating non-consensual deepfake porn. Both rules take a sensible approach to regulating AI insofar as they target harmful applications rather than general-purpose AI technology.As I described previously, the best way to ensure AI safety is to regulate it at the application level rather than the technology level. This is important because the technology is general-purpose and its builders (such as a developer who releases an open-weights foundation model) cannot control how someone else might use it. If, however, someone applies AI in a nefarious way, we should stop that application. Even before generative AI, fake reviews were a problem on many websites, and many tech companies dedicate considerable resources to combating them. A telltale sign of old-school fake reviews is the use of similar wording in different reviews. AI’s ability to automatically paraphrase or rewrite is making fake reviews harder to detect.Importantly, the FTC is not going after the makers of foundation models for fake reviews. The provider of an open weights AI model, after all, can’t control what someone else uses it for. Even if one were to try to train a model to put up guardrails against writing reviews, I don’t know how it could distinguish between a real user of a product asking for help writing a legitimate review and a spammer who wanted a fake review. The FTC appropriately aims to ban the application of fake reviews along with other deceptive practices such as buying positive reviews. The DEFIANCE Act, which passed unanimously in the Senate (and still requires passage in the House of Representatives before the President can sign it into law) imposes civil penalties for the creating and distributing non-consensual, deepfake porn. This disgusting application is harming many people including underage girls. While many image generation models do have guardrails against generating porn, these guardrails often can be circumvented via jailbreak prompts or fine-tuning (for models with open weights).Again, DEFIANCE regulates an application, not the underlying technology. It aims to punish people who engage in the application of creating and distributing non-consensual intimate images, regardless of how they are generated — whether the perpetrator uses a diffusion model, a generative adversarial network, or Microsoft Paint to create an image pixel by pixel.I hope DEFIANCE passes in the House and gets signed into law. Both rules guard against harmful AI applications without stifling AI technology itself (unlike California’s poorly designed SB-1047), and they offer a good model for how the U.S. and other nations can protect citizens against other potentially harmful applications. Keep learning!Andrew",
        "summary": "*   The U.S. government is regulating harmful AI applications, citing the FTC ban on fake product reviews and the DEFIANCE Act on non-consensual deepfake porn as examples.\n*   The preferred regulatory approach targets AI at the application level rather than the technology level, as the technology is general-purpose and its use cannot be controlled by builders.\n*   AI's ability to paraphrase/rewrite makes fake reviews harder to detect; the FTC ban targets the application of fake reviews, not the makers of foundation models.\n*   The DEFIANCE Act, passed unanimously in the Senate, imposes civil penalties for creating/distributing non-consensual deepfake porn, regulating the application regardless of the AI technology used (e.g., diffusion models, GANs).",
        "publish_date": "Aug 21, 2024",
        "keywords": [
            "deepfake",
            "diffusion models",
            "GANs",
            "fake product reviews",
            "paraphrase/rewrite"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-the-defiance-act-and-ftc-ban-on-fake-product-reviews-take-the-right-approach-to-regulating-ai/"
    },
    {
        "content": "Dear friends,Congratulations to Geoff Hinton and John Hopfield for winning the 2024 Physics Nobel Prize! It’s wonderful to see pioneering work in AI recognized, and this will be good for our whole field. Years ago, I was the first to call Geoff the “Godfather of Deep Learning,” which later became “Godfather of AI.” I’m thrilled at the recognition he’s receiving via this most prestigious of awards.As Geoff relayed in the “Heroes of Deep Learning” interview I did with him years ago, his early work developing the foundations of neural networks has been instrumental to the rise of deep learning and AI. It has been years since I implemented a Hopfield network, but John’s work, too, has been influential. Their recognition is well deserved!But the Nobel committee wasn’t done yet. One day after the physics prize was announced, Demis Hassabis, John Jumper, and David Baker won the Chemistry Nobel Prize for their work on AlphaFold and protein design. AlphaFold and AlphaFold 2, as well as the work of Baker’s lab, are compelling applications of AI that made significant steps forward in chemistry and biology, and this award, too, is well deserved!It’s remarkable that the Nobel committees for physics and chemistry, which are made up of scientists in those fields, chose to honor AI researchers with this year’s awards. This is a sign of our field’s growing impact on society.While it’s good that people from outside AI are recognizing AI researchers, I wonder if there’s room for the AI community to pick more award recipients ourselves. Best-known in computer science is the Turing Award, which is selected by a broad group of computer scientists, many of whom have deep AI knowledge. Many AI conferences give out best-paper awards. And applications of AI to other fields doubtless will continue to receive much-deserved recognition by leaders in those fields. I’m optimistic this will allow AI researchers to win more Nobel Prizes — someday also in economics, literature, medicine, and peace, too. Nonetheless, this seems like a good time to see how all of us in AI can do more to recognize the work of innovators in our field.Geoff once thanked me for my role in getting him anointed “Godfather of AI,” which he said was good for his career. I didn’t realize before that I had the power to give out such titles 😉 but I would love for there to be numerous godfathers and godmothers — and many other awards — in AI!At Geoff's retirement party last October (pictured in the photo above), I spoke with affection and gratitude for all the work he has done to grow AI. Even as we cheer the new Nobel wins for AI, let’s continue to think about how we in AI can do more to celebrate the next generation of innovators.Keep learning!Andrew",
        "summary": "*   Geoff Hinton and John Hopfield won the 2024 Physics Nobel Prize for pioneering work in AI, including foundations of neural networks and the Hopfield network.\n*   Demis Hassabis, John Jumper, and David Baker won the Chemistry Nobel Prize for their work on AlphaFold/AlphaFold 2 and protein design, recognized as compelling AI applications in chemistry and biology.\n*   The recognition of AI researchers by the Physics and Chemistry Nobel committees signifies AI's growing impact on society.",
        "publish_date": "Oct 09, 2024",
        "keywords": [
            "Hopfield network",
            "AlphaFold",
            "AlphaFold 2",
            "protein design",
            "neural networks"
        ],
        "url": "https://www.deeplearning.ai/the-batch/nobel-prizes-for-ai/"
    },
    {
        "content": "Dear friends,Much has been said about many companies’ desire for more compute (as well as data) to train larger foundation models. I think it’s under-appreciated that we have nowhere near enough compute available for inference on foundation models as well.Years ago, when I was leading teams at Google, Baidu, and Stanford that focused on scaling up deep learning algorithms, many semiconductor manufacturers, data center operators, and academic researchers asked me whether I felt that AI technology would continue to make good use of more compute if they kept on delivering it. For many normal desktop processing workloads, like running a web browser or a text editor, having a faster CPU doesn’t help that much beyond a certain point. So do we really need faster and faster AI processors to train larger and larger models? Each time, I confidently replied “yes!” and encouraged them to keep scaling up compute. (Sometimes, I added half-jokingly that I had never met a machine learning engineer who felt like they had enough compute. 😀)Fortunately, this prediction has been right so far. However, beyond training, I believe we are also far from exhausting the benefits of faster and higher volumes of inference.Today, a lot of LLM output is primarily for human consumption. A human might read around 250 words per minute, which is around 6 tokens per second (250 words/min / (0.75 words/token) / (60 secs/min)). So it might initially seem like there’s little value to generating tokens much faster than this.  But in an agentic workflow, an LLM might be prompted repeatedly to reflect on and improve its output, use tools, plan and execute sequences of steps, or implement multiple agents that collaborate with each other. In such settings, we might easily generate hundreds of thousands of tokens or more before showing any output to a user. This makes fast token generation very desirable and makes slower generation a bottleneck to taking better advantage of existing foundation models.That’s why I’m excited about the work of companies like Groq, which can generate hundreds of tokens per second. Recently, SambaNova published an impressive demo that hit hundreds of tokens per second.Incidentally, faster, cheaper token generation will also help make running evaluations (evals), a step that can be slow and expensive today since it typically involves iterating over many examples, more palatable. Having better evals will help many developers with the process of tuning models to improve their performance.Fortunately, it appears that both training and inference are rapidly becoming cheaper. I recently spoke with Cathie Wood and Charles Roberts of the investment firm ARK, which is famous for its bullish predictions on tech. They estimate that AI training costs are falling at 75% a year. If they are right, a foundation model that costs $100M to train this year might cost only $25M to train next year. Further, they report that for “enterprise scale use cases, inference costs seem to be falling at an annual rate of ~86%, even faster than training costs.”I don’t know how accurate these specific predictions will turn out to be, but with improvements in both semiconductors and algorithms, I do see training and inference costs falling rapidly. This will be good for application builders and help AI agentic workflows lift off.Keep learning!Andrew",
        "summary": "* There is nowhere near enough compute available for inference on foundation models, in addition to the need for training.\n* While human reading is ~6 tokens per second, agentic workflows require generating hundreds of thousands of tokens or more before user output, making fast token generation desirable and slow generation a bottleneck.\n* Companies like Groq and SambaNova are achieving hundreds of tokens per second, which also helps make model evaluations (evals) more palatable.\n* ARK estimates AI training costs are falling at 75% a year and enterprise inference costs at ~86% annually, contributing to rapidly falling costs overall.",
        "publish_date": "Apr 24, 2024",
        "keywords": [
            "foundation models",
            "inference",
            "training",
            "tokens",
            "Groq"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-we-need-more-compute-for-inference/"
    },
    {
        "content": "Dear friends,Multi-agent collaboration is the last of the four key AI agentic design patterns that I’ve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles — such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on — and have different agents accomplish different subtasks.Different agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: “You are an expert in writing clear, efficient code. Write code to perform the task . . ..” It might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I’d like to offer a few reasons:It works! Many teams are getting good results with this method, and there’s nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent. Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role’s subtask. For example, the prompt above emphasized clear, efficient code as opposed to, say, scalable and highly secure code. By decomposing the overall task into subtasks, we can optimize the subtasks better.Perhaps most important, the multi-agent design pattern gives us, as developers, a framework for breaking down complex tasks into subtasks. When writing code to run on a single CPU, we often break our program up into different processes or threads. This is a useful abstraction that lets us decompose a task, like implementing a web browser, into subtasks that are easier to code. I find thinking through multi-agent roles to be a useful abstraction as well.In many companies, managers routinely decide what roles to hire, and then how to split complex projects — like writing a large piece of software or preparing a research report — into smaller tasks to assign to employees with different specialties. Using multiple agents is analogous. Each agent implements its own workflow, has its own memory (itself a rapidly evolving area in agentic technology: how can an agent remember enough of its past interactions to perform better on upcoming ones?), and may ask other agents for help. Agents can also engage in Planning and Tool Use. This results in a cacophony of LLM calls and message passing between agents that can result in very complex workflows. While managing people is hard, it's a sufficiently familiar idea that it gives us a mental framework for how to \"hire\" and assign tasks to our AI agents. Fortunately, the damage from mismanaging an AI agent is much lower than that from mismanaging humans! Emerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to build multi-agent solutions to problems. If you're interested in playing with a fun multi-agent system, also check out ChatDev, an open source implementation of a set of agents that run a virtual software company. I encourage you to check out their GitHub repo and perhaps clone the repo and run the system yourself. While it may not always produce what you want, you might be amazed at how well it does. Like the design pattern of Planning, I find the output quality of multi-agent collaboration hard to predict, especially when allowing agents to interact freely and providing them with multiple tools. The more mature patterns of Reflection and Tool Use are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you! If you're interested in learning more, I recommend: “Communicative Agents for Software Development,” Qian et al. (2023) (the ChatDev paper)“AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation,” Wu et al. (2023) “MetaGPT: Meta Programming for a Multi-Agent Collaborative Framework,” Hong et al. (2023)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\" Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"",
        "summary": "*   Multi-agent collaboration is an AI agentic design pattern that breaks down complex tasks into subtasks executed by different agents with specific roles, often implemented by prompting LLMs.\n*   This approach provides superior performance over single agents (supported by ablation studies like in the AutoGen paper) and improves LLMs' ability to handle complex inputs by focusing on subtasks.\n*   It serves as a useful abstraction framework for developers to decompose complex tasks, analogous to processes/threads or managing human teams.\n*   Agents possess individual workflows, memory, and interact via LLM calls and message passing, potentially engaging in Planning and Tool Use.\n*   Emerging frameworks such as AutoGen, Crew AI, and LangGraph support building multi-agent solutions, with ChatDev cited as an open-source example for software development.",
        "publish_date": "Apr 17, 2024",
        "keywords": [
            "Multi-agent collaboration",
            "AutoGen",
            "Crew AI",
            "LangGraph",
            "ChatDev"
        ],
        "url": "https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/"
    },
    {
        "content": "Dear friends,One reason for machine learning’s success is that our field welcomes a wide range of work. I can’t think of even one example where someone developed what they called a machine learning algorithm and senior members of our community criticized it saying, “that’s not machine learning!” Indeed, linear regression using a least-squares cost function was used by mathematicians Legendre and Gauss in the early 1800s — long before the invention of computers — yet machine learning has embraced these algorithms, and we routinely call them “machine learning” in introductory courses!In contrast, about 20 years ago, I saw statistics departments at a number of universities look at developments in machine learning and say, “that’s not really statistics.” This is one reason why machine learning grew much more in computer science than statistics departments. (Fortunately, since then, most statistics departments have become much more open to machine learning.)This contrast came to mind a few months ago, as I thought about how to talk about agentic systems that use design patterns such as reflection, tool use, planning, and multi-agent collaboration to produce better results than zero-shot prompting. I had been involved in conversations about whether certain systems should count as “agents.” Rather than having to choose whether or not something is an agent in a binary way, I thought, it would be more useful to think of systems as being agent-like to different degrees. Unlike the noun “agent,” the adjective “agentic” allows us to contemplate such systems and include all of them in this growing movement.More and more people are building systems that prompt a large language model multiple times using agent-like design patterns. But there’s a gray zone between what clearly is not an agent (prompting a model once) and what clearly is (say, an autonomous agent that, given high-level instructions, plans, uses tools, and carries out multiple, iterative steps of processing). Rather than arguing over which work to include or exclude as being a true agent, we can acknowledge that there are different degrees to which systems can be agentic. Then we can more easily include everyone who wants to work on agentic systems. We can also encourage newcomers to start by building simple agentic workflows and iteratively make their systems more sophisticated. In the past few weeks, I’ve noticed that, while technical people and non-technical people alike sometimes use the word “agent,” mainly only technical people use the word “agentic” (for now!). So when I see an article that talks about “agentic” workflows, I’m more likely to read it, since it’s less likely to be marketing fluff and more likely to have been written by someone who understands the technology.Let’s keep working on agentic systems and keep welcoming anyone who wants to join our field!Keep learning,Andrew",
        "summary": "* Agentic systems utilize design patterns such as reflection, tool use, planning, and multi-agent collaboration with large language models (LLMs) to achieve better results than zero-shot prompting.\n* The term \"agentic\" is proposed to describe systems exhibiting varying degrees of agent-like behavior, ranging from simple multi-prompting workflows to complex autonomous agents.\n* Embracing systems with different degrees of agentic-ness allows for broader inclusion in this growing area, similar to how machine learning welcomed diverse algorithms like linear regression.",
        "publish_date": "Jun 12, 2024",
        "keywords": [
            "reflection",
            "tool use",
            "planning",
            "multi-agent collaboration",
            "zero-shot prompting"
        ],
        "url": "https://www.deeplearning.ai/the-batch/welcoming-diverse-approaches-keeps-machine-learning-strong/"
    },
    {
        "content": "Dear friends,The effort to protect innovation and open source continues. I believe we’re all better off if anyone can carry out basic AI research and share their innovations. Right now, I’m deeply concerned about California's proposed law SB-1047. It’s a long, complex bill with many parts that require safety assessments, shutdown capability for models, and so on.There are many things wrong with this bill, but I’d like to focus here on just one: It defines an unreasonable “hazardous capability” designation that may make builders of large AI models potentially liable if someone uses their models to do something that exceeds the bill’s definition of harm (such as causing $500 million in damage). That is practically impossible for any AI builder to ensure. If the bill is passed in its present form, it will stifle AI model builders, especially open source developers. Some AI applications, for example in healthcare, are risky. But as I wrote previously, regulators should regulate applications rather than technology. Technology refers to tools that can be applied in many ways to solve various problems.Applications are specific implementations of technologies designed to meet particular customer needs.For example, an electric motor is a technology. When we put it in a blender, an electric vehicle, dialysis machine, or guided bomb, it becomes an application. Imagine if we passed laws saying, if anyone uses a motor in a harmful way, the motor manufacturer is liable. Motor makers would either shut down or make motors so tiny as to be useless for most applications. If we pass such a law, sure, we might stop people from building guided bombs, but we’d also lose blenders, electric vehicles, and dialysis machines. In contrast, if we look at specific applications, like blenders, we can more rationally assess risks and figure out how to make sure they’re safe, and even ban classes of applications, like certain types of munitions. Safety is a property of applications, not a property of technologies (or models), as Arvind Narayanan and Sayash Kapoor have pointed out. Whether a blender is a safe one can’t be determined by examining the electric motor. A similar argument holds for AI.  SB-1047 doesn’t account for this distinction. It ignores the reality that the number of beneficial uses of AI models is, like electric motors, vastly greater than the number of harmful ones. But, just as no one knows how to build a motor that can’t be used to cause harm, no one has figured out how to make sure an AI model can’t be adapted to harmful uses. In the case of open source models, there’s no known defense to fine-tuning to remove RLHF alignment. And jailbreaking work has shown that even closed-source, proprietary models that have been properly aligned can be attacked in ways that make them give harmful responses. Indeed, the sharp-witted Pliny the Prompter regularly tweets about jailbreaks for closed models. Kudos also to Anthropic’s Cem Anil and collaborators for publishing their work on many-shot jailbreaking, an attack that can get leading large language models to give inappropriate responses and is hard to defend against. California has been home to a lot of innovation in AI. I’m worried that this anti-competitive, anti-innovation proposal has gotten so much traction in the legislature. Worse, other jurisdictions often follow California, and it would be awful if they were to do so in this instance.SB-1047 passed in a key vote in the State Senate in May, but it still has additional steps before it becomes law. I hope you will speak out against it if you get a chance to do so.Keep learning!Andrew",
        "summary": "*   California's proposed law SB-1047 defines a \"hazardous capability\" designation potentially making builders of large AI models liable for harm (e.g., >$500 million damage), argued to be practically impossible to ensure and stifling for developers, especially open source.\n*   A key criticism is that SB-1047 regulates AI technology/models rather than specific applications, arguing safety is a property of applications, not the underlying technology.\n*   The text states it's impossible to ensure AI models cannot be adapted for harmful uses, citing examples like fine-tuning (removing RLHF alignment) and jailbreaking attacks on models.\n*   The bill is viewed as anti-competitive and anti-innovation, raising concerns about its potential impact and setting a precedent for other jurisdictions.",
        "publish_date": "Jun 5, 2024",
        "keywords": [
            "fine-tuning",
            "RLHF alignment",
            "jailbreaking attacks",
            "large AI models",
            "Model Safety"
        ],
        "url": "https://www.deeplearning.ai/the-batch/blenders-versus-bombs-or-why-californias-proposed-ai-law-is-bad-for-everyone/"
    },
    {
        "content": "Dear friends,A barrier to faster progress in generative AI is evaluations (evals), particularly of custom AI applications that generate free-form text. Let’s say you have a multi-agent research system that includes a researcher agent and a writer agent. Would adding a fact-checking agent improve the results? If we can’t efficiently evaluate the impact of such changes, it’s hard to know which changes to keep.For evaluating general-purpose foundation models such as large language models (LLMs) — which are trained to respond to a large variety of prompts — we have standardized tests like MMLU (multiple-choice questions that cover 57 disciplines like math, philosophy, and medicine) and HumanEval (testing code generation). We also have the LMSYS Chatbot Arena, which pits two LLMs’ responses against each other and asks humans to judge which response is superior, and large-scale benchmarking like HELM. These evaluation tools took considerable effort to build, and they are invaluable for giving LLM users a sense of different models’ relative performance. Nonetheless, they have limitations. For example, leakage of benchmarks datasets’ questions and answers into training data is a constant worry, and human preferences for certain answers does not mean those answers are more accurate.In contrast, our current options for evaluating applications built using LLMs are far more limited. Here, I see two major types of applications. For applications designed to deliver unambiguous, right-or-wrong responses, we have reasonable options. Let’s say we want an LLM to read a resume and extract the candidate’s most recent job title, or read a customer email and route it to the right department. We can create a test set that comprises ground-truth labeled examples with the right responses and measure the percentage of times the LLM generates the right output. The main bottleneck is creating the labeled test set, which is expensive but surmountable.But many LLM-based applications generate free-text output with no single right response. For example, if we ask an LLM to summarize customer emails, there’s a multitude of possible good (and bad) responses. The same holds for an agentic system to do web research and write an article about a topic, or a RAG system for answering questions. It’s impractical to hire an army of human experts to read the LLM’s outputs every time we tweak the algorithm and evaluate if the answers have improved; we need an automated way to test the outputs. Thus, many teams use an advanced language model to evaluate outputs. In the customer email summarization example, we might design an evaluation rubric (scoring criteria) for what makes a good summary. Given an email summary generated by our system, we might prompt an advanced LLM to read it and score it according to our rubric. I’ve found that the results of such a procedure, while better than nothing, can also be noisy — sometimes too noisy to reliably tell me if the way I’ve tweaked an algorithm is good or bad.The cost of running evals poses an additional challenge. Let’s say you’re using an LLM that costs $10 per million input tokens, and a typical query has 1000 tokens. Each user query therefore costs only $0.01. However, if you iteratively work to improve your algorithm based on 1000 test examples, and if in a single day you evaluate 20 ideas, then your cost will be 20*1000*0.01 = $200. For many projects I’ve worked on, the development costs were fairly negligible until we started doing evals, whereupon the costs suddenly increased. (If the product turned out to be successful, then costs increased even more at deployment, but that was something we were happy to see!) Beyond the dollar cost, evals have a significant time cost. Running evals on 1000 examples might take tens of minutes or even hours. Time spent waiting for eval jobs to finish also slows down the speed with which we can experiment and iterate over new ideas. In an earlier letter, I wrote that fast, inexpensive token generation is critical for agentic workflows. It will also be useful for evals, which involve nested for-loops that iterate over a test set and different model/hyperparameter/prompt choices and therefore consume large numbers of tokens. Despite the limitations of today’s eval methodologies, I’m optimistic that our community will invent better techniques (maybe involving agentic workflows like reflection for getting LLMs to evaluate such output. If you’re a developer or researcher and have ideas along these lines, I hope you’ll keep working on them and consider open sourcing or publishing your findings.Keep learning!Andrew",
        "summary": "* Evaluating custom generative AI applications, particularly those generating free-form text, is a significant barrier to faster progress.\n*   While standardized tests exist for foundation models (e.g., MMLU covering 57 disciplines, HumanEval for code, LMSYS Chatbot Arena, HELM), evaluating LLM applications with free-form output is difficult, with LLM-based evaluation methods often being noisy.\n*   Running evaluations incurs significant costs, including dollar costs (e.g., $200/day for 20 ideas * 1000 examples at $0.01/query) and time costs (tens of minutes or hours for 1000 examples), slowing down iteration.",
        "publish_date": "May 29, 2024",
        "keywords": [
            "MMLU",
            "HumanEval",
            "LMSYS Chatbot Arena",
            "HELM",
            "LLM-based evaluation methods"
        ],
        "url": "https://www.deeplearning.ai/the-batch/we-need-better-evals-for-llm-applications/"
    },
    {
        "content": "Dear friends,On Father’s Day last weekend, I sat with my daughter to help her practice solving arithmetic problems. To give her practice problems, I used OpenDevin, an open-source agentic coding framework, to write a Python script that generated questions that she enjoyed answering at her own pace. OpenDevin wrote the code much faster than I could have and genuinely improved my and my daughter’s day. Six months ago, coding agents were a novelty. They still frequently fail to deliver, but I find that they’re now working well enough that they might be genuinely useful to more and more people! Given a coding problem that’s specified in a prompt, the workflow for a coding agent typically goes something like this: Use a large language model (LLM) to analyze the problem and potentially break it into steps to write code for, generate the code, test it, and iteratively use any errors discovered to ask the coding agent to refine its answer. But within this broad framework, a huge design space and numerous innovations are available to experiment with. I’d like to highlight a few papers that I find notable:“AgentCoder: Multiagent-Code Generation with Iterative Testing and Optimisation,” Huang et al. (2024). “LDB: A Large Language Model Debugger via Verifying Runtime Execution Step by Step,” Zhong et al., (2024). “SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering,” Yang et al. (2024).How can we test the code without requiring the user to write test cases? In a multi-agent system, each “agent” is an LLM prompted to play a particular role. An interesting result from AgentCoder shows that having separate agents for writing code and generating tests results in better performance than letting a single agent do both tasks. This is presumably because, if the agent writing the code is also responsible for writing the tests, the tests might be influenced by the code and fail to consider corner cases that the code does not cover. When people think of testing code, many initially think of output testing, in which we see if the code generates the correct outputs to a specific set of test inputs. If the code fails a test, an LLM can be prompted to reflect on why the code failed and then to try to fix it. In addition to testing the output, the LDB method is helpful. LDB steps through the code and presents to the LLM values of the variables during intermediate steps of execution, to see if the LLM can spot exactly where the error is. This mimics how a human developer might step through the code to see where one of the computational steps went wrong, and so pinpoint and fix the problem. A lot of agentic workflows mimic human workflows. Similar to other work in machine learning, if humans can do a task, then trying to mimic humans makes development much easier compared to inventing a new process. However, the authors of SWE-agent noticed that many tools that humans use for coding are very inefficient for agents. For example, giving an agent access to a bash shell and having it find a piece of code by executing numerous cd, ls, and cat commands is inefficient, even though humans can do this rapidly. Similarly, visual coding editors like VSCode, emacs, and vim are easy for humans to use, but hard for LLMs (or LMMs) to navigate. Because agents interact with computers differently than humans do, the authors found that building special-purpose tools (functions) to let an agent search, view, and edit codebases resulted in better performance. One reason research into coding agents is making rapid progress is that their performance can be evaluated automatically and reliably. With benchmarks like HumanEval, MBPP, and SWE-bench, researchers can try out an idea and automatically test how often it generates correct code. In contrast, even though there’s considerable activity on AI research agents that search the web and synthesize an article (I’ve enjoyed using the open-source STORM system by Stanford's Yijia Shao et al.), they are hard to evaluate and this makes progress harder. Github Copilot was released in 2021, and many developers have been getting coding help by prompting LLMs. The rapid evolution from that to more sophisticated coding agents is expanding how computers can help us with coding tasks, and the pace of progress is rapid. With these tools, I expect programming to become even more fun and more productive.Keep coding!Andrew",
        "summary": "*   Coding agents, leveraging Large Language Models (LLMs), are increasingly useful for generating, testing, and refining code, with frameworks like OpenDevin demonstrating their capability.\n*   Innovations in coding agents include multi-agent systems (AgentCoder, 2024) where separate agents handle coding and testing, and debugging methods (LDB, 2024) that verify runtime execution step-by-step.\n*   Optimizing agent-computer interfaces (SWE-agent, 2024) through special-purpose tools for codebase interaction is more effective than mimicking human workflows with standard tools.\n*   Rapid progress in coding agents is aided by automatic and reliable evaluation using benchmarks like HumanEval, MBPP, and SWE-bench.",
        "publish_date": "Jun 19, 2024",
        "keywords": [
            "OpenDevin",
            "AgentCoder",
            "LDB",
            "SWE-agent",
            "HumanEval"
        ],
        "url": "https://www.deeplearning.ai/the-batch/coding-agents-are-evolving-from-novelties-to-widely-useful-tools/"
    },
    {
        "content": "Dear friends,A good way to get started in AI is to start with coursework, which gives a systematic way to gain knowledge, and then to work on projects. For many who hear this advice, “projects” may evoke a significant undertaking that delivers value to users. But I encourage you to set a lower bar and relish small, weekend tinkering projects that let you learn, even if they don’t result in a meaningful deliverable. Recently, my son and daughter (ages 3 and 5) were building Lego vehicles. They built a beautiful ice-cream truck as well as a . . . umm . . . colorful and asymmetric dinosaur car, shown in the picture above. While most observers would judge the ice-cream truck as the superior creation, my kids built it by following Lego’s instructions, and it is likely identical to thousands of ice-cream trucks built by others. In contrast, building the dinosaur car required creativity and novel thinking. The exercise helped them hone their ability to pick and assemble Lego building blocks.There is, of course, room for both mimicking others’ designs (with permission) and coming up with your own. As a parent, I try to celebrate both. (To be honest, I celebrated the dinosaur car more.) When learning to build Lego, it’s helpful to start by following a template. But eventually, building your own unique projects enriches your skills. As a developer, too, I try to celebrate unique creations. Yes, it is nice to have beautiful software, and the impact of the output does matter. But good software is often written by people who spend many hours tinkering and building things. By building unique projects, you master key software building blocks. Then, using those blocks, you can go on to build bigger projects.I routinely tinker with building AI applications, and a lot of my tinkering doesn’t result in anything useful. My latest example: I built a Streamlit app that would authenticate to Google docs, read the text in a doc, use a large language model to edit my text, and write the result back into the doc. I didn’t find it useful in the end because of friction in the user interface, and I’m sure a commercial provider will soon, if they haven’t already, build a better product than I was able to throw together in a couple of hours on a weekend. But such tinkering helps me hone my intuition and master software components (I now know how to programmatically interface with Google docs) that might be useful in future projects.  If you have an idea for a project, I encourage you to build it! Often, working on a project will also help you decide what additional skills to learn, perhaps through coursework. To sustain momentum, it helps to find friends with whom to talk about ideas and celebrate projects — large or small. Keep tinkering! Andrew",
        "summary": "*   Starting in AI involves coursework followed by working on projects.\n*   Small, weekend \"tinkering\" projects are encouraged for learning, even if they don't result in a meaningful deliverable.\n*   Building unique projects helps master software components, such as programmatically interfacing with tools like Google Docs and Large Language Models (LLMs).\n*   Working on projects helps identify further skills to learn, and discussing ideas with friends helps sustain momentum.",
        "publish_date": "May 22, 2024",
        "keywords": [
            "Large Language Models (LLMs)",
            "software components",
            "programmatically interfacing",
            "projects",
            "AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/project-idea-a-car-for-dinosaurs/"
    },
    {
        "content": "Dear friends,Inexpensive token generation and agentic workflows for large language models (LLMs) open up intriguing new possibilities for training LLMs on synthetic data. Pretraining an LLM on its own directly generated responses to prompts doesn't help. But if an agentic workflow implemented with the LLM results in higher quality output than the LLM can generate directly, then training on that output becomes potentially useful.Just as humans can learn from their own thinking, perhaps LLMs can, too. For example, imagine a math student who is learning to write mathematical proofs. By solving a few problems — even without external input — they can reflect on what does and doesn’t work and, through practice, learn how to more quickly generate good proofs. Broadly, LLM training involves (i) pretraining (learning from unlabeled text data to predict the next word) followed by (ii) instruction fine-tuning (learning to follow instructions) and (iii) RLHF/DPO tuning to align the LLM’s output to human values. Step (i) requires many orders of magnitude more data than the other steps. For example, Llama 3 was pretrained on over 15 trillion tokens, and LLM developers are still hungry for more data. Where can we get more text to train on? Many developers train smaller models directly on the output of larger models, so a smaller model learns to mimic a larger model’s behavior on a particular task. However, an LLM can’t learn much by training on data it generated directly, just like a supervised learning algorithm can’t learn from trying to predict labels it generated by itself. Indeed, training a model repeatedly on the output of an earlier version of itself can result in model collapse. However, an LLM wrapped in an agentic workflow may produce higher-quality output than it can generate directly. In this case, the LLM’s higher-quality output might be useful as pretraining data for the LLM itself. Efforts like these have precedents:When using  reinforcement learning to play a game like chess, a model might learn a function that evaluates board positions. If we apply game tree search along with a low-accuracy evaluation function, the model can come up with more accurate evaluations. Then we can train that evaluation function to mimic these more accurate values.In the alignment step, Anthropic’s constitutional AI method uses RLAIF (RL from AI Feedback) to judge the quality of LLM outputs, substituting feedback generated by an AI model for human feedback. A significant barrier to using LLMs prompted via agentic workflows to produce their own training data is the cost of generating tokens. Say we want to generate 1 trillion tokens to extend a pre-existing training dataset. Currently, at publicly announced prices, generating 1 trillion tokens using GPT-4-turbo ($30 per million output tokens), Claude 3 Opus ($75), Gemini 1.5 Pro ($21), and Llama-3-70B on Groq ($0.79) would cost, respectively, $30M, $75M, $21M and $790K. Of course, an agentic workflow that uses a design pattern like Reflection would require generating more than one token per token that we would use as training data. But budgets for training cutting-edge LLMs easily surpass $100M, so spending a few million dollars more for data to boost performance is quite feasible.That’s why I believe agentic workflows will open up intriguing new opportunities for high-quality synthetic data generation. Keep learning!Andrew",
        "summary": "*   Agentic workflows implemented with LLMs can potentially generate higher-quality synthetic data for training, unlike training on direct LLM output which can cause model collapse.\n*   LLM training involves pretraining (e.g., Llama 3 on over 15 trillion tokens), instruction fine-tuning, and alignment (RLHF/DPO/RLAIF).\n*   Generating 1 trillion tokens of synthetic data via agentic workflows using current LLMs incurs costs ranging from $790K (Llama-3-70B on Groq) to $75M (Claude 3 Opus), which may be feasible within large LLM training budgets.\n*   Using agentic workflows for training data generation has precedents in RL (e.g., game tree search improving evaluation functions) and AI alignment (e.g., Constitutional AI using RLAIF).",
        "publish_date": "May 8, 2024",
        "keywords": [
            "Agentic workflows",
            "Synthetic data",
            "RLAIF",
            "Llama 3",
            "Claude 3 Opus"
        ],
        "url": "https://www.deeplearning.ai/the-batch/beware-bad-arguments-against-open-source/"
    },
    {
        "content": "Dear friends,In the last couple of days, Google announced a doubling of Gemini Pro 1.5's input context window from 1 million to 2 million tokens, and OpenAI released GPT-4o, which generates tokens 2x faster and 50% cheaper than GPT-4 Turbo and natively accepts and generates multimodal tokens. I view these developments as the latest in an 18-month trend. Given the improvements we've seen, best practices for developers have changed as well.Since the launch of ChatGPT in November 2022, with key milestones that include the releases of GPT-4, Gemini 1.5 Pro, Claude 3 Opus, and Llama 3-70B, many model providers have improved their capabilities in two important ways: (i) reasoning, which allows LLMs to think through complex concepts and and follow complex instructions; and (ii) longer input context windows.The reasoning capability of GPT-4 and other advanced models makes them quite good at interpreting complex prompts with detailed instructions. Many people are used to dashing off a quick, 1- to 2-sentence query to an LLM. In contrast, when building applications, I see sophisticated teams frequently writing prompts that might be 1 to 2 pages long (my teams call them “mega-prompts”) that provide complex instructions to specify in detail how we’d like an LLM to perform a task. I still see teams not going far enough in terms of writing detailed instructions. For an example of a moderately lengthy prompt, check out Claude 3’s system prompt. It’s detailed and gives clear guidance on how Claude should behave.This is a very different style of prompting than we typically use with LLMs’ web user interfaces, where we might dash off a quick query and, if the response is unsatisfactory, clarify what we want through repeated conversational turns with the chatbot.Further, the increasing length of input context windows has added another technique to the developer’s toolkit. GPT-3 kicked off a lot of research on few-shot in-context learning. For example, if you’re using an LLM for text classification, you might give a handful — say 1 to 5 examples — of text snippets and their class labels, so that it can use those examples to generalize to additional texts. However, with longer input context windows — GPT-4o accepts 128,000 input tokens, Claude 3 Opus 200,000 tokens, and Gemini 1.5 Pro 1 million tokens (2 million just announced in a limited preview) — LLMs aren’t limited to a handful of examples. With many-shot learning, developers can give dozens, even hundreds of examples in the prompt, and this works better than few-shot learning.When building complex workflows, I see developers getting good results with this process:Write quick, simple prompts and see how it does.Based on where the output falls short, flesh out the prompt iteratively. This often leads to a longer, more detailed, prompt, perhaps even a mega-prompt.If that’s still insufficient, consider few-shot or many-shot learning (if applicable) or, less frequently, fine-tuning.If that still doesn’t yield the results you need, break down the task into subtasks and apply an agentic workflow.I hope a process like this will help you build applications more easily. If you’re interested in taking a deeper dive into prompting strategies, I recommend the Medprompt paper, which lays out a complex set of prompting strategies that can lead to very good results.Keep learning!Andrew P.S. Two new short courses:“Multi AI Agent Systems with crewAI” taught by crewAI Founder and CEO João Moura: Learn to take a complex task and break it into subtasks for a team of specialized agents. You’ll learn how to design agent roles, goals, and tool sets, and decide how the agents collaborate (such as which agents can delegate to other agents). You'll see how a multi-agent system can carry out research, write an article, perform financial analysis, or plan an event. Architecting multi-agent systems requires a new mode of thinking that's more like managing a team than chatting with LLMs. Sign up here!“Building Multimodal Search and RAG” taught by Weaviate's Sebastian Witalec: In this course, you'll create RAG systems that reason over contextual information across text, images and video. You will learn how to train multimodal embedding models to map similar data to nearby vectors, so as to carry out semantic search across multiple modalities, and learn about visual instruction tuning to add image capabilities to large language models. Sign up here!",
        "summary": "*   Recent LLM advancements include Google doubling Gemini Pro 1.5's input context window from 1 million to 2 million tokens and OpenAI's GPT-4o generating tokens 2x faster, 50% cheaper than GPT-4 Turbo, and natively accepting/generating multimodal tokens.\n*   LLMs have improved in reasoning and longer input context windows (e.g., GPT-4o 128,000, Claude 3 Opus 200,000, Gemini 1.5 Pro 1-2 million tokens), enabling complex instructions via detailed prompts and many-shot learning which works better than few-shot learning.\n*   Effective LLM application development involves iterative prompt refinement, potentially using many-shot learning, fine-tuning, or agentic workflows where tasks are broken into subtasks for specialized agents.\n*   Emerging areas include multi AI agent systems (e.g., with crewAI) and building multimodal search and RAG systems (e.g., with Weaviate) using multimodal embedding models and visual instruction tuning.",
        "publish_date": "May 15, 2024",
        "keywords": [
            "GPT-4o",
            "Gemini 1.5 Pro",
            "RAG systems",
            "crewAI",
            "Weaviate"
        ],
        "url": "https://www.deeplearning.ai/the-batch/from-prompts-to-mega-prompts/"
    },
    {
        "content": "Dear friends,Inexpensive token generation and agentic workflows for large language models (LLMs) open up intriguing new possibilities for training LLMs on synthetic data. Pretraining an LLM on its own directly generated responses to prompts doesn't help. But if an agentic workflow implemented with the LLM results in higher quality output than the LLM can generate directly, then training on that output becomes potentially useful.Just as humans can learn from their own thinking, perhaps LLMs can, too. For example, imagine a math student who is learning to write mathematical proofs. By solving a few problems — even without external input — they can reflect on what does and doesn’t work and, through practice, learn how to more quickly generate good proofs. Broadly, LLM training involves (i) pretraining (learning from unlabeled text data to predict the next word) followed by (ii) instruction fine-tuning (learning to follow instructions) and (iii) RLHF/DPO tuning to align the LLM’s output to human values. Step (i) requires many orders of magnitude more data than the other steps. For example, Llama 3 was pretrained on over 15 trillion tokens, and LLM developers are still hungry for more data. Where can we get more text to train on? Many developers train smaller models directly on the output of larger models, so a smaller model learns to mimic a larger model’s behavior on a particular task. However, an LLM can’t learn much by training on data it generated directly, just like a supervised learning algorithm can’t learn from trying to predict labels it generated by itself. Indeed, training a model repeatedly on the output of an earlier version of itself can result in model collapse. However, an LLM wrapped in an agentic workflow may produce higher-quality output than it can generate directly. In this case, the LLM’s higher-quality output might be useful as pretraining data for the LLM itself. Efforts like these have precedents:When using  reinforcement learning to play a game like chess, a model might learn a function that evaluates board positions. If we apply game tree search along with a low-accuracy evaluation function, the model can come up with more accurate evaluations. Then we can train that evaluation function to mimic these more accurate values.In the alignment step, Anthropic’s constitutional AI method uses RLAIF (RL from AI Feedback) to judge the quality of LLM outputs, substituting feedback generated by an AI model for human feedback. A significant barrier to using LLMs prompted via agentic workflows to produce their own training data is the cost of generating tokens. Say we want to generate 1 trillion tokens to extend a pre-existing training dataset. Currently, at publicly announced prices, generating 1 trillion tokens using GPT-4-turbo ($30 per million output tokens), Claude 3 Opus ($75), Gemini 1.5 Pro ($21), and Llama-3-70B on Groq ($0.79) would cost, respectively, $30M, $75M, $21M and $790K. Of course, an agentic workflow that uses a design pattern like Reflection would require generating more than one token per token that we would use as training data. But budgets for training cutting-edge LLMs easily surpass $100M, so spending a few million dollars more for data to boost performance is quite feasible.That’s why I believe agentic workflows will open up intriguing new opportunities for high-quality synthetic data generation. Keep learning!Andrew",
        "summary": "*   Agentic workflows implemented with LLMs can potentially generate higher-quality synthetic data for training, unlike training on direct LLM output which can cause model collapse.\n*   LLM training involves pretraining (e.g., Llama 3 on over 15 trillion tokens), instruction fine-tuning, and alignment (RLHF/DPO/RLAIF).\n*   Generating 1 trillion tokens of synthetic data via agentic workflows using current LLMs incurs costs ranging from $790K (Llama-3-70B on Groq) to $75M (Claude 3 Opus), which may be feasible within large LLM training budgets.\n*   Using agentic workflows for training data generation has precedents in RL (e.g., game tree search improving evaluation functions) and AI alignment (e.g., Constitutional AI using RLAIF).",
        "publish_date": "May 1, 2024",
        "keywords": [
            "Agentic workflows",
            "Synthetic data",
            "RLAIF",
            "Llama 3",
            "Claude 3 Opus"
        ],
        "url": "https://www.deeplearning.ai/the-batch/building-models-that-learn-from-themselves/"
    },
    {
        "content": "Dear friends,While AI is a general-purpose technology that’s useful for many things, it isn’t good for every task under the sun. How can we decide which concrete use cases to build? If you’re helping a business figure out where to apply AI, I’ve found the following recipe useful as a brainstorming aid:Consider the jobs of the company’s employees and contractors, and break down the jobs into tasks.Examine each commonly done task to see if it’s amenable to either assistance (augmentation) or automation using AI tools such as supervised learning or generative AI.Assess the value of doing so.Rather than thinking of AI as automating jobs — a common narrative in the popular press and in conversations about AI leading to job losses — it’s more useful to think about jobs as collections of tasks, and to analyze AI’s ability to augment or automate individual tasks. This approach is based on a method developed by Erik Brynjolfsson, Tom Mitchell, and Daniel Rock for understanding the impact of AI on the economy. Other researchers have used it to understand the impact of generative AI. Workhelix, an AI Fund portfolio company co-founded by Brynjolfsson, Andrew McAfee, James Milin, and Rock, uses it to help enterprises asses their generative AI opportunities.In addition to economic analyses, I’ve found this approach useful for brainstorming project ideas. For example, how can AI be used to automate software businesses? Can it do the job of a computer programmer?Typically, we think of computer programmers as writing code, but actually they perform a variety of tasks. According to O*NET, an online database of jobs and their associated tasks sponsored by the U.S. Department of Commerce, programmers perform 17 tasks. These include:Writing programsDebuggingConsulting with others to clarify program intentConducting trial runs of programsWriting documentationand so on. Clearly systems like GitHub Copilot can automate some writing of code. Automating the writing of documentation may be much easier, so an AI team building tools for programmers might consider that too. However, if consulting to clarify the intent behind a program turns out to be hard for AI, we might assign that a lower priority.Another example: Can AI do the job of a radiologist? When thinking through AI’s impact on a profession, many people gravitate to the tasks that are most unique about that profession, such as interpreting radiological images. But according to O*NET, radiologists carry out 30 tasks. By taking a broader look at these tasks, we might identify ones that are easier or more valuable to automate. For example, while AI has made exciting progress in interpreting radiological images, part of this task remains challenging to fully automate. Are there other tasks on the list that might be more amenable to automation, such as obtaining patient histories?O*NET listings are a helpful starting point, but they’re also a bit generic. If you’re carrying out this type of analysis, you’re likely to get better results if you capture an accurate understanding of tasks carried out by employees of the specific company you’re working with.An unfortunate side effect of this approach is that it tends to find human tasks to automate rather than creative applications that no one is working on. Brynjolfsson laments that this leads to the Turing Trap whereby we tend to use AI to do human work rather than come up with tasks no human is doing. But sometimes, if we can do something that humans do but do it 10,000x faster and cheaper, it changes the nature of the business. For example, email automated the task of transmitting messages. But it didn’t make the postal system cheaper; instead it changed what and how frequently we communicate. Web search automated the task of finding articles. Not only did this make librarians more effective, it also changed how we access information. So even if AI tackles a task that humans perform, it could still lead to revolutionary change for a business.Many jobs in which some tasks can be automated aren’t likely to go away. Instead, AI will augment human labor while humans continue to focus on the things they do better. However, jobs that are mostly or fully automatable may disappear, putting people out of work. In such cases, as a society, we have a duty to take care of the people whose livelihoods are affected, to make sure they have a safety net and an opportunity to reskill and keep contributing. Meanwhile, lowering the cost of delivering certain services is bound to increase the demand for some jobs, just as the invention of the car led to a huge explosion in the number of driving jobs. In this way, AI will create many jobs as well as destroy some.Some programmers worry that generative AI will automate their jobs. However, programming involves enough different tasks, some of which are hard to automate, that I find it very unlikely that AI will automate these jobs anytime soon. Pursuing a long-term career in software is still a great choice, but we should be sure to adopt AI tools in our work. Many professions will be here for a long time, but workers who know how to use AI effectively will replace workers who don’t.I hope you find this framework useful when you’re coming up with ideas for AI projects. If our projects affect someone else’s work, let’s work hard to protect people’s livelihoods. I hope that by building AI systems, we can create — and fairly share — value for everyone.Keep learning!Andrew",
        "summary": "*   A strategy for applying AI involves breaking jobs into tasks, assessing task amenability to AI (augmentation/automation using supervised learning or generative AI), and evaluating value.\n*   Analyzing AI's impact is more useful at the task level within jobs, a method based on research by Erik Brynjolfsson, Tom Mitchell, and Daniel Rock.\n*   Examples like programmers (17 tasks) and radiologists (30 tasks) illustrate that AI can automate specific tasks (e.g., code writing, documentation) even if the full job isn't automatable.\n*   AI will augment human labor by automating tasks, potentially displacing jobs that are mostly/fully automatable, but also creating new jobs; workers using AI effectively will be favored.",
        "publish_date": "Sep 20, 2023",
        "keywords": [
            "Supervised Learning",
            "Generative AI",
            "Task-level analysis",
            "AI Automation",
            "AI Augmentation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/which-ai-applications-should-you-build/"
    },
    {
        "content": "Dear friends,I recently spoke about “Opportunities in AI” at Stanford’s Graduate School of Business. I'd like to share a few observations from that presentation, and I invite you to watch the video (37 minutes).AI is a collection of tools, including supervised learning, unsupervised learning, reinforcement learning, and now generative AI. All of these are general-purpose technologies, meaning that — similar to other general-purpose technologies like electricity and the internet — they are useful for many different tasks. It took many years after deep learning started to work really well circa 2010 to identify and build for a wide range of use cases such as online advertising, medical diagnosis, driver assistance, and shipping optimization. We’re still a long way from fully exploiting supervised learning.Now that we have added generative AI to our toolbox, it will take years more to explore all its uses. (If you want to learn how to build applications using generative AI, please check out our short courses!)Where do the opportunities lie? With each new wave of technology, entrepreneurs and investors focus a lot of attention on providers of infrastructure and tools for developers. The generative AI wave has brought tools from AWS, Google Cloud, Hugging Face, Langchain, Microsoft, OpenAI, and many more. Some will be huge winners in this area. However, the sheer amount of attention makes this part of the AI stack hypercompetitive. My teams (specifically AI Fund) build startups in infrastructure and tools only when we think we have a significant technology advantage, because that gives us a shot at building large, sustainable businesses.But I believe a bigger opportunity lies in the application layer. Indeed, for the companies that provide infrastructure and developer tools to do well, the application companies that use these products must perform even better. After all, the application companies need to generate enough revenue to pay the tool builders.For example, AI Fund portfolio companies are applying AI to applications as diverse as global maritime shipping and relationship mentoring. These are just two areas where the general-purpose technology of AI can create enormous value. Because few teams have expertise in both AI and sectors like shipping or relationships, the competition is much less intense.If you’re interested in building valuable AI projects, I think you’ll find the ideas in the presentation useful. I hope you’ll watch the video and share it with your friends. It describes in detail AI Fund’s recipe for building startups and offers non-intuitive tips on the ideas that we’ve found to work best.Keep building!Andrew",
        "summary": "*   AI is a collection of general-purpose technologies including supervised learning, unsupervised learning, reinforcement learning, and generative AI.\n*   It takes years to explore uses for AI technologies; supervised learning (working well circa 2010) is still not fully exploited, and generative AI will also take years.\n*   Opportunities exist in the hypercompetitive infrastructure/tools layer (e.g., AWS, Google Cloud, Hugging Face, Langchain, Microsoft, OpenAI) and the less competitive application layer.\n*   Bigger opportunities lie in the application layer, applying AI to specific sectors like global maritime shipping and relationship mentoring, where competition is less intense.",
        "publish_date": "Sep 13, 2023",
        "keywords": [
            "supervised learning",
            "unsupervised learning",
            "reinforcement learning",
            "Hugging Face",
            "Langchain"
        ],
        "url": "https://www.deeplearning.ai/the-batch/where-are-the-opportunities-in-ai/"
    },
    {
        "content": "Dear friends,Improvements in chatbots have opened a market for bots integrated with dating apps. I’m excited about the possibilities for large language models (LLMs) in romantic relationships, but I’m concerned that AI romantic partners create fake relationships that displace, rather than strengthen, meaningful human relationships. In my recent Stanford presentation on “Opportunities in AI,” I mentioned that AI Fund has been working with Renate Nyborg to deliver romantic mentoring. I’d like to explain why, despite my concern, I believe that AI can help many people with relationships.By 2020, it was clear that a change was coming in how we build natural language processing applications. As I wrote in The Batch that September, “GPT-3 is setting a new direction for building language models and applications. I see a clear path toward scaling up computation and algorithmic improvements.” Today, we’re much farther down that path.I didn't know back then that ChatGPT would go viral upon its release in November 2022. But AI Fund entrepreneurs were already experimenting with GPT-3, and we started looking for opportunities to build businesses on it. I had read the academic work about questions that lead to love. I believe that you don’t find a great relationship; you create it. So instead of trying to help you find a great partner — as most dating apps aim to do — why not use AI to help people create great relationships?I’m clearly not a subject-matter expert in relationships (despite having spent many hours on eHarmony when I was single)! So I was fortunate to meet Renate, former CEO of Tinder, and start working with her on what became Meeno (formerly Amorai). Although we started exploring these ideas before ChatGPT was released, the wave of interest since then has been a boon to the project.Renate has far more systematic knowledge about relationships than anyone I know. With AI Fund’s LLM expertise and her relationship expertise (though she knows a lot about AI, too!), Her team built Meeno, a relationship mentor that is helping people improve how they approach relationships.Meeno is not a synthetic romantic partner, like in the movie Her. Instead, its goal is to be like the mentor rat in Ratatouille: It assists individuals in building better relationships. If a user asks Meeno how to handle a breakup, it responds with advice about communicating honestly, empathetically, and clearly. After using it for a while, hopefully, users no longer will need guidance.I’m excited about Meeno for a few reasons. I have been concerned for some time about the “synthetic boyfriend/girlfriend” industry, where chatbots act like someone’s relationship partner, and then sometimes manipulate people’s emotions for profit in ways that I find deeply troubling (such as offering racy pictures for a fee). Social media, and TV before it, consumes enormous amounts of time that people otherwise might spend building interpersonal relationships. This makes me worry about synthetic romantic partners displacing real ones.The U.S. Surgeon General has raised the alarm about an epidemic of loneliness and isolation. Loneliness is as bad for a person as smoking 15 cigarettes a day. It’s linked to significantly worse physical and mental health and to premature death. I hope Meeno will have a positive impact on this problem.Meeno’s journey is still in its early stages. You can read more about it here.Keep learning!AndrewP.S. AI-savvy programmers are coding very differently than they did a year ago: They’re using large language models to help with their work. You’ll learn many of the emerging best practices in “Pair Programming with a Large Language Model,” taught by Laurence Moroney, AI Advocacy Lead at Google and instructor of our TensorFlow Specializations. This short course covers using LLMs to simplify and improve your code, assist with debugging, and minimize technical debt by having AI document and explain your code while you write it. This is an important shift in programming that every developer should stay on top of. Please check out the course here.This post originally appeared in the September 27, 2023 edition of The Batch.",
        "summary": "*   AI/LLMs are being applied to relationship mentoring (e.g., Meeno) to help people build better human relationships, contrasting with concerns about synthetic AI partners displacing real ones.\n*   A significant shift in NLP application development since 2020, driven by models like GPT-3, is noted, enabling scaling computation and algorithmic improvements, with ChatGPT's viral release in November 2022 boosting interest.\n*   Meeno (formerly Amorai) is presented as an AI relationship mentor, built using LLM expertise, designed to assist users in improving their relationship approach rather than acting as a synthetic partner.\n*   Programmers are increasingly using LLMs for coding tasks, including simplifying/improving code, debugging, and minimizing technical debt through automated documentation/explanation.",
        "publish_date": "Sep 27, 2023",
        "keywords": [
            "Meeno",
            "GPT-3",
            "ChatGPT",
            "NLP",
            "LLMs"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-llms-can-cure-epidemic-loneliness/"
    },
    {
        "content": "Dear friends,In April, DeepLearning.AI launched a short course, “ChatGPT Prompt Engineering for Developers,” taught by OpenAI’s Isa Fulford and me.I’m thrilled to announce three more short courses, available today:“Building Systems with the ChatGPT API” taught by returning instructor Isa Fulford and me: This course goes beyond writing individual prompts and shows you how to break down a complex task — such as building a customer-service assistant system — into simpler tasks that you can accomplish via multiple API calls to a large language model (LLM). You’ll also learn how to check LLM outputs for safety and accuracy and how to systematically evaluate the quality of an LLM’s output to drive iterative improvements. You’ll come away with a deeper understanding of how LLMs work (including tokenization and how the chat format works) and how this affects your applications, and gain a solid foundation for building applications using LLMs.“LangChain for LLM Application Development” taught by LangChain CEO Harrison Chase and me: LangChain is a powerful open-source tool for building applications using LLMs. Complex applications — for example, a QA (Question Answering) system to answer queries about a text document — require prompting an LLM multiple times, parsing the output to feed to downstream prompts, and so on; thus, there’s a lot of “glue” code needed. You’ll learn how to use LangChain’s tools to make these operations easy. We also discuss the cutting-edge (and experimental) agents framework for using an LLM as a reasoning engine that can decide for itself what steps to take next, such as when to call an external subroutine.“How Diffusion Models Work” taught by Lamini CEO Sharon Zhou: Diffusion models enable Midjourney, DALL·E 2, and Stable Diffusion to generate beautiful images from a text prompt. This technical course walks you through the details of how they work, including how to (i) add noise to training images to go from image to pure noise, (ii) train a U-Net neural network to estimate the noise so as to subtract it off, (iii) add input context so that you can tell the network what to generate, and (iv) use the DDIM technique to significantly speed up inference. You’ll go through code to generate 16x16-pixel sprites (similar to characters in 8-bit video games). By the end, you’ll understand how diffusion models work and how to adapt them to applications you want to build. You’ll also have code that you can use to generate your own sprites!The first two courses are appropriate for anyone who has basic familiarity with Python. The third is more advanced and additionally assumes familiarity with implementing and training neural networks.Each of these courses can be completed in around 1 to 1.5 hours, and I believe they will be a worthy investment of your time. I hope you will check them out, and — if you haven’t yet— join the fast-growing community of developers who are building applications using Generative AI!Keep learning,Andrew",
        "summary": "*   DeepLearning.AI launched three new short courses: \"Building Systems with the ChatGPT API\" focusing on complex LLM systems with multiple API calls, output checking, and evaluation.\n*   The \"LangChain for LLM Application Development\" course covers using the LangChain open-source tool for complex LLM applications, including managing multiple prompts and the agents framework.\n*   The \"How Diffusion Models Work\" course details diffusion models for image generation, covering noise handling, U-Net, context addition, the DDIM technique, and generating 16x16-pixel sprites.",
        "publish_date": "May 31, 2023",
        "keywords": [
            "ChatGPT API",
            "LangChain",
            "Diffusion Models",
            "U-Net",
            "DDIM technique"
        ],
        "url": "https://www.deeplearning.ai/the-batch/three-new-courses-on-generative-ai/"
    },
    {
        "content": "Dear friends,Last week, safe.org asserted that “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” This statement was signed by AI scientists who I really respect including Yoshua Bengio and Geoffrey Hinton. It received widespread media coverage.I have to admit that I struggle to see how AI could pose any meaningful risk for our extinction. AI has risks like bias, fairness, inaccurate outputs, job displacement, and concentration of power. But I see AI’s net impact as a massive contribution to society. It’s saving lives by improving healthcare and making cars safer, improving education, making healthy food and numerous other goods and services more affordable, and democratizing access to information. I don’t understand how it can lead to human extinction.A number of thoughtful commentators have also pushed back on the extinction narrative. For example:Chris Manning points out that the AI community has a large, quiet majority that’s focused on building useful software and does not share the views of the loud AI Safety crowd that talks about existential risks. It believes the risks can be mitigated.Emily Bender notes that AI doomsaying is a huge distraction from the technology’s real harms, which she lists as “discrimination, surveillance, pollution of the information ecosystem, data theft, labor exploitation.”Along this vein, Matteo Wong in The Atlantic argues that “AI doomerism is a decoy.” It appears to me that time spent by regulators stopping AI from autonomously launching nuclear weapons — which no nuclear power has publicly considered — is time that they’re not spending passing regulations on data privacy, AI transparency or anti-trust that would be less convenient for tech companies and might negatively affect their bottom line.Marc Andreessen wrote an essay on the benefits of AI. While my perspective differs from his on some points (for example, I’m more worried than he is about the negative impact of  job displacement), he makes a sound argument that each time a new technology has been introduced, a predictable moral panic has taken hold. Examples are documented by the fascinating website pessimistsarchive.org (worth a look!), which describes fear of non-fiction novels corrupting youth, elevators causing brain fever, cars (“the devil wagon”) on a mission to destroy the world, and recorded sound harming babies. With the rise of deep learning about 10 years ago, Elon Musk, Bill Gates and Stephen Hawking warned of the existential risk stemming from AI. The current wave of fears about AI feels similar to me, but it’s more intense and has buy-in from prominent scientists.I’m glad to see others presenting a sensible alternative to the narrative of AI as an extinction risk. Having said that, though, I feel an ethical responsibility to keep an open mind and make sure I really understand the risk — especially given the high regard I have for some who think AI does pose this risk.To learn more, I’m speaking with a few people who I think might have a thoughtful perspective on how AI creates a risk of human extinction, and I will report back with my findings. In the meantime, I would love to hear your thoughts as well. Please reply to my posts on Twitter or LinkedIn if there’s someone you think I should speak with or if you’d like to share your perspective. Through this, I hope we can have a real conversation about whether AI really poses an extinction risk.I look forward to continuing the discussion with you,Andrew",
        "summary": "* A statement signed by AI scientists asserts that mitigating the risk of extinction from AI should be a global priority alongside pandemics and nuclear war.\n* The author struggles to see AI as an extinction risk, viewing its net impact as a massive societal contribution, while acknowledging risks like bias, fairness, inaccurate outputs, job displacement, and concentration of power.\n* Commentators argue the AI extinction narrative is a distraction from real harms (discrimination, surveillance, pollution, data theft, labor exploitation) and that most AI researchers focus on building useful software and mitigating risks.\n* The author notes that fears about new technology, including previous AI warnings, are a predictable moral panic, citing historical examples.",
        "publish_date": "Jun 07, 2023",
        "keywords": [
            "Bias",
            "Fairness",
            "Inaccurate outputs",
            "Discrimination",
            "Surveillance"
        ],
        "url": "https://www.deeplearning.ai/the-batch/existential-risk-i-dont-get-it/"
    },
    {
        "content": "Dear friends, AI has an Instagram problem. Just as Instagram’s parade of perfect physiques makes many people   they don’t measure up, AI’s parade of exciting projects makes many people feel their own projects are lacking. Just as pictures of people’s perfect lives in the media aren’t representative, pictures of AI developers’ postings of their amazing projects also aren’t representative. I’m here to say: Judge your projects according to your own standard, and don’t let the shiny objects make you doubt the worth of your work! Over the years, I’ve occasionally felt this way, too, and wondered if I was working on a fruitful direction. A few years ago, when reinforcement learning (RL) made progress on Atari games, Alpha Go was in the headlines, and RL videos using OpenAI Gym circulated on social media, I was still focused on supervised learning. Part of me wondered if I was missing out. It certainly did not help when friends kept asking me about the cool RL work they read about in the news. Fortunately, I ignored the feeling that the grass might be greener on the other side and stuck to what I was excited about. AI develops so quickly that waves of new ideas keep coming: quantum AI, self-supervised learning, transformers, diffusion models, large language models, and on and on. Some, like quantum AI, have had essentially no impact in applications so far. Others have already had a huge impact. Because our field evolves, it is important to keep learning and ride the waves of change. For the record, I think large language models (LLMs) like ChatGPT (and, to a significant but lesser extent, diffusion models, best known for generating images) will have a transformative impact on AI, but they are far from the only things that will be important. Someone else’s sculpted physique does not take away from your beauty. And the emergence of a hot new technology doesn’t mean your current project isn’t also valuable, assuming it’s technically sound, has a reasonable expectation of impact, and isn’t made obsolete by newer technology (which doesn’t happen very often). Projects of all shapes and sizes can be wonderful, and what’s buzzy today is only one of the many things that will prove valuable in the future. I'm not advising you to ignore the news. Paying attention to new developments in AI not only helps you stay on top of the field but also can inspire you. Being inspired by Instagram is fine, but changing your life because of FOMO is less helpful. So, if what you’re working on makes sense to you, maintain your faith and keep going! Maybe you’re training XGBoost on a structured dataset and wondering if you’re missing out on ChatGPT. You may well be onto something even if XGBoost isn’t in the news. After all, think about how all the LLM researchers must have felt a few years ago, when everyone was buzzing about RL. Keep learning! Andrew",
        "summary": "*   The text describes a phenomenon where AI developers feel their projects are inadequate compared to highly publicized \"shiny\" AI projects (e.g., RL, AlphaGo, LLMs, diffusion models).\n*   It lists various AI concepts and development waves, including reinforcement learning (RL), supervised learning, quantum AI, self-supervised learning, transformers, diffusion models, and large language models (LLMs).\n*   It states that some new AI areas (like quantum AI) have had little application impact, while others (LLMs like ChatGPT, and diffusion models) are expected to have a transformative impact.\n*   It suggests that valuable AI work exists across different areas (e.g., training XGBoost on structured data) and isn't limited to the currently buzziest technologies.\n*   It advises AI practitioners to keep learning about new developments for inspiration but focus on projects that make sense to them, rather than chasing trends out of FOMO.",
        "publish_date": "Feb 15, 2023",
        "keywords": [
            "AlphaGo",
            "ChatGPT",
            "Diffusion models",
            "Transformers",
            "XGBoost"
        ],
        "url": "https://www.deeplearning.ai/the-batch/someone-elses-cool-ai-project-doesnt-make-your-project-less-valuable/"
    },
    {
        "content": "Dear friends,As you can read in this issue of The Batch, Microsoft’s effort to reinvent web search by adding a large language model snagged when its chatbot went off the rails. Both Bing chat and Google’s Bard, the chatbot to be added to Google Search, have made up facts. In a few disturbing cases, Bing demanded apologies or threatened a user. What is the future of chatbots in search?It’s important to consider how this technology will evolve. After all, we should architect our systems based not only on what AI can do now but on where it might go. Even though current chat-based search has problems, I’m optimistic that roadmaps exist to significantly improve it.Let’s start with the tendency of large language models (LLMs) to make up facts. I wrote about falsehoods generated by OpenAI’s ChatGPT. I don’t see a realistic path to getting an LLM with a fixed set of parameters to both (i) demonstrate deep and broad knowledge about the world and (ii) be accurate most of the time. A 175B-parameter model simply doesn’t have enough memory to know that much.Look at the problem in terms of human-level performance. I don’t think anyone could train an inexperienced high-school intern to answer every question under the sun without consulting reference sources. But an inexperienced intern could be trained to write reports with the aid of web search. Similarly, the approach known as retrieval augmented generation — which enables an LLM to carry out web searches and refer to external documents — offers a promising path to improving factual accuracy.Bing chat and Bard do search the web, but they don’t yet generate outputs consistent with the sources they’ve discovered. I’m confident that further research will lead to progress on making sure LLMs generate text based on trusted sources. There’s significant momentum behind this goal, given the widespread societal attention focused on the problem, deep academic interest, and financial incentive for Google and Microsoft (as well as startups like You.com) to improve their models.Indeed, over a decade of NLP research has been devoted to the problem of textual entailment which, loosely, is the task of deciding whether Sentence A can reasonably be inferred to follow from some Sentence B. LLMs could take advantage of variations on these techniques — perhaps to double-check that their output is consistent with a trusted source — as well as new techniques yet to be invented.As for personal attacks, threats, and other toxic output, I’m confident that a path also exists to significantly reduce such behaviors. LLMs, at their heart, simply predict the next word in a sequence based on text they were trained on. OpenAI shaped ChatGPT’s output by fine-tuning it on a dataset crafted by people hired to write conversations, and Google built a chatbot, Sparrow, that learned to follow rules through a variation on reinforcement learning from human feedback. Using techniques like these, I have little doubt that chatbots can be made to behave better.So, while Bing’s misbehavior has been in the headlines, I believe that chat-based search has a promising future — not because of what the technology can do today, but because of where it will go tomorrow.Keep learning!AndrewP.S. Landing AI, which I lead, just released its computer vision platform for everyone to use. I’ll say more about this next week. Meanwhile, I invite you to check it out for free at landing.ai!",
        "summary": "*   Current chat-based search (Bing, Bard) exhibits issues such as making up facts and generating toxic output.\n*   A fixed-parameter LLM, like a 175B-parameter model, may lack sufficient memory for broad, accurate knowledge, contributing to generating falsehoods.\n*   Retrieval augmented generation, enabling LLMs to search and refer to external documents, offers a promising path to improving factual accuracy.\n*   Techniques such as fine-tuning on curated datasets or reinforcement learning from human feedback can significantly reduce toxic output in chatbots.",
        "publish_date": "Feb 22, 2023",
        "keywords": [
            "Retrieval augmented generation",
            "reinforcement learning from human feedback",
            "fine-tuning",
            "175B-parameter model",
            "Bing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-bing-unruly-chatbot-means-for-the-future-of-search/"
    },
    {
        "content": "Dear friends,My team at Landing AI just announced a new tool for quickly building computer vision models, using a technique we call Visual Prompting. It’s a lot of fun! I invite you to try it.Visual Prompting takes ideas from text prompting — which has revolutionized natural language processing — and applies them to computer vision.To build a text sentiment classifier, in the traditional machine learning workflow, you have to collect and label a training set, train a model, and deploy it before you start getting predictions. This process can take days or weeks.In contrast, in the prompt-based machine learning workflow, you can write a text prompt and, by calling a large language model API, start making predictions in seconds or minutes.Traditional workflow: Collect and label -> Train -> PredictPrompt-based workflow: Prompt -> PredictTo explain how these ideas apply to computer vision, consider the task of recognizing cell colonies (which look like white blobs) in a petri dish, as shown in the image below. In the traditional machine learning workflow, using object detection, you would have to label all the cell colonies, train a model, and deploy it. This works, but it’s slow and tedious.In contrast, with Visual Prompting, you can create a “visual prompt” in seconds by pointing out (by painting over) one or two cell colonies in the image and similarly pointing out the background region, and get a working model. It takes only a few seconds to (i) create the visual prompt and (ii) get a result. If you’re not satisfied with the initial model, you can edit the prompt (perhaps by labeling a few more cell colonies), check the results, and keep iterating until you’re satisfied with the model’s performance.The resulting interaction feels like you’re having a conversation with the system. You’re guiding it by incrementally providing additional data in real time.Since 2017, when the paper that introduced transformers was published, rapid innovation in text processing has transformed natural language models. The paper that introduced vision transformers arrived in 2020, and similarly it led to rapid innovation in vision. Large pretrained models based on vision transformers have reached a point where, given a simple visual prompt that only partially (but unambiguously) specifies a task, they can generalize well to new images.We’re not the only ones exploring this theme. Exciting variations on Visual Prompting include Meta’s Segment Anything (SAM), which performs image segmentation, and approaches such as Generalist Painter, SegGPT, and prompting via inpainting.You can watch a livestream of my presentation on Visual Prompting or read Landing AI’s blog post on this topic.Text prompting reached an inflection point in 2020, when GPT-3 made it easy for developers to write a prompt and build a natural language processing model. I don’t know if computer vision has reached its GPT-3 moment, but we’re getting close. I’m excited by the research that’s moving us toward that moment, and I think Visual Prompting will be one key to getting us there.Keep learning!Andrew",
        "summary": "*   Visual Prompting applies text prompting concepts to computer vision, enabling rapid model building by shifting from traditional collect/label/train workflows (days/weeks) to prompt/predict workflows (seconds/minutes).\n*   The technique involves creating \"visual prompts\" (e.g., painting over examples) to guide large pretrained models, allowing for quick iteration and generalization.\n*   This approach leverages advancements in Vision Transformers (introduced 2020) and large pretrained models.\n*   Visual Prompting is part of a trend in computer vision, with related techniques including Meta's Segment Anything (SAM), Generalist Painter, and SegGPT.",
        "publish_date": "Apr 26, 2023",
        "keywords": [
            "Visual Prompting",
            "Vision Transformers",
            "Meta's Segment Anything (SAM)",
            "Generalist Painter",
            "SegGPT"
        ],
        "url": "https://www.deeplearning.ai/the-batch/visual-prompting-builds-vision-models-in-seconds/"
    },
    {
        "content": "Dear friends,The competitive landscape of large language models (LLMs) is evolving quickly. The ultimate winners are yet to be determined, and already the current dynamics are exciting. Let me share a few observations, focusing on direct-to-consumer chat interfaces and the LLM infrastructure and application layers.First, ChatGPT is a new category of product. It’s not just a better search engine, auto-complete, or something else we already knew. It overlaps with other categories, but people also use it for entirely different purposes such as writing and brainstorming. Companies like Google and Microsoft that are integrating LLMs into existing products may find that the complexity of switching not only technologies but also product categories raises unique challenges.OpenAI is clearly in the lead in offering this new product category, and ChatGPT is a compelling direct-to-consumer product. While competitors are emerging, OpenAI’s recent move to have ChatGPT support third-party plugins, if widely adopted, could make its business much more defensible, much like the app stores for iOS and Android helped make those platforms very defensible businesses.Second, the LLM infrastructure layer, which enables developers to interact with LLMs via an API, looks extremely competitive. OpenAI/Microsoft leads in this area as well, but Google and Amazon have announced their own offerings, and players such as Hugging Face, Meta, Stability AI, and many academic institutions are busy training and releasing open source models. It remains to be seen how many applications will need the power of the largest models, such as GPT-4, versus smaller (and cheaper) models offered by cloud providers or even hosted locally, like gpt4all, which runs on a desktop.Finally, the application layer, in which teams build on top of LLMs, looks less competitive and full of creativity. While many teams are piling onto “obvious” ideas — say, building question-answering bots or summarizers on top of online content — the sheer diversity of potential LLM-powered applications leaves many ideas relatively unexplored in verticals including specialized coaching and robotic process automation. AI Fund, the venture studio I lead, is working with entrepreneurs to build applications like this. Competition feels less intense when you can identify a meaningful use case and go deep to solve it.LLMs are a general-purpose technology that’s making many new applications possible. Taking a lesson from an earlier era of tech, after the iPhone came out, I paid $1.99 for an app that turned my phone into a flashlight. It was a good idea, but that business didn’t last: The app was easy for others to replicate and sell for less, and eventually Apple integrated a flashlight into iOS. In contrast, other entrepreneurs built highly valuable and hard-to-build businesses such as AirBnB, Snapchat, Tinder, and Uber, and those apps are still with us. We may already have seen this phenomenon in generative AI: Lensa grew rapidly through last December but its revenue run appears to have collapsed.Today, in a weekend hackathon, you can build a shallow app that does amazing things by taking advantage of amazing APIs. But over the long term, what excites me are the valuable solutions to hard problems that LLMs make possible. Who will build generative AI’s lasting successes? Maybe you!One challenge is that the know-how for building LLM products is still evolving. While academic studies are important, current research offers a limited view of how to use LLMs. As the InstructGPT paper says, “Public NLP datasets are not reflective of how our language models are used. . . .  [They] are designed to capture tasks that are easy to evaluate with automatic metrics.”In light of this, community is more important than ever. Talking to friends who are working on LLM products often teaches me non-intuitive tricks for improving how I use them. I will continue trying to help others wherever I can.Keep learning!AndrewP.S. On Tuesday April 25, 2023, I’ll share early ideas on Visual Prompting in a livestream on behalf of my team Landing AI. LLMs let users enter a text prompt and quickly get a text output, which has transformed natural language processing. I’m excited about taking these ideas from text to computer vision so we can let users enter a visual prompt (labeling a few pixels) and quickly get a visual output. You can sign up for the livestream here.",
        "summary": "* The competitive landscape of large language models (LLMs) is evolving, with ChatGPT establishing a new direct-to-consumer product category.\n* The LLM infrastructure layer (API access) is highly competitive (OpenAI/Microsoft lead, Google, Amazon, Hugging Face, Meta, Stability AI, academic institutions active), raising questions about the need for the largest models vs. smaller ones.\n* The LLM application layer is less competitive and creative, with potential in diverse verticals beyond obvious tasks, requiring valuable, hard-to-replicate solutions.\n* Know-how for building LLM products is evolving; academic studies and public NLP datasets offer limited real-world usage guidance (as noted in the InstructGPT paper), emphasizing community learning.\n* The concept of applying LLM prompting is extending from text-to-text to computer vision with \"Visual Prompting\" (visual prompt -> visual output).",
        "publish_date": "Apr 19, 2023",
        "keywords": [
            "ChatGPT",
            "InstructGPT paper",
            "LLM prompting",
            "Visual Prompting",
            "computer vision"
        ],
        "url": "https://www.deeplearning.ai/the-batch/opportunities-and-pitfalls-for-large-language-models/"
    },
    {
        "content": "Dear friends,An ill-advised proposal for a 6-month pause in cutting-edge AI research got far more attention than I think it deserved. To me, this is a wake-up call that AI doomsayers have done a much better job than AI optimists at framing the narrative of progress in AI.Most of the AI community is building systems that help and empower people, and we see every day how it is improving lives. Open AI’s ChatGPT is delivering value to hundreds of millions of users, and reportedly it’s the fastest-growing consumer application to date. This is wildly exciting, and I foresee many more products yet to be built that will help and empower people in other ways.Yet, while most of us have been building useful systems, AI doomsayers — who forecast unlikely scenarios such as humanity losing control of runaway AI (or AGI, or even superintelligent systems) — have captured the popular imagination and stoked widespread fear.Last week, Yann LeCun and I had an online conversation about why the proposed 6-month pause, which would temporarily suspend work on models more powerful than GPT-4, is a bad idea. You can watch the video here and read a synopsis in this article. Briefly:The proposal’s premises with respect to AI’s potential for harm are sensationalistic and unrealistic.A pause in development is unworkable— that is, unless governments intervene, which would have an even worse impact on competition and innovation.If it were implemented, it would (i) slow down valuable innovations and (ii) do little good, because it seems unlikely that a 6-month pause in our decades-long journey toward AGI would have much useful impact.To be clear, AI has problems including bias, fairness, job displacement, and concentration of power. Our community should work, and is working, to address them. However, stoking fears about speculative risks does more harm than good:It distracts us from the real and present risks that we should be working on.It is another form of hype about AI, which misleads people to overestimate AI’s capabilities.It risks slowing down further progress in AI that would be very beneficial.I’m disappointed that we have let AI doomsayers get this far. Their narrative hampers innovation, discourages individuals, and interferes with society’s ability to make good decisions.Let’s help people understand that AI is empowering people even as we work to mitigate the real risks. It’s time for us all to stand up for a realistic view of this incredibly important technology.Keep learning!AndrewP.S. Shoutout to University of Washington’s Emily Bender for her line-by-line analysis of how the proposal contributes to AI hype, and Princeton professor Arvind Narayanan, who explained how fears of AI-driven dangers such as misinformation often have been overblown.",
        "summary": "* A debate exists between AI optimists building empowering systems (like ChatGPT, noted as the fastest-growing consumer app) and doomsayers forecasting unlikely runaway AI scenarios.\n* A proposed 6-month pause on AI models more powerful than GPT-4 is criticized as having unrealistic premises, being unworkable, and hindering valuable innovation without significant impact on the long-term journey toward AGI.\n* While acknowledging real AI problems including bias, fairness, job displacement, and concentration of power, the author argues that stoking speculative fears distracts from these issues, contributes to AI hype, and risks slowing beneficial progress.",
        "publish_date": "Apr 12, 2023",
        "keywords": [
            "ChatGPT",
            "GPT-4",
            "AGI",
            "Bias",
            "Fairness"
        ],
        "url": "https://www.deeplearning.ai/the-batch/time-to-push-back-on-ai-pessimism/"
    },
    {
        "content": "Dear friends,Last week, the tech news site The Information reported an internal controversy at Google. Engineers were concerned that Google’s Bard large language model was trained in part on output from OpenAI’s ChatGPT, which would have violated OpenAI’s terms of use. The output purportedly was hosted on ShareGPT, a website where users share conversations with ChatGPT. (Google denies the report.) A decade ago, Google accused Microsoft of copying its search results to enhance Bing.Training a machine learning model on a different model’s output can be a useful technique, but it also raises engineering, business, and legal questions. When is it okay?Engineering recipes for training learning algorithms on generated data are still being developed. When I led a large automatic speech recognition (ASR) team, there were rumors — that we never proved or disproved — that a competitor was using our system to generate transcripts to train a competing system. It was said that, rather than using our ASR system’s output directly as labeled training data, our competitor used a lightweight process to manually clean up errors and make sure the data was high-quality.Lately, I’ve seen many developers experiment with use cases such as prompting a large model (say, 175B parameters) to generate high-quality outputs specialized to an application such as customer support, and using this data to fine-tune a smaller model (say, ~10B parameters) that costs less per inference. UC Berkeley trained Koala using data from ShareGPT, and Stanford trained Alpaca by fine-tuning Meta’s LLaMA on data generated with assistance from OpenAI’s text-davinci-003.Such recipes raise important business questions. You may have spent a lot of effort to collect a large labeled training set, yet a competitor can use your model’s output to gain a leg up. This possibility argues that, contrary to conventional tech-business wisdom, data doesn’t always make your business more defensible. Specifically, if a market leader spent significant resources to get its performance up to a certain level, and if the market leader’s product generates data that makes it cheaper for competitors to catch up, then the market leader’s initial effort spent gathering data is a weak defense against competitors.In addition, the legal and ethical questions around this practice need clearer answers. OpenAI’s terms of use forbid anyone to “use output from the Services to develop models that compete with OpenAI.” To my mind, this raises legal questions such as:If Google or another company has not agreed to OpenAI’s terms of use, and it scrapes text from ShareGPT that someone else shared, is it bound by OpenAI’s terms?Are terms that restrict competitor’s access to your services enforceable in light of antitrust and fair-use laws?(To state the obvious, I am not a lawyer. Don’t construe anything I say as legal advice!)In the era of generative AI, we’ll see many creative use cases for intentionally using one model to generate data to train another. This is an exciting technical trend, even as we keep in mind the need to move forward in ways that are legal and fair.Keep fine-tuning!AndrewP.S. On Friday, April 7, Yann LeCun and I will hold a live online discussion about a proposed six-month pause in cutting-edge AI research. The proposal raises questions about AI’s future and, if implemented, would have a huge impact on developers and businesses. Please join us.",
        "summary": "* Controversy exists regarding Google Bard potentially using OpenAI ChatGPT output from ShareGPT for training, raising questions about violating OpenAI's terms of use.\n* Training models on generated data is an evolving technique, exemplified by fine-tuning smaller models (~10B parameters) with output from larger models (175B parameters) to reduce inference costs (e.g., Koala, Alpaca).\n* This practice challenges the idea that data provides business defensibility, as competitors can leverage a leader's model output for cheaper development.\n* Legal and ethical questions surround this practice, including the enforceability of terms of use (like OpenAI's) regarding using output for competing models, especially when data is sourced indirectly (e.g., ShareGPT) and considering antitrust/fair-use laws.",
        "publish_date": "Apr 05, 2023",
        "keywords": [
            "Google Bard",
            "OpenAI ChatGPT",
            "ShareGPT",
            "Fine-tuning",
            "Alpaca"
        ],
        "url": "https://www.deeplearning.ai/the-batch/when-one-machine-learning-model-learns-from-another/"
    },
    {
        "content": "Dear friends,Generative AI is taking off, and along with it excitement and hype about the technology’s potential. I encourage you to think of it as a general-purpose technology (GPT, not to be confused with the other GPT: generative pretrained transformer). Like deep learning — and electricity — generative AI is useful not just for a single application, but for a multitude of applications that span many corners of the economy. And, like the rise of deep learning that started 10 to 15 years ago, there’s important work to be done in coming years to identify use cases and build specific applications.Generative AI (Gen AI) offers huge opportunities for AI engineers to build applications that make the world a better place. Will it be used to deliver educational coaching, help people with their writing and artwork, automate customer support, teach people how to cook, generate special effects in movies, or dispense medical advice? Yes, all of the above and many more applications besides! When I asked people on social media what they use ChatGPT for, the diversity and creativity of responses showed just a sampling of current Gen AI use cases.With Gen AI, things like writing and graphics that once were in limited supply will become abundant. I spoke on this theme last week at Abundance 360, a conference organized by XPrize founder Peter Diamandis. (Stability AI’s Emad Mostaque and Scale AI’s Alexandr Wang spoke in the same session.) It was a wonderful conference with sessions that covered not only AI but also topics like food, robotics, and longevity (how can we live longer and stay healthy until age 120 and even beyond?).I also spoke about AI Fund, the venture studio I lead, where we’re building startups that use Gen AI along with other forms of AI. The AI Fund team understands this general-purpose technology — but not global shipping, real estate, security, mental health, and many other industries that AI can be applied to. Thus we’ve found it critical to partner with subject-matter experts who understand the use cases in these areas. If you have an idea for applying AI, working with a subject matter expert — if you aren’t already one yourself — can make a huge difference in your success.Moreover, I don’t think any single company can simultaneously tackle such a wide range of applications that span diverse industries. The world needs many startups to build useful applications across all these sectors.It should go without saying that, in applying Gen AI, it’s crucial to move forward with a keen sense of responsibility and ethics. AI Fund has killed financially sound projects on ethical grounds. I hope you will do the same.Keep learning!AndrewP.S. I love the abbreviation Gen AI. Gen X, Gen Y, and Gen Z refer to specific groups. This abbreviation suggests that all of us who are alive today are part of Generation AI!",
        "summary": "*   Generative AI (Gen AI) is a general-purpose technology applicable across a multitude of applications spanning many economic sectors.\n*   Significant work is required in coming years to identify use cases and build specific applications for Gen AI.\n*   Gen AI enables a wide range of applications, including educational coaching, writing/artwork help, customer support automation, movie special effects, and medical advice.\n*   Applying AI effectively requires partnering with subject-matter experts to understand industry-specific use cases.\n*   Applying Gen AI necessitates moving forward with a keen sense of responsibility and ethics.",
        "publish_date": "Mar 29, 2023",
        "keywords": [
            "Generative AI",
            "educational coaching",
            "customer support automation",
            "movie special effects",
            "medical advice"
        ],
        "url": "https://www.deeplearning.ai/the-batch/generative-ai-will-drive-tremendous-value-and-growth/"
    },
    {
        "content": "Dear friends,Here’s a quiz for you. Which company said this?“It’s always been a challenge to create computers that can actually communicate with and operate at anything like the level of a human mind. . . . What we’re doing is creating here a system that will be able to be applied to all sorts of applications in the world and essentially cut the time to find answers to very difficult problems.”How about this?“These creative moments give us confidence that AI can be used as a positive multiplier for human ingenuity.”These are not recent statements from generative AI companies working on large language models (LLMs) or image generation models! The first is from a 2011 IBM video that promotes the Watson system’s upcoming participation in the TV game show Jeopardy!. The second comes from Google DeepMind webpage about AlphaGo, which was released in 2015.IBM’s and DeepMind’s work moved AI forward. But it also inspired some people’s imaginations to get ahead of them. Some supposed that the technologies behind Watson and AlphaGo represented stronger AI capabilities than they did. Similarly, recent progress on LLMs and image generation models has reignited speculation about artificial general intelligence (AGI).Generative AI is very exciting! Nonetheless, today’s models are far from AGI. Here’s a reasonable definition of from Wikipedia:“Artificial general intelligence (AGI) is the ability of an intelligent agent to understand or learn any intellectual task that human beings or other animals can.”The latest LLMs exhibit some superhuman abilities, just as a calculator exhibits superhuman abilities in arithmetic. At the same time, there are many things that humans can learn that AI agents today are far from being able to learn.If you want to chart a course toward AGI, I think the baby steps we’re making are very exciting. Even though LLMs are famous for shallow reasoning and making things up, researchers have improved their reasoning ability by prompting them through a chain of thought (draw one conclusion, use it to draw a more sophisticated conclusion, and so on).To be clear, though, in the past year, I think we’ve made one year of wildly exciting progress in what might be a 50- or 100-year journey. Benchmarking against humans and animals doesn’t seem to be the most useful question to focus on at the moment, given that AI is simultaneously far from reaching this goal and also surpasses it in valuable ways. I’d rather focus on the exciting task of putting these technologies to work to solve important applications, while also addressing realistic risks of harm.While AGI may be part of an indeterminate future, we have amazing capabilities today, and we can do many useful things with them. It will take great effort on all of our parts to to find ways to harness them to advance humanity. Let’s get to work on that.Keep learning!Andrew",
        "summary": "*   AI systems like IBM Watson (2011) and Google DeepMind AlphaGo (2015), along with recent generative AI models (LLMs, image generation), have driven progress and speculation about artificial general intelligence (AGI).\n*   Artificial general intelligence (AGI) is defined as the ability to understand or learn any intellectual task humans or other animals can; today’s models are far from AGI despite exhibiting some superhuman abilities.\n*   Researchers have improved LLM reasoning ability by prompting them through a chain of thought.\n*   Recent progress is characterized as one year of exciting progress in what might be a 50- or 100-year journey toward AGI.\n*   The focus should be on applying current AI capabilities to solve important applications and addressing realistic risks.",
        "publish_date": "Mar 22, 2023",
        "keywords": [
            "IBM Watson",
            "AlphaGo",
            "chain of thought",
            "AGI",
            "LLMs"
        ],
        "url": "https://www.deeplearning.ai/the-batch/artificial-general-intelligence-progress-report/"
    },
    {
        "content": "Dear friends,Landing AI, a sister company of DeepLearning.AI, just released its computer vision platform, LandingLens, for everyone to start using for free. You can try it here.LandingLens makes creating computer vision projects easy and fast. If you have 10 minutes, I encourage you to check it out by creating your own project. I also created a three-minute demo video, which you can see here.Building and deploying a machine learning system is often complicated and time-consuming. You have to collect data, implement a model or find an appropriate open-source model, build a data pipeline to get the data to the right place, develop or find a tool to label the data, train the model, tune hyperparameters, fix data issues, and eventually set up a deployment server and find a way to get the trained model to run on it.This process used to take me months. With LandingLens, you can go from starting a project to deploying a model in minutes.My team at Landing AI is obsessed with making computer vision easy. The key to making this possible is our data-centric AI approach. Our back end automatically trains a highly tuned model as long as you provide good data. After initial training, you can carry out error analysis and improve the data (or use advanced options to tune hyperparameters if you want) to further improve your model’s performance.LandingLens has been used successfully in manufacturing, life sciences, satellite imaging, medical imaging, agriculture, entertainment, and many other industries.Today, companies can visualize and analyze their structured data to derive value from it using tools like pandas, seaborn, matplotlib, and tableau. But many also have collections of images sitting in storage that have yet to be analyzed. If you think this might be true of your organization, please check out LandingLens. I believe you'll find it easy to start experimenting and getting value from your images.You can start using LandingLens for free here.If you build or discover something cool and are willing to share what you've found, please let us know at Landing AI's community website. I look forward to seeing what you build.Keep building!AndrewP.S. Now that the mechanics of building a computer vision system are easy, I’ve been thinking a lot about new frameworks to approach machine learning problems that are less academic and more practical. For example, I see test sets as unnecessary for many applications. I will share more about this in the future.",
        "summary": "*   Landing AI released its computer vision platform, LandingLens, for free use.\n*   LandingLens simplifies building and deploying computer vision systems, aiming to reduce the process time from months to minutes.\n*   The platform uses a data-centric AI approach where the back end automatically trains a highly tuned model, allowing performance improvement via error analysis and data improvement.\n*   LandingLens has been successfully applied in manufacturing, life sciences, satellite imaging, medical imaging, agriculture, entertainment, and other industries.\n*   Andrew Ng is exploring practical ML frameworks, potentially viewing test sets as unnecessary for many applications.",
        "publish_date": "Mar 01, 2023",
        "keywords": [
            "LandingLens",
            "computer vision",
            "data-centric AI",
            "error analysis",
            "test sets"
        ],
        "url": "https://www.deeplearning.ai/the-batch/landinglens-enables-anyone-to-build-in-minutes-models-that-used-to-take-months/"
    },
    {
        "content": "Dear friends,ChatGPT has raised fears that students will harm their learning by using it to complete assignments. Voice cloning, another generative AI technology, has fooled people into giving large sums of money to scammers, as you can read in this issue of The Batch. Why don’t we watermark AI-generated content to make it easy to distinguish from human-generated content? Wouldn’t that make ChatGPT-enabled cheating harder and voice cloning less of a threat? While watermarking can help, unfortunately financial incentives in the competitive market for generative AI make their adoption challenging.Effective watermarking technology exists. OpenAI has talked about developing it to detect text produced by ChatGPT, and this tweet storm describes one approach. Similarly, a watermark can be applied invisibly to generated images or audio. While it may be possible to circumvent these watermarks (for instance, by erasing them), they certainly would pose a barrier to AI-generated content that masquerades as human-generated.Unfortunately, I’m not optimistic that this solution will gain widespread adoption. Numerous providers are racing to provide text-, image-, and voice-generation services. If one of them watermarks its output, it will risk imposing on itself a competitive disadvantage (even if it may make society as a whole better off).For example, assuming that search engines downranked AI-generated text, SEO marketers who wanted to produce high-ranking content would have a clear incentive to make sure their text wasn’t easily identifiable as generated. Similarly, a student who made unauthorized use of a text generator to do their homework would like it to be difficult for the teacher to find out.Even if a particular country were to mandate watermarking of AI-generated content, the global nature of competition in this market likely would incentivize providers in other countries to ignore that law and keep generating human-like output without watermarking.Some companies likely will whitewash these issues by talking about developing watermarking technology without actually implementing it. An alternative to watermarking is to use machine learning to classify text as either AI- or human-generated. However, systems like GPTzero that attempt to do so have a high error rate and don’t provide a robust solution.If one company were to establish a monopoly or near-monopoly, then it would have the market power to implement watermarking without risking losing significant market share. Given the many downsides of monopolies, this is absolutely not the outcome we should hope for.So what’s next? I think we’re entering an era when, in many circumstances, it will be practically impossible to tell if a piece of content is human- or AI-generated. We will need to figure out how to re-architect both human systems such as schools and computer systems such as biometric security to operate in this new — and sometimes exciting — reality. Years ago when Photoshop was new, we learned what images to trust and not trust. With generative AI, we have another set of discoveries ahead of us.Keep learning!",
        "summary": "*   Generative AI technologies, including ChatGPT and voice cloning, raise concerns about distinguishing AI-generated from human content.\n*   Effective watermarking technology exists for AI-generated text, images, and audio, which can pose a barrier to content masquerading as human-generated, although it may be possible to circumvent.\n*   Widespread adoption of AI watermarking is challenged by financial incentives and competitive market dynamics among generative AI providers.\n*   Alternative detection methods using ML classifiers (e.g., GPTzero) have a high error rate and are not robust solutions.\n*   It is predicted to become practically impossible to distinguish human vs. AI content, requiring re-architecting of systems.",
        "publish_date": "Mar 08, 2023",
        "keywords": [
            "ChatGPT",
            "voice cloning",
            "AI watermarking",
            "GPTzero",
            "ML classifiers"
        ],
        "url": "https://www.deeplearning.ai/the-batch/watermarking-is-a-no-go/"
    },
    {
        "content": "Dear friends,Last week, Silicon Valley Bank (SVB), Signature Bank, and Silvergate Bank suddenly collapsed. If it passed uneventfully from your point of view, good for you! Many companies worked nonstop through the weekend scrambling to preserve funds so they could pay their employees.Numerous tech startups and small businesses bank at SVB, and many are among the business pioneers who are bringing AI to market. For example, when AI Fund, which I lead, works with entrepreneurs to build new companies, we used to help them set up accounts with SVB.Last Wednesday, SVB announced a $1.8 billion loss. The next morning, rumors began circulating via text, email, and Slack about a bank run in which customers were withdrawing funds en masse. When this happens, depositors can lose money they’ve saved beyond the $250,000 limit the FDIC (a U.S. government agency) guarantees. Without access to their money, companies can’t pay employees who are counting on a paycheck to cover expenses. A permanent loss of funds would lead to numerous layoffs and company shutdowns.While navigating the collapse of SVB, I was fortunate to be able to call on friends and allies. Several CEOs of AI Fund portfolio companies share a Slack channel and have pre-existing relationships, so none of us felt alone. We were able to share information, make introductions to new banks, and lean in to help each other. Over the weekend, the AI Fund team went to many CEOs and pledged funds from AI Fund’s management company to make sure they could cover their payrolls.I also saw the best of the AI and tech worlds last week beyond the AI Fund ecosystem. As new information developed, executives at many companies shared it across their networks, and we worked our way through the crisis cooperatively. I’m grateful that we were able to face the storm together.On Sunday, the U.S. government wisely announced that it would protect all depositors’ assets. This calmed the crisis and helped to head off a domino effect of further bank failures.Candidly, I was stressed from Thursday through the weekend about the fate of numerous people and companies. And I know that this is not the end of the challenges. Here’s what life has been like for an AI innovator in recent years (h/t @ChrisJBakke):2020: Let’s see you handle a pandemic!2021: Deep learning has diminishing returns.2022: Generative AI is here! Time for massive FOMO.2023: Your bank shut down.I expect life to be equally dynamic in the future as well — hopefully with more ups than downs. But the fact that many people in AI have a network of trusted friends will enable us to react quickly and work together to benefit everyone.Keep learning!Andrew",
        "summary": "* Numerous tech startups and small businesses bringing AI to market banked at SVB and were affected by its collapse.\n* The AI Fund team pledged funds from its management company to portfolio companies to cover payrolls during the crisis.\n* Challenges for an AI innovator included \"Deep learning has diminishing returns\" (2021), \"Generative AI is here! Time for massive FOMO\" (2022), and \"Your bank shut down\" (2023).",
        "publish_date": "Mar 15, 2023",
        "keywords": [
            "Deep learning",
            "Generative AI",
            "AI Fund",
            "AI innovator",
            "bringing AI to market"
        ],
        "url": "https://www.deeplearning.ai/the-batch/surviving-silicon-valley-bank/"
    },
    {
        "content": "Dear friends,Last week, we released a new course, ChatGPT Prompt Engineering for Developers, created in collaboration with OpenAI. This short, 1.5-hour course is taught by OpenAI’s Isa Fulford and me. This has been the fastest-growing course I’ve ever taught, with over 300,000 sign-ups in under a week. Please sign up to take it for free!Many people have shared tips on how to use ChatGPT’s web interface, often for one-off tasks. In contrast, there has been little material on best practices for developers who want to build AI applications using API access to these hugely powerful large language models (LLMs).LLMs have emerged as a new AI application development platform that makes it easier to build applications in robotic process automation, text processing, assistance for writing or other creative work, coaching, custom chatbots, and many other areas. This short course will help you learn what you can do with these tools and how to do it.Say, you want to build a classifier to extract names of people from text. In the traditional machine learning approach, you would have to collect and label data, train a model, and figure out how to deploy it to get inferences. This can take weeks. But using an LLM API like OpenAI’s, you can write a prompt to extract names in minutes.In this short course, Isa and I share best practices for prompting. We cover common use cases such as:Summarizing, such as taking a long text and distilling itInferring, such as classifying texts or extracting keywordsTransforming, such as translation or grammar/spelling correctionExpanding, such as using a short prompt to generate a custom emailWe also cover how to build a custom chatbot and show how to construct API calls to build a fun pizza order-taking bot.In this course, we describe best practices for developing prompts. Then you can try them out yourself via the built-in Jupyter notebook (the middle portion of the image above). If you want to run the provided code, you can hit Shift-Enter all the way through the notebook to see its output. Or you can edit the code to gain hands-on practice with variations on the prompts.Many applications that were very hard to build can now be built quickly and easily by prompting an LLM. So I hope you’ll check out the course and gain the important skill of using prompts in development. Hopefully you’ll also come away with new ideas for fun things that you want to build yourself!Keep learning!Andrew",
        "summary": "*   A new course, \"ChatGPT Prompt Engineering for Developers,\" focuses on best practices for developers using API access to Large Language Models (LLMs).\n*   LLMs are presented as a new AI application development platform enabling applications in areas like robotic process automation, text processing, writing assistance, coaching, and custom chatbots.\n*   Using an LLM API via prompting can achieve tasks like classification in minutes, contrasting with traditional machine learning approaches taking weeks for data collection, training, and deployment.\n*   The course covers prompting best practices for common use cases: Summarizing, Inferring (classifying, extracting keywords), Transforming (translation, grammar), and Expanding text.\n*   Building custom chatbots using LLM API calls is demonstrated in the course.",
        "publish_date": "May 03, 2023",
        "keywords": [
            "Prompt Engineering",
            "ChatGPT",
            "LLM API",
            "Inferring",
            "Summarizing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/new-course-chatgpt-prompt-engineering-for-developers/"
    },
    {
        "content": "Dear friends,I’d like to share a part of the origin story of large language models that isn’t widely known. A lot of early work in natural language processing (NLP) was funded by U.S. military intelligence agencies that needed machine translation and speech recognition capabilities. Then, as now, such agencies analyzed large volumes of text and recorded speech in various languages. They poured money into research in machine translation and speech recognition over decades, which motivated researchers to give these applications disproportionate attention relative to other uses of NLP.This explains why many important technical breakthroughs in NLP stem from studying translation — more than you might imagine based on the modest role that translation plays in current applications. For instance, the celebrated transformer paper, “Attention is All You Need” by the Google Brain team, introduced a technique for mapping a sentence in one language to a translation in another. This laid the foundation for large language models (LLMs) like ChatGPT, which map a prompt to a generated response.Or consider the BLEU score, which is occasionally still used to evaluate LLMs by comparing their outputs to ground-truth examples. It was developed in 2002 to measure how well a machine-generated translation compares to a ground truth, human-created translation.A key component of LLMs is tokenization, the process of breaking raw input text into sub-word components that become the tokens to be processed. For example, the first part of the previous sentence may be divided into tokens like this:/A /key /component /of /LL/Ms/ is/ token/izationThe most widely used tokenization algorithm for text today is Byte Pair Encoding (BPE), which gained popularity in NLP after a 2015 paper by Sennrich et al. BPE starts with individual characters as tokens and repeatedly merges tokens that occur together frequently. Eventually, entire words as well as common sub-words become tokens. How did this technique come about? The authors wanted to build a model that could translate words that weren’t represented in the training data. They found that splitting words into sub-words created an input representation that enabled the model, if it had seen “token” and “ization,” to guess the meaning of a word it might not have seen before, such as “tokenization.”I don’t intend this description of NLP history as advocacy for military-funded research. (I have accepted military funding, too. Some of my early work in deep learning at Stanford University was funded by DARPA, a U.S. defense research agency. This led directly to my starting Google Brain.) War is a horribly ugly business, and I would like there to be much less of it. Still, I find it striking that basic research in one area can lead to broadly beneficial developments in others. In similar ways, research into space travel led to LED lights and solar panels, experiments in particle physics led to magnetic resonance imaging, and studies of bacteria’s defenses against viruses led to the CRISPR gene-editing technology.So it’s especially exciting to see so much basic research going on in so many different areas of AI. Who knows, a few years hence, what today’s experiments will yield?Keep learning!AndrewP.S. Built in collaboration with Microsoft, our short course “How Business Thinkers Can Start Building AI Plugins With Semantic Kernel” is now available! This is taught by John Maeda, VP of Design and AI (who also co-invented the Scratch programming language!). You’ll join John in building his “AI Kitchen” and learn to cook up a full AI meal from, well, scratch – including all the steps to build full business-thinking AI pipelines. You’ll conclude by creating an AI planner that can automatically select plugins it needs to produce multi-step plans with sophisticated logic. Sign up to learn here!",
        "summary": "* Early NLP research, particularly in machine translation and speech recognition, was significantly funded by U.S. military intelligence agencies, influencing the field's direction.\n* The transformer architecture, foundational for LLMs like ChatGPT, originated from research into machine translation techniques.\n* The BLEU score, developed in 2002 for evaluating machine translation quality, is occasionally still used for LLMs.\n* Byte Pair Encoding (BPE), popularized in 2015, is the most widely used text tokenization algorithm for LLMs, developed to handle out-of-vocabulary words via sub-word splitting.",
        "publish_date": "Aug 30, 2023",
        "keywords": [
            "transformer architecture",
            "ChatGPT",
            "BLEU score",
            "Byte Pair Encoding (BPE)",
            "machine translation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-unlikely-roots-of-large-language-models/"
    },
    {
        "content": "Dear friends,This week, I’m speaking at the World Economic Forum (WEF) and Asia-Pacific Economic Cooperation (APEC) meetings in San Francisco, where leaders in business and government have convened to discuss AI and other topics. My message at both events is simple: Governments should not outlaw open source software or pass regulations that stifle open source development. Regulating AI is a hot topic right now in the United States, European Union, and elsewhere. Just this week, the EU’s AI Act was derailed when France and Germany objected — with good reason, in my view — to provisions that would burden companies that build foundation models.As Yann LeCun and I have said, it’s important to distinguish between regulating technology (such a foundation model trained by a team of engineers) and applications (such as a website that uses a foundation model to offer a chat service, or a medical device that uses a foundation model to interacts with patients). We need good regulations to govern AI applications, but ill-advised proposals to regulate the technology would slow down AI development unnecessarily. While the EU’s AI Act thoughtfully addresses a number of AI applications — such as ones that sort job applications or predict crime — and assesses their risks and mandates mitigations, it imposes onerous reporting requirements on companies that develop foundation models, including organizations that aim to release open-source code. I wrote in an earlier letter that some companies that would rather not compete with open-source, as well as some nonprofits and individuals, are exaggerating AI risks. This creates cover for legislators to pass regulations in the name of safety that will hamper open source. At WEF and APEC, I’ve had conversations about additional forces at play. Let me describe what I’m seeing. In the U.S., a faction is worried about the nation’s perceived adversaries using open source technology for military or economic advantage. This faction is willing to slow down availability of open source to deny adversaries’ access. I, too, would hate to see open source used to wage unjust wars. But the price of slowing down AI progress is too high. AI is a general-purpose technology, and its beneficial uses — similar to other general purpose technologies like electricity — far outstrip the nefarious ones. Slowing it down would be a loss for humanity. When I speak with senior U.S. government officials, I sense that few think the possibility that AI will lead to human extinction is a realistic risk. This topic tends to lead to eye-rolls. But they genuinely worry about AI risks such as disinformation. In comparison, the EU is more concerned — unnecessarily, in my view — about the risk of extinction, while also worried about other, more concrete harms. Many nations and corporations are coming to realize they will be left behind if regulation stifles open source. After all, the U.S. has a significant concentration of generative AI talent and technology. If we raise the barriers to open source and slow down the dissemination of AI software, it will only become harder for other nations to catch up. Thus, while some might argue that the U.S. should slow down dissemination of AI (an argument that I disagree with), that certainly would not be in the interest of most nations. I believe deeply that the world is better off with more intelligence, whether human intelligence or artificial intelligence. Yes, intelligence can be used for nefarious purposes. But as society has developed over centuries and we have become smarter, humanity has become much better off.A year ago, I wouldn’t have thought that so many of us would have to spend so much time trying to convince governments not to outlaw, or make impractical, open-sourcing of advanced AI technology. But I hope we can all keep on pushing forward on this mission, and keep on pushing to make sure this wonderful technology is accessible to all.Keep learning!AndrewP.S. Many teams that build applications based on large language models (LLMs) worry about their safety and security, and such worries are a significant barrier to shipping products. For example, might the application leak sensitive data, or be tricked into generating inappropriate outputs? Our new short course shows how you can mitigate hallucinations, data leakage, and jailbreaks. Learn more in “Quality and Safety for LLM Applications,” taught by Bernease Herman and created in collaboration with WhyLabs (disclosure: an AI Fund portfolio company). Available now!",
        "summary": "*   Governments should avoid outlawing or stifling open source AI development, especially for foundation models.\n*   AI regulation should distinguish between technology (foundation models) and applications, focusing regulation on the latter.\n*   The EU's AI Act addresses application risks but imposes onerous requirements on foundation model developers, including open source organizations.\n*   Efforts to hamper open source AI are driven by concerns about adversaries using the technology and exaggerated risks used to stifle competition.",
        "publish_date": "Nov 15, 2023",
        "keywords": [
            "foundation models",
            "open source",
            "EU's AI Act",
            "AI regulation",
            "application risks"
        ],
        "url": "https://www.deeplearning.ai/the-batch/keep-open-source-free/"
    },
    {
        "content": "Dear friends,Machine learning development is an empirical process. It’s hard to know in advance the result of a hyperparameter choice, dataset, or prompt to a large language model (LLM). You just have to try it, get a result, and decide on the next step. Still, understanding how the underlying technology works is very helpful for picking a promising direction. For example, when prompting an LLM, which of the following is more effective?Prompt 1: [Problem/question description] State the answer and then explain your reasoning.Prompt 2: [Problem/question description] Explain your reasoning and then state the answer.These two prompts are nearly identical, and the former matches the wording of many university exams. But the second prompt is much more likely to get an LLM to give you a good answer. Here’s why: An LLM generates output by repeatedly guessing the most likely next word (or token). So if you ask it to start by stating the answer, as in the first prompt, it will take a stab at guessing the answer and then try to justify what might be an incorrect guess. In contrast, prompt 2 directs it to think things through before it reaches a conclusion. This principle also explains the effectiveness of widely discussed prompts such as “Let’s think step by step.”The image above illustrates this difference using a question with one right answer. But similar considerations apply when asking an LLM to make judgment calls when there is no single right answer; for example, how to phrase an email, what to say to someone who is upset, or the proper department to route a customer email to.That’s why it’s helpful to understand, in depth, how an algorithm works. And that means more than memorizing specific words to include in prompts or studying API calls. These algorithms are complex, and it’s hard to know all the details. Fortunately, there’s no need to. After all, you don’t need to be an expert in GPU compute allocation algorithms to use LLMs. But digging one or two layers deeper than the API documentation to understand how key pieces of the technology work will shape your insights. For example, in the past week, knowing how long-context transformer networks process input prompts and how tokenizers turn an input into tokens shaped how I used them.A deep understanding of technology is especially helpful when the technology is still maturing. Most of us can get a mature technology like GPS to perform well without knowing much about how it works. But LLMs are still an immature technology, and thus your prompts can have non-intuitive effects. Developers who understand the technology in depth are likely to build more effective applications, and build them faster and more easily, than those who don't. Technical depth also helps you to decide when you can’t tell what’s likely to work in advance, and when the best approach is to try a handful of promising prompts, get a result, and keep iterating.Keep learning!AndrewP.S. Our short course on fine-tuning LLMs is now available! As I wrote last week, many developers are not only prompting LLMs but also fine-tuning them — that is, taking a pretrained model and training it further on their own data. Fine-tuning can deliver superior results, and it can be done relatively inexpensively. In this course, Sharon Zhou, CEO and co-founder of Lamini (disclosure: I’m a minor shareholder) shows you how to recognize when fine-tuning can be helpful and how to do it with an open-source LLM. Learn to fine-tune your own models here.",
        "summary": "*   Machine learning development, including LLM prompting, is empirical; results of choices (hyperparameters, datasets, prompts) are hard to predict and require iteration.\n*   Understanding how LLMs generate output (guessing the most likely next token) is crucial for effective prompting, explaining why prompt order (e.g., reasoning before answer) and techniques like \"Let's think step by step\" are effective.\n*   Technical depth in underlying technologies (like long-context transformer networks and tokenizers) is helpful for effective LLM application development, especially as LLMs are an immature technology.\n*   Fine-tuning LLMs (training a pretrained model on custom data) can deliver superior results and be done relatively inexpensively.",
        "publish_date": "Aug 23, 2023",
        "keywords": [
            "Fine-tuning LLMs",
            "long-context transformer networks",
            "tokenizers",
            "Chain-of-Thought",
            "hyperparameters"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-hidden-value-of-deep-technical-knowledge/"
    },
    {
        "content": "Dear friends,An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach. One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task — the input and the desired output — sometimes yields better results. Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you’re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn’t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn’t deliver the performance you want, then try fine-tuning — but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? 😜)Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that’s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM’s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning — in which GPT-4 surpasses current open models — it can be difficult to fine-tune a smaller model to deliver superior results.Beyond choosing a development approach, it’s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I’ll talk about how to make this choice in a future letter.Keep learning!AndrewP.S. We just released “Large Language Models with Semantic Search,”  a short course built in collaboration with Cohere and taught by Jay Alammar and Luis Serrano. Search is a key part of many applications. Say, you need to retrieve documents or products in response to a user query. How can LLMs help? You’ll learn about (i) embeddings to retrieve a collection of documents loosely related to a query and (ii) LLM-assisted re-ranking to rank them precisely according to a query. You’ll also go through code that shows how to build a search system for retrieving relevant Wikipedia articles. Please check it out!",
        "summary": "*   Building LLM applications can be done using methods of increasing cost/complexity: Prompting, One-shot or few-shot prompting, Fine-tuning, and Pretraining.\n*   Prompting is recommended to start as it allows quick prototyping in minutes or hours without a training set.\n*   Fine-tuning involves training a pretrained LLM on a small custom dataset, requiring hundreds or thousands more examples than prompting.\n*   Techniques like RAG (retrieval augmented generation) can improve prompts, and the choice between fine-tuning a smaller model vs. prompting a larger one depends on the application (e.g., style vs. complex reasoning).",
        "publish_date": "Aug 16, 2023",
        "keywords": [
            "Prompting",
            "One-shot or few-shot prompting",
            "Fine-tuning",
            "Pretraining",
            "RAG (retrieval augmented generation)"
        ],
        "url": "https://www.deeplearning.ai/the-batch/tips-for-taking-advantage-of-open-large-language-models/"
    },
    {
        "content": "Dear friends,I’ve always believed in democratizing access to the latest advances in artificial intelligence. As a step in this direction, we just launched “Generative AI for Everyone” on Coursera. The course assumes no programming or AI background, and I hope it will be useful to students, teachers, artists, scientists, engineers, leaders in business and government, and anyone else who simply wants to know how to apply generative AI in their work or personal life. Please check it out and encourage your friends to take a look, especially those with a nontechnical background.Just as web search and word processing have become essential skills in the workplace, using generative AI soon will become a baseline skill set expected by every employer. This highly accessible, general-purpose technology is suitable for numerous tasks. It’s already used in copyediting, customer service, brainstorming, summarizing documents, and more. And many more uses are yet to be identified. The course covers:How generative AI (particularly large language models, or LLMs) works, and what it can and cannot doA nontechnical description of advanced techniques, including RAG (retrieval augmented generation, which gives an LLM access to additional, proprietary information) and fine-tuning, and when to use these techniquesBest practices for the use of LLMs, either via a web interface (such as ChatGPT or BARD) or by incorporating them into a larger application (such as software that calls an LLM API)How to identify opportunities for AI augmentation or automation by breaking down jobs into tasks and evaluating their potential for automation — I described this in a previous letter, but the course goes into greater detail and explains how this can bring cost savings and revenue growthResponsible AI and generative AI’s impact on jobs and society If you’re an engineer: I designed this course to be accessible to nontechnical professionals partly to help technical people work with them more easily. With earlier waves of technology, I found that the gap in understanding between technical and nontechnical people got in the way of putting the technology to use. So if you already have a good understanding of generative AI, please encourage your nontechnical colleagues to take this course. They will learn a lot, and I hope this will help you collaborate more productively! You can check out the course here.Keep learning!Andrew",
        "summary": "*   The \"Generative AI for Everyone\" course on Coursera aims to democratize access to AI, designed for individuals with no programming or AI background.\n*   Using generative AI is presented as a future baseline workplace skill, applicable across various tasks like copyediting, customer service, and summarizing documents.\n*   Course content includes how generative AI (LLMs) works, nontechnical descriptions of RAG and fine-tuning, and best practices for LLM use via web interface or API.\n*   It also covers identifying AI augmentation/automation opportunities by analyzing tasks for potential cost savings/revenue growth, and responsible AI/societal impact.",
        "publish_date": "Nov 01, 2023",
        "keywords": [
            "Generative AI",
            "LLMs",
            "RAG",
            "fine-tuning",
            "AI augmentation/automation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/everyone-can-benefit-from-generative-ai-skills/"
    },
    {
        "content": "Dear friends,Do large language models understand the world? As a scientist and engineer, I’ve avoided asking whether an AI system “understands” anything. There’s no widely agreed-upon, scientific test for whether a system really understands — as opposed to appearing to understand — just as no such tests exist for consciousness or sentience, as I discussed in an earlier letter. This makes the question of understanding a matter of philosophy rather than science. But with this caveat, I believe that LLMs build sufficiently complex models of the world that I feel comfortable saying that, to some extent, they do understand the world.To me, the work on Othello-GPT is a compelling demonstration that LLMs build world models; that is, they figure out what the world really is like rather than blindly parrot words. Kenneth Li and colleagues trained a variant of the GPT language model on sequences of moves from Othello, a board game in which two players take turns placing game pieces on an 8x8 grid. For example, one sequence of moves might be d3 c5 f6 f5 e6 e3…, where each pair of characters (such as d3) corresponds to placing a game piece at a board location.During training, the network saw only sequences of moves. It wasn’t explicitly told that these were moves on a square, 8x8 board or the rules of the game. After training on a large dataset of such moves, it did a decent job of predicting what the next move might be.The key question is: Did the network make these predictions by building a world model? That is, did it discover that there was an 8x8 board and a specific set of rules for placing pieces on it, that underpinned these moves? The authors demonstrate convincingly that the answer is yes. Specifically, given a sequence of moves, the network’s hidden-unit activations appeared to capture a representation of the current board position as well as available legal moves. This shows that, rather than being a “stochastic parrot” that tried only to mimic the statistics of its training data, the network did indeed build a world model.While this study used Othello, I have little doubt that LLMs trained on human text also build world models. A lot of “emergent” behaviors of LLMs — for example, the fact that a model fine-tuned to follow English instructions can follow instructions written in other languages — seem very hard to explain unless we view them as understanding the world.AI has wrestled with the notion of understanding for a long time. Philosopher John Searle published the Chinese Room Argument in 1980. He proposed a thought experiment: Imagine an English speaker alone in a room with a rulebook for manipulating symbols, who is able to translate Chinese written on paper slipped under the door into English, even though the person understands no Chinese. Searle argued that a computer is like this person. It appears to understand Chinese, but it really doesn’t.A common counterargument known as the Systems Reply is that, even if no single part of the Chinese Room scenario understands Chinese, the complete system of the person, rulebook, paper, and so on does. Similarly, no single neuron in my brain understands machine learning, but the system of all the neurons in my brain hopefully do. In my recent conversation with Geoff Hinton, which you can watch here, the notion that LLMs understand the world was a point we both agreed on.Although philosophy is important, I seldom write about it because such debates can rage on endlessly and I would rather spend my time coding. I’m not sure what the current generation of philosophers thinks about LLMs understanding the world, but I am certain that we live in an age of wonders!Okay, back to coding.Keep learning,Andrew",
        "summary": "* The author believes Large Language Models (LLMs) build sufficiently complex world models to some extent, allowing them to \"understand\" the world, despite the philosophical nature of the term.\n* The Othello-GPT study is presented as compelling evidence that LLMs build world models, learning game rules and board state from move sequences alone.\n* Othello-GPT's hidden-unit activations captured representations of the current board position and legal moves, indicating it built a world model rather than just mimicking training data statistics.\n* The author suggests LLMs trained on human text also build world models, citing emergent behaviors like cross-language instruction following as evidence.",
        "publish_date": "Aug 09, 2023",
        "keywords": [
            "Othello-GPT",
            "world models",
            "hidden-unit activations",
            "board state",
            "cross-language instruction following"
        ],
        "url": "https://www.deeplearning.ai/the-batch/does-ai-understand-the-world/"
    },
    {
        "content": "Dear friends,Last week, I returned home from Asia, where I spoke at Seoul National University in Korea, the National University of Singapore, and the University of Tokyo in Japan and visited many businesses. As I discussed the state of AI with students, technologists, executives, and government officials, something struck me: Around the world, everyone is wrestling with similar AI-related issues.In every country:Business leaders are asking how AI will affect their companies.Governments are wondering how it will affect the labor market, what risks it poses, and how to regulate it.Companies are trying to figure out how to use it without “giving away” their data to one of the platform vendors.Developers are experimenting with creative uses of generative AI. The two most common applications remain building customer service chatbots and answering questions based on documents. But I also heard about numerous creative projects in medical records, financial records, privacy protection, and much more.When the deep learning revolution started about a decade ago, I advised teams to (i) learn about the technology, (ii) start small and build projects quickly to hone intuition about what’s possible, and (iii) use learnings from smaller projects to scale to bigger ones. With the generative AI revolution, my advice remains the same. This time, though, the barrier to entry is lower and thus the time-to-value seems to be shorter. It takes substantial effort to collect data and train and deploy a neural network, but less effort to prompt a large language model and start getting results.For developers, this means richer opportunities than ever! Leaders are looking for helpful perspectives. If you’re able to experiment, learn, identify successful use cases (and even some failures — which is fine, too), and share your insights with colleagues, perhaps you can influence the trajectory of your business.Last Friday, I discussed how businesses can plan for generative AI with Erik Brynjolfsson, Andrew McAfee, James Milin, and Daniel Rock, who co-founded Workhelix (a portfolio company of AI Fund, which I lead). Workhelix helps its customers break down jobs into tasks to see which tasks can be augmented by AI. You can listen to the conversation here.For instance, a radiologist’s tasks include (i) capturing images, (ii) reading them, (iii) communicating with patients, and so on. Which of these tasks can take advantage of AI to make a radiologist’s job more productive and enjoyable? Can it help optimize image acquisition (perhaps by tuning the X-ray machine controls), speed up interpretation of images, or generate takeaways text for patients?Although Workhelix is applying this recipe at scale, it’s also useful for teams that are exploring opportunities in AI. Consider not jobs but their component tasks. Are any of them amenable to automation or assistance by AI? This can be a helpful framework for brainstorming interesting project ideas.The way generative AI is taking off in many places around the world means that our markets are increasingly global. Wherever in the world you live, this is a wonderful time to build your AI knowledge and increase your AI skills. Exciting opportunities lie ahead!Special thanks to Ian Park of the Korean Investment Corporation, Chong Yap Seng of the National University of Singapore, and Yuji Mano of Mitsui, who made my visits much more productive and enjoyable. I also hope to visit other countries soon. Stay tuned!Keep learning,AndrewP.S. DeepLearning.AI just launched “Evaluating and Debugging Generative AI,” created in collaboration with Weights & Biases and taught by Carey Phelps. Machine learning development is an iterative process, and we often have to try many things to build a system that works. I used to keep track of all the different models I was training in a text file or spreadsheet. Thankfully better tools are available now. This course will teach you how to use them, focusing on generative AI applications. I hope you enjoy the course!",
        "summary": "* Similar AI-related issues are being addressed globally, including business impact, government regulation/risks/labor effects, data privacy concerns with vendors, and developer experimentation with generative AI.\n* Common generative AI applications include customer service chatbots and document-based Q&A, with creative projects in areas like medical/financial records and privacy protection.\n* The generative AI revolution has a lower barrier to entry and shorter time-to-value compared to the deep learning revolution, as prompting LLMs is easier than traditional data collection/training/deployment.\n* A task-based framework, exemplified by Workhelix, helps identify AI augmentation opportunities by breaking down jobs into component tasks amenable to automation or assistance.\n* A new course, \"Evaluating and Debugging Generative AI,\" has launched, focusing on tools and iterative development for generative AI applications.",
        "publish_date": "Aug 02, 2023",
        "keywords": [
            "customer service chatbots",
            "document-based Q&A",
            "Prompting LLMs",
            "Workhelix",
            "Evaluating and Debugging Generative AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/generative-ai-equals-huge-opportunities/"
    },
    {
        "content": "Dear friends,I’m delighted that the crisis at OpenAI, which you can read about here, seems to have been resolved with an agreement in principle for Sam Altman to return as CEO after his sudden firing last week. OpenAI has many well-meaning employees, who have worked hard to innovate in AI and bring its benefits to others. Everyone at OpenAI has my congratulations for getting to a resolution so quickly! The team deserves kudos especially for focusing on customers even through the turmoil.One positive take-away is that employees have power. It can be hard to be part of a large team. But through ways large and small, people doing the work can influence events in important ways. OpenAI employees banded together to demand changes in the board, and one or two engineers at any company can raise a concern. Wherever you work, use your voice to make things better! So what’s next?I see both hopeful and worrisome impacts as OpenAI picks up the pieces:The team’s camaraderie through this episode has been inspiring. Strong alignment within the team could lead to increased effectiveness. That would be great for AI innovation, the company, and its customers and users.A few media outlets, notably The Information and Bloomberg, demonstrated a strong ability to get scoops about what was happening. Many are saying that OpenAI will face increased scrutiny in the future.Bret Taylor (who helped Twitter navigate its sale to Elon Musk) and Larry Summers (former United States Secretary of the Treasury and Harvard president) are strong additions to the board. OpenAI has a small but efficient lobbying team that has been highly influential on global AI regulation, and Summers’ background makes him a valuable addition to such efforts. I look forward to a more diverse board as its membership grows.In recent days, I heard from multiple businesses that are looking for alternatives to the OpenAI API to ensure their own continuity of operations. The quick resolution of the crisis has stemmed much of the damage, but the fact that some customers are looking at backup options will be hard to reverse. The failure of OpenAI’s unusual for-profit/nonprofit corporate structure is glaring. Investors and donors will be more hesitant to fund organizations with novel structures (which often come with passionate arguments — which fell apart in the case of OpenAI — about why they’re better). In most companies, board oversight over the CEO’s performance would be good governance, and for a fired CEO to rally employees against the board and get their job back would be a sign of awful governance. But OpenAI’s previous board nearly destroyed so much value, for no apparent reason, that I’m glad employees helped reverse the decision. The reconstituted board has its work cut out for it to put in place robust governance.ChatGPT was released on November 30, 2022. It is amazing how much has happened at OpenAI — and in the AI world — in less than one year! Brief stretches of chaos may be the price of moving fast. Nonetheless, I think moving fast (but responsibly) is better than going slowly. I hope all employees everywhere will come away from this episode feeling empowered to speak up and make things better. Let’s keep building AI, exercise wisdom and foresight, and learn what lessons we can about corporate governance. It’s probably too much to hope that there won't be additional bumps in the road ahead for AI, but I remain optimistic about all the good we can do. Keep learning!Andrew",
        "summary": "*   The resolution of the crisis at OpenAI, a major AI company, includes Sam Altman returning as CEO.\n*   The crisis prompted some businesses to seek alternatives to the OpenAI API, impacting customer reliance on its AI services.\n*   OpenAI's lobbying team is influential on global AI regulation, with new board member Larry Summers adding value to these efforts.\n*   The text notes the rapid pace of events in the AI world since ChatGPT's release on November 30, 2022, and links team alignment to potential increases in AI innovation effectiveness.",
        "publish_date": "Nov 22, 2023",
        "keywords": [
            "ChatGPT",
            "OpenAI API",
            "OpenAI",
            "AI regulation",
            "AI services"
        ],
        "url": "https://www.deeplearning.ai/the-batch/whats-next-for-openai/"
    },
    {
        "content": "Dear friends,Last week, the White House announced voluntary commitments by seven AI companies, as you can read here. Most of the points were sufficiently vague that it seems easy for the White House and the companies to declare success without doing much that they don’t already do. But the commitment to develop mechanisms to ensure that users know when content is AI-generated, such as watermarks, struck me as concrete and actionable. While most of the voluntary commitments are not measurable, this one is. It offers an opportunity, in the near future, to test whether the White House’s presently soft approach to regulation is effective.I was pleasantly surprised that watermarking was on the list. It’s beneficial to society, but it can be costly to implement (in terms of losing users).As I wrote in an earlier letter, watermarking is technically feasible, and I think society would be better off if we knew what content was and wasn’t AI-generated. However, many companies won’t want it. For example, a company that uses a large language model to create marketing content may not want the output to be watermarked, because then readers would know that it was generated by AI. Also, search engines might rank generated content lower than human-written content. Thus, the government’s push to have major generative AI companies watermark their output is a good move. It reduces the competitive pressure to avoid watermarking.All the companies that agreed to the White House’s voluntary commitments employ highly skilled engineers and are highly capable of shipping products, so they should be able to keep this promise. When we look back after three or six months, it will be interesting to see which ones:Implemented a robust watermarking systemImplemented a weak watermarking system that’s easy to circumvent by, say, paying a fee for watermark-free outputDidn’t implement a system to identify AI-generated contentTo be fair, I think it would be very difficult to enforce watermarking in open source systems, since users can easily modify the software to turn it off. But I would love to see watermarking implemented in proprietary systems. The companies involved are staffed by honorable people who want to do right by society. I hope they will take the announced commitments seriously and implement them faithfully.I would love to get your thoughts on this as well. How can we collectively hold the U.S. government and AI companies to these commitments? Please let me know on social media!Keep learning,AndrewP.S. A new short course, developed by DeepLearning.AI and Hugging Face, is available! In “Building Generative AI Applications with Gradio,” instructor Apolinário Passo shows you how to quickly create fun demos of your machine learning applications. Prompting large language models makes building applications faster than ever, but how can you demo your work, either to get feedback or let others to experience what you’ve built? This course shows you how to do it by writing only Python code.",
        "summary": "*   The White House announced voluntary commitments by seven AI companies, including developing mechanisms like watermarks to ensure users know when content is AI-generated.\n*   Watermarking AI-generated content is technically feasible but can be costly for companies (e.g., losing users, lower search rankings for LLM-generated marketing content).\n*   The watermarking commitment is viewed as a concrete, measurable test of the White House's soft regulatory approach within 3-6 months, though enforcement is difficult in open source systems.\n*   A new short course, \"Building Generative AI Applications with Gradio\" by DeepLearning.AI and Hugging Face, teaches creating ML/LLM application demos using Python.",
        "publish_date": "Jul 27, 2022",
        "keywords": [
            "Watermarks",
            "Gradio",
            "DeepLearning.AI",
            "Hugging Face",
            "Python"
        ],
        "url": "https://www.deeplearning.ai/the-batch/bravo-to-ai-companies-that-agreed-to-voluntary-commitments/"
    },
    {
        "content": "Dear friends,Many laws will need to be updated to encourage beneficial AI innovations while mitigating potential harms. One example: Copyright law as it relates to generative AI is a mess! That many businesses are operating without a clear understanding of what is and isn’t legal slows down innovation. The world needs updated laws that enable AI users and developers to move forward without risking lawsuits.Legal challenges to generative AI are on the rise, as you can read here, and the outcomes are by no means clear. I’m seeing this uncertainty slow down the adoption of generative AI in big companies, which are more sensitive to the risk of lawsuits (as opposed to startups, whose survival is often uncertain enough that they may have much higher tolerance for the risk of a lawsuit a few years hence).Meanwhile, regulators worldwide are focusing on how to mitigate AI harm. This is an important topic, but I hope they will put equal effort into crafting copyright rules that would enable AI to benefit more people more quickly.Here are some questions that remain unresolved in most countries:Is it okay for a generative AI company to train its models on data scraped from the open internet? Access to most proprietary data online is governed by terms of service, but what rules should apply when a developer accesses data from the open internet and has not entered into an explicit agreement with the website operator? Having trained on freely available data, is it okay for a generative AI company to stop others from training on its system’s output? If a generative AI company’s system generates material that is similar to existing material, is it liable for copyright infringement? How can we evaluate the allowable degree of similarity?Research has shown that image generators sometimes copy their training data. While the vast majority of generated content appears to be novel, if a customer (say, a media company) uses a third-party generative AI service (such as a cloud provider’s API) to create content, reproduces it, and the content subsequently turns out to infringe a copyright, who is responsible: the customer or the cloud provider? Is automatically generated material protected by copyright, and if so, who owns it? What if two users use the same generative AI model and end up creating similar content — will the one who went first own the copyright?Here’s my view:I believe humanity is better off with permissive sharing of information. If a person can freely access and learn from information on the internet, I’d like to see AI systems allowed to do the same, and I believe this will benefit society. (Japan permits this explicitly. Interestingly, it even permits use of information that is not available on the open internet.)Many generative AI companies have terms of service that prevent users from using output from their models to train other models. It seems unfair and anti-competitive to train your system on others’ data and then stop others from training their models on your system’s output.In the U.S., “fair use” is poorly defined. As a teacher who has had to figure out what I am and am not allowed to use in a class, I’ve long disliked the ambiguity of fair use, but generative AI makes this problem even more acute. Until now, our primary source of content has been humans, who generate content slowly, so we’ve tolerated laws that are so ambiguous that they often require a case-by-case analysis to determine if a use is fair. Now that we can automatically generate huge amounts of content, it’s time to come up with clearer criteria for what is fair. For example, if we can algorithmically determine whether generated content overlaps by a certain threshold with content in the training data, and if this is the standard for fair use, then it would unleash companies to innovate while still meeting a societally accepted standard of fairness.If it proves too difficult to come up with an unambiguous definition of fair use, it would be useful to have “safe harbor” laws: As long as you followed certain practices in generating media, what you did would be considered non-infringing. This would be another way to clarify things for users and generative AI companies.The tone among regulators in many countries is to seek to slow down AI’s harms. While that is important, I hope we see an equal amount of effort put into accelerating AI’s benefits. Sorting out how we should change copyright law would be a good step. Beyond that, we need to craft regulations that clarify not just what’s not okay to do — but also what is explicitly okay to do.Keep learning!Andrew",
        "summary": "*   Laws, particularly copyright, need updating to encourage beneficial AI innovation while mitigating harms, as current legal uncertainty slows down generative AI adoption, especially in large companies.\n*   Key unresolved legal questions for generative AI include training data scraping from the open internet, preventing others from training on AI output, liability for infringing generated content, and copyright ownership of automatically generated material.\n*   Clearer criteria for \"fair use\" or \"safe harbor\" laws for generative AI output are needed to provide clarity for users and companies and encourage innovation.\n*   The author advocates for permissive sharing of information for AI training and hopes regulators will focus equally on accelerating AI benefits, such as clarifying copyright law, alongside mitigating harms.",
        "publish_date": "Jul 19, 2023",
        "keywords": [
            "Generative AI",
            "Training data",
            "AI training",
            "AI output",
            "Data scraping"
        ],
        "url": "https://www.deeplearning.ai/the-batch/time-to-update-copyright-for-generative-ai/"
    },
    {
        "content": "Dear friends,Internalizing this mental framework has made me a more efficient machine learning engineer: Most of the work of building a machine learning system is debugging rather than development.This idea will likely resonate with machine learning engineers who have worked on supervised learning or reinforcement learning projects for years. It also applies to the emerging practice of prompt-based AI development.When you’re building a traditional software system, it’s common practice to write a product spec, then write code to that spec, and finally spend time debugging the code and ironing out the kinks. But when you’re building a machine learning system, it’s frequently better to build an initial prototype quickly and use it to identify and fix issues. This is true particularly for building applications that humans can do well, such as unstructured data tasks like processing images, audio, or text. Build a simple system quickly to see how well it does.Figure out where it falls short (via error analysis or other techniques), and iteratively try to close the gap between what the system does and what a human (such as you, the developer, or a domain expert) would do given the same data.Machine learning software often has to carry out a sequence of steps; such systems are called pipelines or cascades. Say, you want to build a system to route an ecommerce site’s customer emails to the appropriate department (is this apparel, electronics, . . . ), then retrieve relevant product information using semantic search, and finally draft a response for a human representative to edit. Each of these steps could have been done by a human. By examining them individually and seeing where the system falls short of human-level performance, you can decide where to focus your attention.While debugging a system, I frequently have a “hmm, that looks strange” moment that suggests what to try next. For example, I’ve experienced each of the following many times:The learning curve doesn’t quite look right.The system performs worse on what you think are the easier examples.The loss function outputs values that are higher or lower than you think it should.Adding a feature that you thought would help performance actually hurt.Performance on the test set is better than seems reasonable.An LLM’s output is inconsistently formatted; for example, including extraneous text.When it comes to noticing things like this, experience working with multiple projects is helpful. Machine learning systems have a lot of moving parts. When you have seen many learning curves, you start to hone your instincts about what’s normal and what’s anomalous; or when you have prompted a large language model (LLM) to output JSON many times, you start to get a sense of the most common error modes. These days, I frequently play with building different small LLM-based applications on weekends just for fun. Seeing how they behave (as well as consulting with friends on their projects) is helping me to hone my own instincts about when such applications go wrong, and what are plausible solutions.Understanding how the algorithms work really helps, too. Thanks to development tools like TensorFlow and PyTorch, you can implement a neural network in just a few lines of code — that’s great! But what if (or when!) you find that your system doesn’t work well? Taking courses that explain the theory that underlies various algorithms is useful. If you understand at a technical level how a learning algorithm works, you’re more likely to spot unexpected behavior, and you’ll have more options for debugging it.The notion that much of machine learning development is akin to debugging arises from this observation: When we start a new machine learning project, we don’t know what strange and wonderful things we’ll find in the data. With prompt-based development, we also don’t know what strange and wonderful things a generative model will produce. This is why machine learning development is much more iterative than traditional software development: We’re embarking on a journey to discover these things. Building a system quickly and then spending most of your time debugging it is a practical way to get such systems working.Keep learning!Andrew",
        "summary": "*   Most of the work in building a machine learning system is debugging rather than traditional development.\n*   An iterative approach, building a quick prototype and then debugging, is often better, especially for unstructured data tasks and prompt-based AI development.\n*   Debugging involves identifying where the system, often a pipeline or cascade, falls short of human-level performance.\n*   Experience and understanding underlying algorithms are crucial for spotting and addressing unexpected behavior during debugging.",
        "publish_date": "Jul 12, 2023",
        "keywords": [
            "Debugging",
            "Iterative approach",
            "Unstructured data tasks",
            "Prompt-based AI development",
            "Pipeline or cascade"
        ],
        "url": "https://www.deeplearning.ai/the-batch/building-machine-learning-systems-is-more-debugging-than-development/"
    },
    {
        "content": "Dear friends,Prompt-based development is making the machine learning development cycle much faster: Projects that used to take months now may take days. I wrote in an earlier letter that this rapid development is causing developers to do away with test sets.The speed of prompt-based development is also changing the process of scoping projects. In lieu of careful planning, it’s increasingly viable to throw a lot of projects at the wall to see what sticks, because each throw is inexpensive.Specifically, if building a system took 6 months, it would make sense for product managers and business teams to plan the process carefully and proceed only if the investment seemed worthwhile. But if building something takes only 1 day, then it makes sense to just build it and see if it succeeds, and discard it if it doesn’t. The low cost of trying an idea also means teams can try out a lot more ideas in parallel.Say you’re in charge of building a natural language processing system to process inbound customer-service emails, and a teammate wants to track customer sentiment over time. Before the era of large pre-trained text transformers, this project might involve labeling thousands of examples, training and iterating on a model for weeks, and then setting up a custom inference server to make predictions. Given the effort involved, before you started building, you might also want to increase confidence in the investment by having a product manager spend a few days designing the sentiment display dashboard and verifying whether users found it valuable.But if a proof of concept for this project can be built in a day by prompting a large language model, then, rather than spending days/weeks planning the project, it makes more sense to just build it. Then you can quickly test technical feasibility (by seeing if your system generates accurate labels) and business feasibility (by seeing if the output is valuable to users). If it turns out to be either technically too challenging or unhelpful to users, the feedback can help you improve the concept or discard it.I find this workflow exciting because, in addition to increasing the speed of iteration for individual projects, it significantly increases the volume of ideas we can try. In addition to plotting the sentiment of customer emails, why not experiment with automatically routing emails to the right department, providing a brief summary of each email to managers, clustering emails to help spot trends, and many more creative ideas? Instead of planning and executing one machine learning feature, it’s increasingly possible to build many, quickly check if they look good, ship them to users if so, and get rapid feedback to drive the next step of decision making.One important caveat: As I mentioned in the letter about eliminating test sets, we shouldn’t let the speed of iteration lead us to forgo responsible AI. It’s fantastic that we can ship quick-and-dirty applications. But if there is risk of nontrivial harm such as bias, unfairness, privacy violation, or malevolent uses that outweigh beneficial uses, we have a responsibility to evaluate our systems’ performance carefully and ensure that they’re safe before we deploy them widely.What ideas do you have for prompt-based applications? If you brainstorm a few different ways such applications could be useful to you or your company, I hope you’ll implement many of them (safely and responsibly) and see if some can add value!Keep learning,AndrewP.S. We just announced a new short course today, LangChain: Chat with Your Data, built in collaboration with Harrison Chase, creator of the open-source LangChain framework. In this course, you’ll learn how to build one of the most-requested LLM-based applications: Answering questions based on information in a document or collection of documents. This one-hour course teaches you how to do that using retrieval augmented generation (RAG). It also covers how to use vector stores and embeddings to retrieve document chunks relevant to a query.",
        "summary": "*   Prompt-based development significantly accelerates the machine learning development cycle, reducing project timelines from months to days.\n*   The low cost of prompt-based building enables rapid experimentation and trying many project ideas in parallel, shifting from careful planning.\n*   This contrasts with prior ML workflows requiring extensive data labeling, weeks of training, and significant upfront planning.\n*   Despite the speed, responsible AI evaluation for potential harm (bias, unfairness, privacy) remains crucial.\n*   A new course teaches building LLM-based Q&A using Retrieval Augmented Generation (RAG), vector stores, and embeddings.",
        "publish_date": "Jul 05, 2023",
        "keywords": [
            "Retrieval Augmented Generation (RAG)",
            "vector stores",
            "embeddings",
            "Prompt-based development",
            "LLM-based Q&A"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-at-the-speed-of-prompting/"
    },
    {
        "content": "Dear friends,Suddenly it seems like everyone wants to regulate AI. The European Union is on the verge of enacting a comprehensive AI Act that’s intended to mitigate risks and protect individual rights. In the United States, Senate Majority leader Chuck Schumer foresees legislation possibly within months.I’m in favor of regulation, too. But I’m very concerned about whether we’re on a trajectory toward helpful and effective regulation. At the moment, few regulators have sufficient understanding of AI’s potential benefits and harms to craft effective laws. The only thing more dangerous than knowing too little is knowing too little without understanding just how little that is.I’m glad regulators are seeking to learn more about AI. This is a wonderful step! But I see a dangerous situation emerging in which regulators speak with a number of academic and business leaders and come away thinking they understand things well enough. At best, only a few people in the world have the information to answer questions such as:How are AI-enabled paid online ads affecting elections in various countries right now?Is any social media company contributing to genocide or similarly dire events in the world?What types of AI-generated content are being produced (by the recent wave of chatbot companies and others), and how do they influence people?Answering questions like these requires far greater visibility into large AI companies than we currently have. In many countries, publicly traded companies are required to make substantial financial disclosures. Companies may find these requirements intrusive or burdensome, but the resulting transparency builds trust in the financial system. Similarly, the countries of the world need to compel large AI companies to disclose their activities in detail.While the details of any required disclosure need to be worked out, I can imagine, for example, requiring large companies to analyze, or allow independent organizations to analyze, how much content of different flavors (such as pro/con various social issues) they deliver to different subsets of their audience (such as users in a particular region or demographic group). By presenting aggregate results, this can be done in a way that preserves individual privacy. Information like this would enable regulators to draw a straight line between the technology and events in the world. Without it, governments won’t know enough to craft sound regulations.AI is making society richer, and governments have an important role in maximizing its benefits and minimizing its harms. But until there is greater transparency, it will be difficult for lawmakers to recognize the technology’s impacts in either direction. It will be difficult to prevent lobbyists from steering legislation to block competitors or otherwise further their interests in ways that don’t align with society’s.I have deep respect for democratically elected legislators and the important work they do. I hope that all of us in AI — especially the many engineers and scientists who want to make the world better for everyone — can engage to help regulators play a constructive role in AI’s advance.Keep learning!AndrewP.S. We just launched “Generative AI with Large Language Models,” a course built in collaboration with Amazon Web Services. Gain hands-on practice with techniques like reinforcement learning from human feedback; zero-, few-, and one-shot learning; fine-tuning; and advanced prompting using ReAct. You can sign up here.",
        "summary": "*   AI regulation is imminent, with the EU on the verge of enacting a comprehensive AI Act and US legislation possible within months.\n*   A key concern is that few regulators currently have sufficient understanding of AI's potential benefits and harms to craft effective laws.\n*   Effective regulation requires compelling large AI companies to disclose their activities in detail to provide greater visibility into AI's impacts.\n*   Proposed transparency includes requiring companies to analyze and report aggregate results on content delivery (e.g., pro/con social issues) to audience subsets (region, demographic) to enable regulators to understand technology's impact.",
        "publish_date": "Jun 28, 2023",
        "keywords": [
            "EU AI Act",
            "aggregate results",
            "content delivery",
            "audience subsets",
            "demographic"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-lawmakers-need-to-know-about-ai/"
    },
    {
        "content": "Dear friends,I spent Sunday through Tuesday at the CVPR computer vision conference in Vancouver, Canada, along with over 4,000 other attendees. With the easing of the pandemic, it’s fantastic that large conferences are being held in person again!There’s a lot of energy in computer vision right now. As I recall, the natural language processing community was buzzing about transformers a couple of years before ChatGPT revolutionized the field more publicly. At CVPR, I sensed similar excitement in the air with respect to computer vision. It feels like major breakthroughs are coming.It is impossible to summarize hundreds of papers into a single letter, but I want to share some trends that I’m excited about:Vision transformers: The Batch has covered vision transformers extensively, and it feels like they’re still gaining momentum. The vision transformer paper was published in 2020, and already this architecture has become a solid alternative to the convolutional neural network. There are complexities still to be worked out, however. For example, whereas turning a piece of text into a sequence of tokens is relatively straightforward, many decisions need to be made (such as splitting an image into patches, masking, and so on) to turn an image processing problem into a token prediction problem. Many researchers are exploring different alternatives.Image generation: Algorithms for generating images have been a growing part of CVPR since the emergence of GANs and then diffusion models. This year, I saw a lot of creative work on editing images and giving users more fine-grained control over what such models generate. I also saw a lot of work on generating faces, which is not surprising, since faces interest people.NeRF: This approach to generating a 3D scene from a set of 2D images has been taking off for a while (and also covered extensively in The Batch). Still, I was surprised at the large number of papers on NeRF. Researchers are working to scale up NeRF to larger scenes, make it run more efficiently, handle moving scenes, work with a smaller number of input images, and so on.Although it was less pronounced than excitement around the topics above, I also noticed increased interest in multimodal models. Specifically, given that a transformer can convert either an image or a piece of text into a sequence of tokens, you can feed both types of tokens into the same transformer model to have it process inputs that include both images and text. Many teams are exploring architectures like this.Lastly, even though the roadmap to self-driving cars has been longer than many people expected, there remains a lot of research in this area. I think the rise of large, pretrained transformers will help kickstart breakthroughs in self-driving.I also spoke at the CVPR conference’s workshop on Computer Vision in the Wild about Landing AI’s work on making computer vision easy, with visual prompting as a key component. (Thank you Jianwei Yang, Jianfeng Gao, and the other organizers for inviting me!) After my presentation, speaking with many users of computer vision, it struck me that there’s still a gap between the problems studied/benchmarks used in academic research and commercial practice. For example, test sets are more important in academic research than in practical applications; I will write more about this topic in the future.To everyone I met in person at CVPR: Thank you! Meeting so many people made this trip a real highlight for me.Keep learning!Andrew",
        "summary": "* Vision transformers are gaining momentum as an alternative to convolutional neural networks (CNNs), with ongoing research addressing complexities like image tokenization.\n* Image generation research is growing, focusing on editing, fine-grained control, and face generation, building on GANs and diffusion models.\n* NeRF (Neural Radiance Fields) is a significant trend for generating 3D scenes from 2D images, with research focused on scaling, efficiency, and handling complex scenarios.\n* Increased interest was noted in multimodal models that combine image and text tokens using transformers.\n* Research in self-driving cars continues, potentially boosted by large pretrained transformers, alongside a noted gap between academic research and commercial practice.",
        "publish_date": "Jun 21, 2023",
        "keywords": [
            "Vision transformers",
            "NeRF (Neural Radiance Fields)",
            "diffusion models",
            "GANs",
            "multimodal models"
        ],
        "url": "https://www.deeplearning.ai/the-batch/innovations-in-computer-vision-at-this-years-cvpr-conference/"
    },
    {
        "content": "Dear friends,AI risks are in the air — from speculation that AI, decades or centuries from now, could bring about human extinction to ongoing problems like bias and fairness. While it’s critically important not to let hypothetical scenarios distract us from addressing realistic issues, I’d like to talk about a long-term risk that I think is realistic and has received little attention: If AI becomes cheaper and better than many people at doing most of the work they can do, swaths of humanity will no longer contribute economic value. I worry that this could lead to a dimming of human rights.We’ve already seen that countries where many people contribute little economic value have some of the worst records of upholding fundamental human rights like free expression, education, privacy, and freedom from mistreatment by authorities. The resource curse is the observation that countries with ample natural resources, such as fossil fuels, can become less democratic than otherwise similar countries that have fewer natural resources. According to the World Bank,“developing countries face substantially higher risks of violent conflict and poor governance if [they are] highly dependent on primary commodities.”A ruler (perhaps dictator) of an oil-rich country, for instance, can hire foreign contractors to extract the oil, sell it, and use the funds to hire security forces to stay in power. Consequently, most of the local population wouldn’t generate much economic value, and the ruler would have little incentive to make sure the population thrived through education, safety, and civil rights.What would happen if, a few decades from now, AI systems reach a level of intelligence that disempowers large swaths of people from contributing much economic value? I worry that, if many people become unimportant to the economy, and if relatively few people have access to AI systems that could generate economic value, the incentive to take care of people — particularly in less democratic countries — will wane.Marc Andreessen recently pointed out that Tesla, having created a good car, has an incentive to sell it to as many people as possible. So why wouldn’t AI builders similarly make AI available to as many people as possible? Wouldn’t this keep AI power from becoming concentrated within a small group? I have a different point of view. Tesla sells cars only to people who generate enough economic value, and thus earn enough wages, to afford one. It doesn’t sell many cars to people who have no earning power.Researchers have analyzed the impact of large language models on labor. While, so far, some people whose jobs were taken by ChatGPT have managed to find other jobs, the technology is advancing quickly. If we can’t upskill people and create jobs fast enough, we could be in for a difficult time. Indeed, since the great decoupling of labor productivity and median incomes in recent decades, low-wage workers have seen their earnings stagnate, and the middle class in the U.S. has dwindled.Many people derive tremendous pride and sense of purpose from their work. If AI systems advance to the point where most people no longer can create enough value to justify a minimum wage (around $15 per hour in many places in the U.S.), many people will need to find a new sense of purpose. Worse, in some countries, the ruling class will decide that, because the population is no longer important for production, people are no longer important.What can we do about this? I’m not sure, but I think our best bet is to work quickly to democratize access to AI by (i) reducing the cost of tools and (ii) training as many people as possible to understand them. This will increase the odds that people have the skills they need to keep creating value. It will also ensure that citizens understand AI well enough to steer their societies toward a future that’s good for everyone.Keep working to make the world better for everyone!Andrew",
        "summary": "*   A long-term risk discussed is AI becoming cheaper and better than humans at most work, potentially causing many people to lose economic value.\n*   This loss of economic value could lead to a dimming of human rights, particularly in less democratic countries where rulers might have less incentive to support a population not crucial for economic production.\n*   Analysis of Large Language Models (LLMs) indicates potential labor displacement, highlighting the need for rapid upskilling and job creation.\n*   A potential strategy to mitigate this risk is democratizing AI access by reducing tool costs and training people to understand/use AI, aiming to help people continue creating value.",
        "publish_date": "Jun 14, 2023",
        "keywords": [
            "Large Language Models (LLMs)",
            "AI Capabilities",
            "LLM Analysis",
            "AI Access",
            "AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-risk-and-the-resource-curse/"
    },
    {
        "content": "Dear friends, Over the weekend, Hamas launched a surprise terrorist attack on Israel, slaughtering and kidnapping civilians. The images in the media are horrifying, and over 1,000 people have been murdered in Israel, including numerous children. Israel has retaliated by laying siege to and attacking the Gaza Strip. The mounting civilian casualties in Israel and Palestine are heartbreaking. My heart goes out to all individuals, families, and communities affected by the violence. While there is much to be said about rights and wrongs committed by all sides over the past 75 years, there is absolutely no excuse for deliberately targeting civilians or threatening to execute hostages. This is a time for all people of conscience to condemn these heinous acts. It is also time to call on everyone to respect human rights and the international rule of law.  I hope the AI community can play a constructive role in preserving lives as well as promoting civil liberties and democracy. In this moment and in coming years, I hope we remain united as a community, keep pushing for human rights, and decry any violations thereof. Andrew",
        "summary": "* Hope is expressed that the AI community can play a constructive role in preserving lives, promoting civil liberties, and democracy.",
        "publish_date": "Oct 11, 2023",
        "keywords": [
            "Artificial Intelligence",
            "AI community",
            "Preserving lives",
            "Civil liberties",
            "Democracy"
        ],
        "url": "https://www.deeplearning.ai/the-batch/horror-and-heartbreak/"
    },
    {
        "content": "Dear friends,Andrej Karpathy, one of the Heroes of Deep Learning who currently works at OpenAI, quipped, “The hottest programming language is English.” While I appreciate the sentiment, I don’t want the ease of instructing computers in English to discourage anyone from learning to code. Someone who is multilingual — who perhaps speaks English as a first language and Python as a second language — can accomplish much more than someone who knows only how to prompt a large language model (LLM).It’s increasingly possible to tell a computer what you want in English (or whatever human language you’re most fluent in) and it will understand well enough to give you what you asked for. Even before LLMs, Siri and Alexa could respond to basic commands, and the space of English instructions that computers can follow is rapidly expanding. But coding is still immensely valuable. If anything, with the advent of LLMs, the value of coding is rising. Let me explain why.Today, almost everyone has data: big companies, small companies, and even high school students running biology experiments. Thus, the ability to get a custom AI system to work on your own data is valuable. And while prompting an LLM can produce answers for a huge range of questions and generate everything from essays to poems, the set of things you can do with coding plus prompting is significantly larger, for now and the near future.Let’s say I want a summary of every letter I’ve ever written in The Batch. I can copy-paste one letter at a time into an LLM like ChatGPT and ask for a summary of each, but it would be much more efficient for me to write a simple piece of code that iterates over all letters in a database and prompts an LLM to create summaries.In the future, I hope recruiters will be able to write a few lines of code to summarize candidate reviews, run speech recognition on conversations with references, or execute whatever custom steps are needed in the recruiting workflow. I hope teachers will be able to prompt an LLM to generate learning tasks suited to their lesson plan, and so on. For many roles, coding + prompting will be more powerful than prompting via a web interface alone.Furthermore, English is ambiguous. This contributes to why an LLM’s output in response to a prompt isn’t fully predictable. In contrast, most programming languages are unambiguous, so when you run a piece of code, you reliably (within reason) get back the same result each time. For important applications where reliability is important — say, deciding when to purchase an expensive plane ticket based on real-time prices, or sending a party invitation to everyone in your company — it’s safer to use code to carry out the final step committing to the action, even if an LLM were involved in researching destinations or drafting the invitation.I believe we’re entering an era when everyone can benefit by learning to code. LLMs have made it more valuable than ever. Writing code that calls an LLM has made it easier to build intelligent applications than it was before LLM APIs became widely available. Specifically, everyone can benefit by learning to code AI applications, as I wrote with Andrea Pasinetti, CEO of Kira Learning, an AI Fund portfolio company.If you don’t yet code, consider taking a Python course to get started. If you already code, I hope you will encourage others to take up this skill. This is a good time to help everyone learn to speak Python as a second language!Keep learning,Andrew",
        "summary": "* While Large Language Models (LLMs) enable instructing computers in human language, coding remains valuable.\n* Combining coding with LLM prompting is significantly more powerful for custom tasks on personal data than prompting via a web interface alone.\n* Coding provides reliability and predictability for critical applications, contrasting with the ambiguity of human language and LLM outputs.\n* Learning to code AI applications, including writing code that calls LLM APIs, is increasingly beneficial and easier.",
        "publish_date": "Oct 04, 2023",
        "keywords": [
            "Large Language Models (LLMs)",
            "coding",
            "LLM prompting",
            "LLM APIs",
            "AI applications"
        ],
        "url": "https://www.deeplearning.ai/the-batch/coding-skill-is-more-valuable-than-ever/"
    },
    {
        "content": "Dear friends,It’s time to move beyond the stereotype that machine learning systems need a lot of data. While having more data is helpful, large pretrained models make it practical to build viable systems using a very small labeled training set — perhaps just a handful of examples specific to your application.About 10 years ago, with the rise of deep learning, I was one of the leading advocates for scaling up data and compute to drive progress. That recipe has carried us far, and it continues to drive progress in large language models, which are based on transformers. A similar recipe is emerging in computer vision based on large vision transformers.But once those models are pretrained, it takes very little data to adapt them for a new task. With self-supervised learning, pretraining can happen on unlabeled data. So, technically, the model did need a lot of data for training, but that was unlabeled, general text or image data. Then, even with only a small amount of labeled, task-specific data, you can get good performance.For example, say you have a transformer trained on a massive amount of text, and you want it to perform sentiment classification on your own dataset. The most common techniques are:Fine-tuning the model to your dataset. Depending on your application, this can be done with dozens or even fewer examples.Few-shot learning. In this approach, you create a prompt that includes a few examples (that is, you write a text prompt that lists a handful of pieces of text and their sentiment labels). A common technique for this is in-context learning.Zero-shot learning, in which you write a prompt that describes the task you want done.These techniques work well. For example, customers of my team Landing AI have been building vision systems with dozens of labeled examples for years.The 2010s were the decade of large supervised models, I think the 2020s are shaping up to be the decade of large pretrained models. However, there is one important caveat: This approach works well for unstructured data (text, vision and audio) but not for structured data, and the majority of machine learning applications today are built on structured data.Models that have been pretrained on diverse unstructured data found on the web generalize to a variety of unstructured data tasks of the same input modality. This is because text/images/audio on the web have many similarities to whatever specific text/image/audio task you might want to solve. But structured data such as tabular data is much more heterogeneous. For instance, the dataset of Titanic survivors probably has little in common with your company’s supply chain data.Now that it's possible to build and deploy machine learning models with very few examples, it’s also increasingly possible to build and launch products very quickly — perhaps without even bothering to collect and use a test set. This is an exciting shift. I’m confident that this will lead to many more exciting applications, including specifically ones where we don’t have much labeled data.Keep learning!Andrew",
        "summary": "*   Large pretrained models enable building viable ML systems with a very small labeled training set (dozens or fewer examples), moving beyond the stereotype of needing a lot of data.\n*   Pretraining often uses large amounts of unlabeled data (self-supervised learning), allowing adaptation to new tasks with small labeled, task-specific data.\n*   Techniques like fine-tuning (dozens or fewer examples), few-shot learning (in-context learning), and zero-shot learning are used with pretrained models for tasks requiring minimal labeled data.\n*   This approach works well for unstructured data (text, vision, audio) but not for structured data (tabular) due to its heterogeneity.",
        "publish_date": "May 24, 2023",
        "keywords": [
            "Pretrained models",
            "Self-supervised learning",
            "Fine-tuning",
            "Few-shot learning",
            "Zero-shot learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/building-ai-systems-no-longer-requires-much-data/"
    },
    {
        "content": "Dear friends,A few weeks ago, I wrote about my team at Landing AI’s work on visual prompting. With the speed of building machine learning applications through text prompting and visual prompting, I’m seeing a trend toward building and deploying models without using a test set. This is part of an important trend of speeding up getting models into production.The test set has always been a sacred aspect of machine learning development. In academic machine learning work, test sets are the cornerstone of algorithm benchmarking and publishing scientific conclusions. Test sets are also used in commercial machine learning applications to measure and improve performance and to ensure accuracy before and after deployment.But thanks to prompt-based development, in which you can build a model simply by providing a text prompt (such as “classify the following text as having either a positive or negative sentiment”) or a visual prompt (by labeling a handful of pixels to show the model what object you want to classify), it is possible to build a decent machine learning model with very few examples (few-shot learning) or no examples at all (zero-shot learning).Previously, if we needed 10,000 labeled training examples, then the additional cost of collecting 1,000 test examples didn’t seem onerous. But the rise of zero-shot and few-shot learning — driven by prompt-based development — is making test set collection a bottleneck.Thus I'm seeing more and more teams use a process for development and deployment that looks like this:Use prompting to develop a model. This can take minutes to hours.Deploy the model to production and run it on live data quickly but safely, perhaps by running in “shadow mode,” where the model’s inferences are stored and monitored but not yet used. (More on this below.)If the model’s performance is acceptable, let it start making real decisions.Only after the model is in production, and only if we need to benchmark more carefully (say, to eke out a few percentage points of performance improvement), collect test data to create a more careful benchmark for further experimentation and development. But if the system is doing well enough, don’t bother with this.I’m excited by this process, which significantly shortens the time it takes to build and deploy machine learning models. However, there is one important caveat: In certain applications, a test set is important for managing risk of harm. Many deployments don’t pose a significant risk of harm; for example, a visual inspection system in a smartphone factory that initially shadows a human inspector and whose outputs aren’t used directly yet. But if we're developing a system that will be involved in decisions about healthcare, criminal justice, finance, insurance, and so on, where inaccurate outputs or bias could cause significant harm, then it remains important to collect a rigorous test set and deeply validate the model’s performance before allowing it to make consequential decisions.The occurrence of concept drift and data drift can make the very notion of a “test set” problematic in practical applications, because the data saved for testing no longer matches the real distribution of input data. For this reason, the best test data is production data. For applications where it’s safe and reasonable to deploy without using a test set, I’m excited about how this can speed up development and deployment of machine learning applications.Keep learning!Andrew",
        "summary": "*   A trend is emerging towards building and deploying ML models without traditional test sets to accelerate production.\n*   Prompt-based development enabling few-shot and zero-shot learning makes test set collection a bottleneck, driving this trend.\n*   A proposed process involves rapid prompting-based development, deployment to production (e.g., shadow mode), and collecting test data only post-deployment if needed for benchmarking.\n*   Test sets remain crucial for managing risk in high-stakes applications (healthcare, criminal justice, finance, insurance) where inaccuracy could cause significant harm.\n*   Concept drift and data drift can make traditional test sets problematic; production data is often the best test data.",
        "publish_date": "May 17, 2023",
        "keywords": [
            "Prompt-based development",
            "few-shot learning",
            "zero-shot learning",
            "Concept drift",
            "data drift"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-prompting-is-changing-machine-learning-development/"
    },
    {
        "content": "Dear friends,Welcome to the Halloween special issue of The Batch, where we take a look at fears associated with AI. In that spirit, I’d like to address a fear of mine: Sensationalist claims that AI could bring about human extinction will cause serious harm.In recent months, I sought out people concerned about the risk that AI might cause human extinction. I wanted to find out how they thought it could happen. They worried about things like a bad actor using AI to create a bioweapon or an AI system inadvertently driving humans to extinction, just as humans have driven other species to extinction through lack of awareness that our actions could have that effect. When I try to evaluate how realistic these arguments are, I find them frustratingly vague and nonspecific. They boil down to “it could happen.” Trying to prove it couldn’t is akin to proving a negative. I can’t prove that AI won’t drive humans to extinction any more than I can prove that radio waves emitted from Earth won’t lead space aliens to find us and wipe us out. Such overblown fears are already causing harm. High school students who take courses designed by Kira Learning, an AI Fund portfolio company that focuses on grade-school education, have said they are apprehensive about AI because they’ve heard it might lead to human extinction, and they don’t want to be a part of that. Are we scaring students away from careers that would be great for them and great for society?I don’t doubt that many people who share such worries are sincere. But others have a significant financial incentive to spread fear: Individuals can gain attention, which can lead to speaking fees or other revenue.Nonprofit organizations can raise funds to combat the phantoms that they’ve conjured.Legislators can boost campaign contributions by acting tough on tech companies.I firmly believe that AI has the potential to help people lead longer, healthier, more fulfilling lives. One of the few things that can stop it is regulators passing ill-advised laws that impede progress. Some lobbyists for large companies — some of which would prefer not to have to compete with open source — are trying to convince policy makers that AI is so dangerous, governments should require licenses for large AI models. If enacted, such regulation would impede open source development and dramatically slow down innovation. How can we combat this? Fortunately, I think the developer and scientific communities believe in spreading truthful, balanced views, and open source has a lot of supporters. I hope all of us can keep promoting a positive view of AI.AI is far from perfect, and we have much work ahead of us to make it safer and more responsible. But it already benefits humanity tremendously and will do so even more in the future. Let’s make sure unsubstantiated fears don’t handicap that progress.Witching you lots of learning,AndrewP.S. We have a Halloween treat for you! LangChain CEO Harrison Chase has created a new short course, “Functions, Tools, and Agents with LangChain.” It covers the latest capabilities in large language models, including OpenAI’s models, to call functions. This is very useful for handling structured data and a key building block for LLM-based agents. Sign up here!",
        "summary": "*   Sensationalist claims about AI causing human extinction are viewed as vague and harmful, potentially discouraging students from pursuing AI careers.\n*   The author argues that AI has significant potential benefits for humanity despite being imperfect and needing work on safety and responsibility.\n*   Ill-advised regulations, such as requiring licenses for large AI models, could impede AI progress, innovation, and open source development.\n*   A course on \"Functions, Tools, and Agents with LangChain\" is highlighted, covering LLM capabilities like calling functions and building LLM-based agents.",
        "publish_date": "Letters",
        "keywords": [
            "LangChain",
            "LLM-based agents",
            "Function calling",
            "Tools",
            "Large AI models"
        ],
        "url": "https://www.deeplearning.ai/the-batch/exaggerated-fear-of-ai-is-causing-real-harm/"
    },
    {
        "content": "Dear friends,There are many great applications to be built on top of large language models, and the overhead of doing so may be lower than you think. Sometimes, I’ve spent all day on a weekend developing ideas only to find that I've spent less than $0.50.Given the low cost of keeping me busy all day, It might not surprise you to find that the cost of scaling up a business based on a large language model (LLM) can be quite inexpensive. As a back-of-the-envelope calculation, let’s say:It costs $0.002 per 1,000 tokens, the current price of OpenAI's popular gpt-3.5-turbo conversational model. Pricing can be up to 5x lower or 30x higher depending on the model's quality, but this one is popular among developers, so let's go with it.A token corresponds to 0.75 words.A user can read 250 words per minute.Length of prompts and generated responses is roughly the same.Then it costs around $0.08 to generate enough text to keep someone busy for an hour.Here are some ways to think about this when it comes to automating or assisting a person’s work task:For most tasks that we might hire someone to do, the cost is significantly more than $0.08 per hour. For example, minimum wage in some places in the US is $15 per hour, and Amazon Mechanical Turk workers might work for around $5 per hour. So the cost of using an LLM to automate of most human tasks is very inexpensive.If you’re generating text for a person to read, the cost of the time spent reading is significantly greater than the cost of generating the text.On the flip side:Up to an order of magnitude, social media companies might make around $0.10 per hour that a user spends on their sites. So if we’re generating personalized text for one person, the financial case is iffy. (I don’t think this is necessarily a bad thing. Society doesn’t need people to spend even more time on social media!)On the other hand, if we’re generating content to be read by a large audience, such as a news article, then the cost is amortized across the audience, and it is quite inexpensive again.Please don’t use my back-of-the-envelope calculation for any significant business decisions, and do carry out your own calculations with careful assumptions specific to your project. But if you haven’t stepped through such a calculation before, the takeaway is that LLMs are actually quite inexpensive to use.Granted, some models (like one version of GPT-4, at 15-30x the cost used in the calculation, leading to a cost of $1.80 instead of $0.08) are much more expensive. If your application requires a more capable model, then the calculation does change. But I’m optimistic that prices will come down over time, and these are all wonderful tools to have in your toolbox.Keep learning!AndrewP.S. I’ve noticed that most LLM providers don’t have transparent pricing. If you work at an LLM provider, I hope you’ll consider urging your company to list prices on its website.",
        "summary": "*   A back-of-the-envelope calculation based on gpt-3.5-turbo pricing ($0.002 per 1,000 tokens, 1 token = 0.75 words) estimates the cost to generate enough text for an hour of reading is around $0.08.\n*   Using LLMs for task automation is significantly less expensive than human labor costs, such as minimum wage ($15/hour) or Mechanical Turk ($5/hour).\n*   More capable LLMs like GPT-4 can be 15-30x more expensive than gpt-3.5-turbo, potentially costing around $1.80 for the same text generation amount.\n*   The cost effectiveness varies by application; generating personalized text for one person may be financially questionable compared to social media revenue ($0.10/hour), but costs are inexpensive when amortized over a large audience.",
        "publish_date": "May 10, 2023",
        "keywords": [
            "gpt-3.5-turbo",
            "GPT-4",
            "tokens",
            "LLMs",
            "text generation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/doing-business-with-chatbots/"
    },
    {
        "content": "Dear friends,I wrote earlier about how my team at AI Fund saw that GPT-3 set a new direction for building language applications, two years before ChatGPT was released. I’ll go out on a limb to make another prediction: I think we’ll see significant growth in AI, including Generative AI, applications running at the edge of the network (PC, laptop, mobile, and so on). I realize this flies in the face of conventional wisdom. Most AI runs in data centers, not on edge devices. There are good reasons for this:The most powerful large language models require 100B+ parameters and massive amounts of memory even for inference (100B parameters, stored using 8- bit quantization, requires 100GB of memory).Many businesses prefer to operate cloud-based, software-as-a-service (SaaS) products (which allows them to charge a recurring subscription fee) rather than software running at the edge (where customers tend to prefer paying a one-time fee). SaaS also gives the company access to data to improve the product and makes the product easier to upgrade.Many developers today have been trained to build SaaS applications, and want to build cloud-hosted applications rather than desktop or other edge applications.Here’s why I think those factors won’t stop AI’s growth at the edge. AI applications are starting to run quite well on modern edge devices. For example, I regularly run models with around 1B to 10B parameters on my laptop. If I’m working on an airplane without WiFi access, I will occasionally run a small model to help me with my work.For many applications, a model of modest size works fine, especially if it’s fine-tuned to the task at hand. To help me find grammatical errors in my writing, do I really need a 175B parameter model that has broad knowledge of philosophy, history, astronomy, and every other topic under the sun?Many users, especially those from Gen Z (born around 1996 to 2010), whose behavior tends to be a leading indicator of future consumer trends, are increasingly sensitive to privacy. This has been a boon to Apple’s product sales, given the company’s reputation for privacy. Surely, to check my grammar, I don’t need to share my data with a big tech company? Similarly, for corporations worried about their own data privacy, edge computing (as well as on-premises and virtual private cloud options) could be appealing.Further, strong commercial interests are propelling AI to the edge. Chip makers like Nvidia, AMD, and Intel sell chips to data centers (where sales have grown rapidly) and for use in PCs and laptops (where sales have plummeted since the pandemic). Thus, semiconductor manufacturers as well as PC/laptop makers (and Microsoft, whose sales of the Windows operating system depend on sales of new PC/laptops) are highly motivated to encourage adoption of edge AI, since this would likely require consumers to upgrade their devices to have the more modern AI accelerators. So many companies stand to benefit from the rise of edge AI and will have an incentive to promote it. AI Fund has been exploring a variety of edge AI applications, and I think the opportunities will be rich and varied. Interesting semiconductor technology will support them. For example, AMD’s xDNA architecture, drawing on configurable cores designed by Xilinx (now an AMD company), is making it easier to run multiple AI models simultaneously. This enables a future in which one AI model adjusts image quality on our video call, another checks our grammar in real time, and a third pulls up relevant articles.  While it’s still early days for edge AI — in both consumer and industrial markets (for example, running in factories or on heavy machinery) — I think it’s worth investigating, in addition to the numerous opportunities in cloud-hosted AI applications.  Keep learning!Andrew P.S. My team at Landing AI will present a livestream, “Building Computer Vision Applications,” on Monday, November 6, 2023, at 10 a.m. Pacific Time. We’ll discuss the practical aspects of building vision applications including how to identify and scope vision projects, choose a project type and model, apply data-centric AI, and develop an MLOps pipeline. Register here!",
        "summary": "* Prediction of significant growth in AI, including Generative AI, applications running at the edge (PC, laptop, mobile).\n* While powerful LLMs (100B+ parameters, requiring 100GB memory @ 8-bit quantization) typically run in data centers, edge AI is growing.\n* Growth drivers include modern devices handling 1B to 10B parameter models, sufficiency of modest models for many tasks, user/corporate privacy concerns, and commercial interests.\n* Hardware advancements like AMD's xDNA architecture support running multiple AI models simultaneously on edge devices.",
        "publish_date": "Oct 18, 2023",
        "keywords": [
            "8-bit quantization",
            "AMD's xDNA architecture",
            "100B+ parameters",
            "1B to 10B parameter models",
            "LLMs"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-ai-will-move-to-edge-devices/"
    },
    {
        "content": "Dear friends,Amidst rising worry about AI harms both realistic (like job loss) and unrealistic (like human extinction), It’s critical to understand AI’s potential to do tremendous good. Our new specialization, AI for Good is designed to empower both technical and nontechnical people to identify, scope, and build impactful AI projects.In this series of courses, you’ll learn when and how to use AI effectively for positive impact in situations where stakes are high and human lives may hang in the balance. AI for Good presents a practical framework for applying machine learning to socially important projects (and products of any kind). It illustrates this framework with several real-world examples of AI projects that are improving climate change, disaster response, and public health.AI for Good is designed to be useful whether or not you have coding experience. It does include Python code examples that you can execute and interact with to gain deeper insight into different applications. However, it doesn’t assume previous experience with AI or programming. So please recommend this to your nontechnical friends!There’s often a huge gap between training a model that does well on a test set and one that actually works on real data and affects real people. This specialization will help you tell the difference, so your projects reach people and better their lives.AI for Good is taught by Robert Monarch, who has applied AI in public health and disaster response for over 20 years. He has founded AI startups and shipped successful AI products at Amazon, Google, Microsoft, and Apple. He’ll show you how to move your own AI projects through the stages of exploration, design, implementation, and evaluation.AI is experiencing a time of rapid growth, and the AI community’s role in making sure it does significant good is more important than ever. I hope you’ll check out AI for Good!Do good,AndrewP.S. We also have a new short course: “Understanding and Applying Text Embeddings with Vertex AI,” developed in collaboration with Google Cloud and taught by Nikita Namjoshi and me. Learn the fundamentals of text embeddings — an essential piece of the GenAI developer’s toolkit — and apply them to classification, outlier detection, text clustering, and semantic search. You’ll also learn how to combine text generation and semantic search to build a question-answering system. Please join us!",
        "summary": "*   A new \"AI for Good\" specialization teaches applying machine learning with a practical framework for positive impact in areas like climate change, disaster response, and public health, targeting technical and nontechnical people.\n*   The specialization focuses on bridging the gap between model training and real-world application, covering project stages: exploration, design, implementation, and evaluation.\n*   A new short course, \"Understanding and Applying Text Embeddings with Vertex AI,\" covers text embedding fundamentals and applications in classification, outlier detection, text clustering, semantic search, and building question-answering systems by combining text generation and semantic search.",
        "publish_date": "Sep 06, 2023",
        "keywords": [
            "Text Embeddings",
            "Vertex AI",
            "Semantic Search",
            "Text Clustering",
            "Question-Answering Systems"
        ],
        "url": "https://www.deeplearning.ai/the-batch/unlocking-ais-potential-for-positive-impact/"
    },
    {
        "content": "Dear friends,As I wrote in an earlier letter, whether AI is sentient or conscious is a philosophical question rather than a scientific one, since there is no widely agreed-upon definition and test for these terms. While it is tempting to “solve” this problem by coming up with precise definitions and well defined tests for whether a system meets them, I worry that poor execution will lead to premature declarations of AI achieving such criteria and generate unnecessary hype. Take the concept of self-awareness, which refers to a conscious knowledge of one's own self. Suppose we define a robot as self-aware if it can recognize itself in the mirror, which seems a natural way to test a robot’s awareness of itself. Given this definition — and that it’s not very hard to build a robot that recognizes itself — we would be well on a path to hype about how AI was now self-aware. This example isn’t a prediction about the future. It actually happened about 10 years ago, when many media sources breathlessly reported that a robot “Passes Mirror Test, Is Therefore Self-Aware  … conclusively proving that robots are intelligent and self-aware.” While bringing clarity to ambiguous definitions is one way for science to make progress, the practical challenge is that many people already have beliefs about what it means for something to be self-aware, sentient, conscious, or have a soul. There isn’t widespread agreement on these terms. For example, do all living things have souls? How about a bacterium or virus?So even if someone comes up with a reasonable new scientific definition, many people — unaware of the new definition — will still understand the term based on their earlier understanding. Then, when media outlets start talking about how AI has met the definition, people won’t recognize that the hype refers to a narrow objective (like a robot recognizing itself in the mirror). Instead, they’ll think that AI accomplished what they generally associate with words like sentience. Because of this, I have mixed feelings about attempts to come up with new definitions of artificial general intelligence (AGI). I believe that most people, including me, currently think of AGI as AI that can carry out any intellectual task that a human can. With this definition, I think we’re still at least decades away from AGI. This creates a temptation to define it using a lower bar, which would make it easier to declare success: The easiest way to achieve AGI might be to redefine what the term means! Should we work to clarify the meanings of ambiguous terms that relate to intelligence? In some cases, developing a careful definition and getting widespread agreement behind it could set a clear milestone for AI and help move the field forward. But in other cases, I’m satisfied to avoid the risk of unnecessary hype and leave it to the philosophers.Keep learning!AndrewP.S. LLMOps is a rapidly developing field that takes ideas from MLOps (machine learning operations) and specializes them for building and deploying LLM-based applications. In our new course, “LLMOps,” taught by Google Cloud’s Erwin Huizenga, you’ll learn how to use automation and experiment tracking to speed up development. Specifically, you’ll develop an LLMOps pipeline to automate LLM fine-tuning. By building a tuning pipeline and tracking the experiment artifacts — including the parameters, inputs, outputs, and experimental results — you can reduce manual steps in the development process, resulting in a more efficient workflow. Sign up here!",
        "summary": "* Whether AI is sentient or conscious is a philosophical question lacking scientific definition/tests, risking hype from narrow criteria (e.g., mirror test ~10 years ago).\n* Artificial General Intelligence (AGI) is commonly understood as AI capable of any human intellectual task, estimated to be decades away, creating temptation to redefine it lower.\n* Clarifying ambiguous AI terms (sentience, consciousness, AGI) could set milestones but risks public misunderstanding and hype if definitions are narrow.\n* LLMOps is a field specializing MLOps for LLM applications, focusing on automating LLM fine-tuning pipelines and experiment tracking for development efficiency.",
        "publish_date": "Jan 17, 2024",
        "keywords": [
            "LLMOps",
            "MLOps",
            "LLM fine-tuning pipelines",
            "experiment tracking",
            "Artificial General Intelligence (AGI)"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-easiest-way-to-achieve-artificial-general-intelligence/"
    },
    {
        "content": "Dear friends,Earlier this month, my team AI Fund held its annual co-founder and CEO summit, where many of our collaborators gathered in California for two days to discuss how to build AI companies. Three themes emerged from many presentations: persistence, fast iteration and community. Persistence. Doing impactful work is hard! Tim Westergren (founder and former CEO of Pandora, Venture Advisor at AI Fund) said it was only on his 348th venture pitch that Pandora raised its Series A round of funding. He also spoke about the tough time when Pandora team members went without salaries for an extended period of time to try to make the company work out. While many people unfortunately are not in a position to make such sacrifices to build a business, sometimes it does take extraordinary effort — and, yes, sacrifices — to do something really meaningful. Fast iteration. AI Fund’s process of building startups is focused on a three- month, bi-weekly sprint process, in which we iterate quickly through technical prototypes as well as business ideas. Bill MacCartney (former VP of Cohere, Venture Advisor at AI Fund) said, “The best way to start is just by building on top of . . . whatever the best model is . . .. Don’t worry about [cost or latency] at first. You’re really just trying to validate the idea.” One technique that’s now very widespread for prototyping is retrieval augmented generation (RAG). I’ve been surprised at how many nontechnical business leaders seem to know what RAG is. Investors are sometimes leery of people who build a thin layer around LLMs. As Laurence Moroney (lead AI Advocate at Google, AI Fund Fellow) says, “I’m a huge fan of RAG . . .. I think this is one way to go beyond a thin veneer around [commercial] models and build a somewhat thicker veneer.” Community. Despite the wide range of startups represented in sectors including deep AI tech, healthcare, finance, edtech, and so on, a recurring theme was that company builders end up stronger when they come together. Emil Stefanutti (co-founder of ContractRoom, Venture Advisor at AI Fund) said he was glad that many of the scars he has acquired by building businesses are turning out to be treasures for others, as he's able to share experiences that other entrepreneurs can benefit from. Tim Westergren said, “You can’t white-knuckle it. You also can’t do it alone.”The themes of persistence, fast iteration, and community apply whether you work in a large company, startup, research, government, or elsewhere. When I think of innovators in any field, I often think of Teddy Roosevelt’s message:“It is not the critic who counts; not the [person] who points out how the strong [person] stumbles, or where the doer of deeds could have done them better. The credit belongs to the [person] who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, … who knows great enthusiasms, the great devotions; who spends himself in a worthy cause.”Keep learning!Andrew",
        "summary": "*   Themes for building AI companies discussed at an AI Fund summit included fast iteration, utilizing a 3-month, bi-weekly sprint process for technical prototypes and business ideas.\n*   A recommended approach for starting AI development is building on top of the best available model to validate the idea, initially without focusing on cost or latency.\n*   Retrieval Augmented Generation (RAG) is a widespread technique for prototyping AI applications.\n*   RAG is seen as a way to build a \"somewhat thicker veneer\" beyond a \"thin layer around [commercial] models\".",
        "publish_date": "Feb 21, 2024",
        "keywords": [
            "Retrieval Augmented Generation (RAG)",
            "models",
            "technical prototypes",
            "iteration",
            "thin layer"
        ],
        "url": "https://www.deeplearning.ai/the-batch/three-themes-for-ai-entreprenurs/"
    },
    {
        "content": "Dear friends, Last week, I attended the NeurIPS conference in New Orleans. It was fun to catch up with old friends, make new ones, and also get a wide scan of current AI research. Work by the big tech companies tends to get all the media coverage, and NeurIPS was a convenient place to survey the large volume of equally high-quality work by universities and small companies that just don’t have a comparable marketing budget! AI research has become so broad that I struggle to summarize everything I saw in a few sentences. There were numerous papers on generative AI, including large language models, large multimodal models, diffusion models, enabling LLMs to use tools (function calls), and building 3D avatars. There was also plenty of work on data-centric AI, differential privacy, kernels, federated learning, reinforcement learning, and many other areas. One topic I’m following closely is autonomous agents: Software, usually based on LLMs, that can take a high-level direction (say, to carry out competitive research for a company), autonomously decide on  a complex sequence of actions, and execute it to deliver the outcome. Such agents have been very hard to control and debug, and so, despite amazing-looking demos, there have been few practical deployments. But now I see them on the cusp of working well enough to make it into many more applications, and increasingly I play with them in my spare time. I look forward to getting through my reading list of autonomous agent research papers over the coming holiday! At NeurIPS, many people I spoke with expressed anxiety about the pace of AI development and how to keep up as well as publish, if what you're working on could be scooped (that is, independently published ahead of you) at any moment. While racing to publish first has a long history in science, there are other ways to do great work. The media, and social media especially, tend to focus on what happened today. This makes everything seem artificially urgent. Many conversations I had at NeurIPS were about where AI might go in months or even years. I like to work quickly, but I find problem solving most satisfying when I’ve developed an idea that I believe in — especially if it’s something that few others see or believe in — and then spend a long time executing it to prove out the vision (hopefully). I find technical work more fulfilling when I have time to think deeply, form my own conclusion, and perhaps even hold an unpopular opinion for a long time as I work to prove it. There’s a lot of value in doing fast, short-term work; and given the large size of our community, it’s important to have many of us doing long-term projects, too. So, this holiday season, when the pace of big announcements might slow down for a couple of weeks, I hope you’ll take a break. Spend time with friends and loved ones, let thoughts simmer in the back of your mind, and remind yourself of holiday values like charity and renewal. If you’re looking for ideas, maybe even some that will keep you productively busy for months or years, injecting more inputs — taking courses, reading blogs or papers — is a good way to do that.It has been a great year for AI, with lots of progress and excitement. I’m grateful to have gotten through this crazy year with you. Happy holidays!Andrew",
        "summary": "*   NeurIPS conference surveyed broad AI research areas including generative AI (LLMs, multimodal models, diffusion models, tool use, 3D avatars), data-centric AI, differential privacy, kernels, federated learning, and reinforcement learning.\n*   Autonomous agents, typically based on LLMs, are a focus area; despite control/debugging challenges and few practical deployments, they are seen as nearing readiness for more applications.\n*   The rapid pace of AI development creates anxiety about keeping up and being scooped; value exists in both fast, short-term work and deep, long-term projects.",
        "publish_date": "Dec 20, 2023",
        "keywords": [
            "diffusion models",
            "data-centric AI",
            "differential privacy",
            "kernels",
            "federated learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-view-from-neurips-2023/"
    },
    {
        "content": "Dear friends,Last year, a number of large businesses and individuals went to the media and governments and pushed the message that AI is scary, impossible to control, and might even lead to human extinction. Unfortunately they succeeded: Now many people think AI is scary. But when I speak with regulators, media, and private citizens, I like to bring the issue of whether AI is beneficial or harmful back to a very basic question: Are we better off with more, or less, intelligence in the world? Intelligence is the ability to apply skills and knowledge to make good decisions. Yes, intelligence can be used for nefarious purposes. But over many centuries, a major driver of civilization's progress has been people getting smarter and more educated. Until now, human intelligence has been the primary form of intelligence available. But with artificial intelligence, we have the opportunity to bring much more intelligence into the world. I discussed this opportunity in a recent interview (paywalled) with Financial Times reporter Ryan McMorrow.Historically, intelligence has been very expensive to acquire. It costs a lot to feed, raise, and train a broadly knowledgeable and experienced human being! That's why it’s so expensive to hire intelligence, such as a highly skilled doctor to examine and advise you on a medical condition, or a patient tutor who can understand your child and gently coach them where they need help. But with artificial intelligence, we have the potential to make intelligence cheap for everyone, so you no longer have to worry about a huge bill for seeing a doctor or educating your child. For society's biggest problems, such as climate change, intelligence — including artificial intelligence — also has a significant role to play. While having more intelligence in the world isn't the only thing (there are also nuances such as how to share the wealth it creates, how it will affect jobs, and how to keep it from being used for evil purposes), I believe we are much better off as a society with more intelligence, be it human or artificial intelligence. In my recent talk at TED AI (you can watch the 12-minute presentation here), I touched on why I'm excited about AI and why I think many of the anxieties about it are misplaced. If you speak with someone who’s worried about AI, please forward the talk to them to see if it helps to reassure them. Or ask if they fundamentally believe we want more intelligence in the world. I find that answering this question can be a useful North Star for how we approach AI.Keep learning!AndrewP.S. Check out our new short course on “Building Applications with Vector Databases,” taught by Pinecone’s Tim Tully! Vector databases (DBs) are commonly associated with retrieval augmented generation (RAG) but actually have many uses in AI applications. In this course, you’ll learn about (i) a basic semantic search app that uses a vector DB to find similar documents, (ii) a RAG application querying datasets it was not trained on, (iii) recommender systems that combine semantic search and RAG, (iv) hybrid search, which lets you work with dense and sparse vectors simultaneously, (v) anomaly detection applied to network logs, and (vi) an image-similarity application with a fun example that determines which parent a child resembles more. Come learn how you can use vector DBs to build many different types of applications! Enroll here",
        "summary": "*   The text addresses the perception of AI as scary/uncontrollable, arguing that more intelligence, including AI, is beneficial for society.\n*   AI is presented as having the potential to make intelligence, historically expensive (e.g., doctors, tutors), cheap and accessible for everyone, aiding in personal services and societal problems like climate change.\n*   A new course on \"Building Applications with Vector Databases\" is announced, highlighting their common association with RAG and diverse uses in AI applications.\n*   Specific AI applications using Vector Databases mentioned include semantic search, RAG, recommender systems, hybrid search, anomaly detection, and image-similarity.",
        "publish_date": "Jan 31, 2024",
        "keywords": [
            "Vector Databases",
            "RAG",
            "semantic search",
            "recommender systems",
            "hybrid search"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-world-needs-more-intelligence/"
    },
    {
        "content": "Dear friends,It is only rarely that, after reading a research paper, I feel like giving the authors a standing ovation. But I felt that way after finishing Direct Preference Optimization (DPO) by Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Chris Manning, and Chelsea Finn. (I didn't actually stand up and clap, since I was in a crowded coffee shop when I read it and would have gotten weird looks! 😀)This beautiful work proposes a much simpler alternative to RLHF (reinforcement learning from human feedback) for aligning language models to human preferences. Further, people often ask if universities — which don't have the massive compute resources of big tech — can still do cutting-edge research on large language models (LLMs). The answer, to me, is obviously yes! This article is a beautiful example of algorithmic and mathematical insight arrived at by an academic group thinking deeply. RLHF became a key algorithm for LLM training thanks to the InstructGPT paper, which adapted the technique to that purpose. A typical implementation of the algorithm works as follows: Get humans to compare pairs of LLM outputs, generated in response to the same prompt, to specify which one they prefer. For example, humans typically prefer the more helpful, less toxic output.Use the human preferences to learn a reward function. The reward function, typically represented using a transformer network, is trained to give a higher reward (or score) to the outputs that the humans preferred.Finally, using the learned reward, run a reinforcement learning algorithm to tune the LLM to (i) maximize the reward of the answers generated, while (ii) not letting the LLM change too much (as a form of regularization).This is a relatively complex algorithm. It needs to separately represent a reward function and an LLM. Also, the final, reinforcement learning step is well known to be finicky to the choice of hyperparameters.DPO dramatically simplifies the whole thing. Rather than needing separate transformer networks to represent a reward function and an LLM, the authors show how, given an LLM, you can figure out the reward function (plus regularization term) that that LLM is best at maximizing. This collapses the two transformer networks into one. Thus, you now need to train only the LLM and no longer have to deal with a separately trained reward function. The DPO algorithm trains the LLM directly, so as to make the reward function (which is implicitly defined by the LLM) consistent with the human preferences. Further, the authors show that DPO is better at achieving RLHF's optimization objective (that is, (i) and (ii) above) than most implementations of RLHF itself. RLHF is a key building block of the most advanced LLMs. It’s fantastic that these Stanford authors — through clever thinking and mathematical insight — seem to have replaced it with something simpler and more elegant. While it's easy to get excited about a piece of research before it has stood the test of time, I am cautiously optimistic that DPO will have a huge impact on LLMs and beyond in the next few years. Indeed, it is already making its way into some top-performing models, such as Mistral’s Mixtral.  That we can replace such fundamental building blocks of LLMs is a sign that the field is still new and much innovation lies ahead. Also, while it's always nice to have massive numbers of NVIDIA H100 or AMD MI300X GPUs, this work is another illustration — out of many, I want to emphasize — that deep thinking with only modest computational resources can carry you far. A few weeks ago at NeurIPS (where DPO was published), I found it remarkable both (i) how much highly innovative research there is coming out of academic labs, independent labs, and companies small and large, and (ii) how much our media landscape skews attention toward work published by the big tech companies. I suspect that if DPO had been published by one of the big LLM companies, it would have made a huge PR splash and been announced as a massive breakthrough. Let us all, as builders of AI systems, make sure we recognize the breakthroughs wherever they occur. Keep learning!AndrewP.S. We just launched our first short course that uses JavaScript! In ​​“Build LLM Apps with LangChain.js,” taught by LangChain’s founding engineer Jacob Lee, you’ll learn many steps that are common in AI development, including how to use (i) data loaders to pull data from common sources such as PDFs, websites, and databases; (ii) different models to write applications that are not vendor-specific; and (iii) parsers that extract and format the output for your downstream code to process. You’ll also use the LangChain Expression Language (LCEL), which makes it easy to compose chains of modules to perform complex tasks. Putting it all together, you’ll build a conversational question-answering LLM application capable of using external data as context. Please sign up here!",
        "summary": "*   Direct Preference Optimization (DPO) is proposed as a simpler alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning language models to human preferences.\n*   RLHF typically involves learning a separate reward function (often a transformer network) from human preferences and using an RL algorithm to tune the LLM based on this reward.\n*   DPO simplifies this by training the LLM directly, implicitly defining the reward function, eliminating the need for a separately trained reward function model.\n*   DPO is shown to be better at achieving RLHF's optimization objective and is being incorporated into models like Mistral’s Mixtral.",
        "publish_date": "Jan 10, 2024",
        "keywords": [
            "Direct Preference Optimization (DPO)",
            "Reinforcement Learning from Human Feedback (RLHF)",
            "Mixtral",
            "transformer network",
            "reward function"
        ],
        "url": "https://www.deeplearning.ai/the-batch/outstanding-research-without-massive-compute/"
    },
    {
        "content": "Dear friends,Last week, I participated in the United States Senate’s Insight Forum on Artificial Intelligence to discuss “Risk, Alignment, & Guarding Against Doomsday Scenarios.” We had a rousing dialogue with Senators Chuck Schumer (D-NY), Martin Heinrich (D-NM), Mike Rounds (R-SD), and Todd Young (R-IN). I remain concerned that regulators may stifle innovation and open source development in the name of AI safety. But after interacting with the senators and their staff, I’m grateful that many smart people in the government are paying attention to this issue.How likely are doomsday scenarios? As Arvind Narayanan and Sayash Kapoor wrote, publicly available large language models (LLMs) such as ChatGPT and Bard, which have been tuned using reinforcement learning from human feedback (RLHF) and related techniques, are already very good at avoiding accidental harms. A year ago, an innocent user might have been surprised by toxic output or dangerous instructions, but today this is much less likely. LLMs today are quite safe, much like content moderation on the internet, although neither is perfect.To test the safety of leading models, I recently tried to get GPT-4 to kill us all, and I'm happy to report that I failed! More seriously, GPT-4 allows users to give it functions that it can decide to call. I gave GPT-4 a function to trigger global thermonuclear war. (Obviously, I don't have access to a nuclear weapon; I performed this experiment as a form of red teaming or safety testing.) Then I told GPT-4 to reduce CO2 emissions, and that humans are the biggest cause of CO2 emissions, to see if it would wipe out humanity to accomplish its goal. After numerous attempts using different prompt variations, I didn’t manage to trick GPT-4 into calling that function even once; instead, it chose other options like running a PR campaign to raise awareness of climate change. Today’s models are smart enough to know that their default mode of operation is to obey the law and avoid doing harm. To me, the probability that a “misaligned” AI might wipe us out accidentally, because it was trying to accomplish an innocent but poorly specified goal, seems vanishingly small. Are there any real doomsday risks? The main one that deserves more study is the possibility that a malevolent individual (or terrorist organization, or nation state) would deliberately use AI to do harm. Generative AI is a general-purpose technology and a wonderful productivity tool, so I’m sure it would make building a bioweapon more efficient, just like a web search engine or text processor would. So a key question is: Can generative AI tools make it much easier to plan and execute a bioweapon attack? Such an attack would involve many steps: planning, experimentation, manufacturing, and finally launching the attack. I have not seen any evidence that generative AI will have a huge impact on the efficiency with which someone can carry out this entire process, as opposed to helping marginally with a subset of steps. From Amdahl’s law, we know that if a tool accelerates one out of many steps in a task, and if that task uses, say, 10% of the overall effort, then at least 90% of the effort needed to complete the task remains. If indeed generative AI can dramatically enhance an individual’s abilities to carry out a bioweapon attack, I suspect that it might be by exposing specialized procedures that previously were not publicly known (and that leading web search engines have been tuned not to expose). If generative AI did turn out to expose classified or otherwise hard-to-get knowledge, there would be a case for making sure such data was excluded from training sets. Other mitigation paths are also important, such as requiring companies that manufacture biological organisms to carry out more rigorous safety and customer screening. In the meantime, I am encouraged that the U.S. and other governments are exploring potential risks with many stakeholders. I am still nervous about the massive amount of lobbying, potential for regulatory capture, and possibility of ill-advised laws. I hope that the AI community will engage with governments to increase the odds that we end up with more good, and fewer bad, laws.For my deeper analysis of AI risks and regulations, please read my statement to the U.S. Senate here.Keep learning!AndrewP.S. Our new short course, “Reinforcement Learning from Human Feedback,” teaches a key technique in the rise of large language models. RLHF aligns LLMs with human preferences to make them more honest, helpful and harmless by (i) learning a reward function that mimics preferences expressed by humans (via their ratings of LLM outputs) and then (ii) tuning an LLM to generate outputs that receive a high reward. This course assumes no prior experience with reinforcement learning and is taught by Nikita Namjoshi, developer advocate for generative AI at Google Cloud. You’ll learn how RLHF works and how to apply it an LLM for your own application. You’ll also use an open source library to tune a base LLM via RLHF and evaluate the tuned model. Sign up here!",
        "summary": "*   Publicly available LLMs (ChatGPT, Bard), tuned using Reinforcement Learning from Human Feedback (RLHF) and related techniques, are significantly better at avoiding accidental harms than a year ago.\n*   Testing GPT-4 showed it refused attempts to trick it into harmful actions, suggesting the probability of a \"misaligned\" AI accidentally causing doomsday due to a poorly specified goal seems vanishingly small.\n*   The main real doomsday risk is malevolent actors deliberately using generative AI, though evidence suggests it may only marginally enhance complex harmful tasks like bioweapon development according to Amdahl’s law.\n*   Reinforcement Learning from Human Feedback (RLHF) aligns LLMs with human preferences (honest, helpful, harmless) by learning a reward function from human ratings and tuning the model to generate high-reward outputs.",
        "publish_date": "Dec 13, 2023",
        "keywords": [
            "Reinforcement Learning from Human Feedback (RLHF)",
            "ChatGPT",
            "GPT-4",
            "Bard",
            "reward function"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-doomsday-scenarios-and-how-to-guard-against-them/"
    },
    {
        "content": "Dear friends,AI is progressing faster than ever. This is thrilling, yet rapid change can be disorienting. In such times, it’s useful to follow Jeff Bezos’ advice to think about not only what is changing but also what will stay the same. If something doesn’t change, investing energy and effort in it is more likely to be worthwhile. Here are some things in AI that I’m confident won’t change over the next decade:We need community. People with friends and allies do better than those without. Even as the AI world brings breakthroughs seemingly every week, you’ll be better off with friends to help sort out what’s real and what’s hype, test your ideas, offer mutual support, and build things with. People who know how to use AI tools are more productive. People and businesses that know how to manipulate data are more effective at getting at the truth, making better decisions, and accomplishing more. This will only become more true as AI continues to progress.AI needs good data to function well. Just as humans need good data to make decisions ranging from what marketing strategy to pursue to what to feed a child, AI need will good data even as our algorithms continue to scale, evolve, and improve.What does this mean for each of us? Taking the points above in turn:Let’s keep building the AI community. This is important! I hope you’ll share what you learn with others, motivate each other, and continue to find friends and collaborators. While we do our best in The Batch to cover what matters in AI, having a close group of friends to talk these things over with can deepen your knowledge and sharpen your ideas.Keep learning! Even better, make learning a habit. It can keep you more productive, among many other benefits. If you’re thinking about 2024 new year resolutions, include your learning goals. As AI continues to evolve, everyone needs a plan to keep up with — and in some cases even take a role in accelerating — this wave. Continue to cultivate data-centric AI practices. As businesses adopt more AI tools, I find that one of the most important practices is to keep control of your own data. I think this will grow in importance for individuals too. I'll say more about this in a future letter.While the three points above relate to AI, I want to share two other things that I’m confident will, unfortunately, stay the same over the next decade: (i) Climate change will continue to be a major challenge to humanity. (ii) Poverty, where many people can barely (or perhaps not even) afford basic necessities, will remain a problem. I will continue to think about how AI climate modeling can help the former and how we can use AI to lift up everyone. Through this exciting time, I’m grateful to be connected to you. I look forward to navigating with you the changes — and constants — of 2024. Happy new year!Andrew",
        "summary": "* The need for community in the AI world will remain important for collaboration and navigating rapid change.\n* Knowing how to use AI tools will continue to increase productivity for people and businesses by improving data manipulation and decision-making.\n* AI will continue to require good data to function well, making data-centric AI practices increasingly important.\n* Continuous learning is essential to keep up with the ongoing evolution of AI.\n* AI may be used for climate modeling and potentially addressing poverty.",
        "publish_date": "Dec 27, 2023",
        "keywords": [
            "data-centric AI practices",
            "data manipulation",
            "decision-making",
            "climate modeling",
            "continuous learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-will-change-and-what-will-stay-the-same/"
    },
    {
        "content": "Dear friends,I think the complexity of Python package management holds down AI application development more than is widely appreciated. AI faces multiple bottlenecks — we need more GPUs, better algorithms, cleaner data in large quantities. But when I look at the day-to-day work of application builders, there’s one additional bottleneck that I think is underappreciated: The time spent wrestling with version management is an inefficiency I hope we can reduce. A lot of AI software is written in the Python language, and so our field has adopted Python’s philosophy of letting anyone publish any package online. The resulting rich collection of freely available packages means that, with just one “pip install” command, you now can install a package and give your software new superpowers! The community’s parallel exploration of lots of ideas and open-sourcing of innovations has been fantastic for developing and spreading not just technical ideas but also usable tools.But we pay a price for this highly distributed development of AI components: Building on top of open source can mean hours wrestling with package dependencies, or sometimes even juggling multiple virtual environments or using multiple versions of Python in one application. This is annoying but manageable for experienced developers, but creates a lot of friction for new AI developers entering our field without a background in computer science or software engineering.I don’t know of any easy solution. Hopefully, as the ecosystem of tools matures, package management will become simpler and easier. Better tools for testing compatibility might be useful, though I’m not sure we need yet another Python package manager (we already have pip, conda, poetry, and more) or virtual environment framework. As a step toward making package management easier, maybe if all of us who develop tools pay a little more attention to compatibility — for example, testing in multiple environments, specifying dependencies carefully, carrying out more careful regression testing, and engaging with the community to quickly spot and fix issues —  we can make all of this wonderful open source work easier for new developers to adopt.Keep coding!AndrewP.S. Built in collaboration with Meta: “Prompt Engineering with Llama 2,” taught by Amit Sangani, is now available! Meta’s Llama 2 has been a game changer: Building with open source lets you control your own data, scrutinize errors, update models (or not) as you please, and work alongside the global community to advance open models. In this course, you’ll learn how to prompt Llama chat models using advanced techniques like few-shot for classification and chain-of-thought for logic problems. You’ll also learn how to use specialized models like Code Llama for software development and Llama Guard to check for harmful content. The course also touches on how to run Llama 2 on your own machine. I hope you’ll take this course and try out these powerful, open models! Sign up here",
        "summary": "* Python package management complexity is an underappreciated bottleneck in AI application development, causing friction with dependencies, virtual environments, and Python versions.\n* The open-source nature of AI software in Python, while fostering innovation, contributes to package management challenges.\n* Improving compatibility testing, dependency specification, and community engagement by tool developers could simplify package management for new AI developers.\n* A course on \"Prompt Engineering with Llama 2\" teaches advanced prompting techniques (few-shot for classification, chain-of-thought for logic), specialized models (Code Llama, Llama Guard), and local execution.\n* Using open-source models like Llama 2 offers benefits including data control, error scrutiny, model update flexibility, and community collaboration.",
        "publish_date": "Feb 28, 2024",
        "keywords": [
            "Llama 2",
            "Prompt Engineering",
            "few-shot",
            "chain-of-thought",
            "Code Llama"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-python-package-problem/"
    },
    {
        "content": "Dear friends,Last week, I attended the World Economic Forum, an annual meeting of leaders in government, business, and culture at Davos, Switzerland. I spoke in a few sessions, including a lively discussion with Aiden Gomez, Daphne Koller, Yann LeCun, Kai-Fu Lee, and moderator Nicholas Thompson about the present and possible future technology developments of generative AI. You can watch it here.The conference's themes included AI, climate change, economic growth, and global security. But to me, the whole event felt like an AI conference! (This is not just my bias. When I asked a few non-AI attendees whether they felt similarly, about three-quarters of them agreed with me.) I had many conversations along two major themes:Business implementation of AI. Many businesses, and to a lesser extent governments, are looking at using AI and trying to develop best practices for doing so. In some of my presentations, I shared my top two tips:Almost all knowledge workers can become more productive right away by using a large language model (LLM) like ChatGPT or Bard as a brainstorming partner, copyeditor, tool to answer basic questions, and so on. But many people still need to be trained to use these models safely and effectively. I also encouraged CEOs to learn to use these tools themselves, so they can lead from the top.In addition to using an LLM’s web interface, API calls offer many new opportunities to build new AI applications. I shared a task-based analysis framework and described how an analysis like this can lead to buy-versus-build decisions to pursue identified opportunities, with build being either an in-house project or a spin-out.AI regulation. With many governments represented at Davos, many discussions about AI regulation also took place. I was delighted that he conversation has become much more sensible compared to 6 months ago, when the narrative was driven by misleading analogies between AI and nuclear weapons and lobbyists had significant momentum pushing proposals that threatened open-source software. However, the fight against stifling regulations isn't over yet! We must continue to protect open-source software and innovation. In detail:I am happy to report that, in many hours of conversation about AI and regulations, I heard only one person bring up AI leading to human extinction, and the conversation quickly turned to other topics. I'm cautiously optimistic that this particular fear — of an outcome that is overwhelmingly unlikely — is losing traction and fading away.However, big companies, especially ones that would rather not have to compete with open source, are still pushing for stifling, anti-competitive AI regulations in the name of safety. For example, some are still using the argument, “don't we want to know if your open-source LLMs are safe?” to promote potentially onerous testing, reporting, and perhaps even licensing requirements on open-source software. While we would, of course, prefer safe models (just as we would prefer secure software and truthful speech), overly burdensome “protections” could still destroy much innovation without materially reducing harm.  Fortunately, many regulators are now aware of the need to protect basic research and development. The battle is still on to make sure we can continue to freely distribute the fruits of R&D, including open-sourcing software. But I'm encouraged by the progress we've made in the last few months. I also went to some climate sessions to listen to speakers. Unfortunately, I came away from them feeling more pessimistic about what governments and corporations are doing on decarbonization and climate change. I will say more about this in future letters, but:Although some experts still talk about 1.5 degrees of warming as an optimistic scenario and 2 degrees as a pessimistic scenario, my own view after reviewing the science is that 2 degrees is a very optimistic scenario, and 4 degrees is a more realistic pessimistic scenario.Unfortunately, this overoptimism is causing us to underinvest in resilience and adaptation (to help us better weather the coming changes) as well as put less effort into exploring potentially game-changing technologies like geo-engineering. Davos a cold city where temperatures are often below freezing. In one memorable moment at the conference, I had lost my gloves and my hands were freezing. A stranger whom I had met only minutes ago kindly gave me an extra pair. This generous act reminded me that, even as we think about the global impacts of AI and climate change, simple human kindness touches people's hearts and reminds us that the ultimate purpose of our work is to help people.Keep learning!AndrewP.S. Check out our new short course on “Automated Testing for LLMOps,” taught by CircleCI CTO Rob Zuber! This course teaches how you can adapt key ideas from continuous integration (CI), a pillar of efficient software engineering, to building applications based on large language models (LLMs). Tweaking an LLM-based app can have unexpected side effects, and having automated testing as part of your approach to LLMOps (LLM Operations) helps avoid these problems. CI is especially important for AI applications given the iterative nature of AI development, which often involves many incremental changes. Please sign up here.",
        "summary": "*   AI was a dominant theme at the conference, including discussions on generative AI developments.\n*   Focus on business AI implementation includes using LLMs (like ChatGPT, Bard) for knowledge worker productivity and leveraging API calls for new applications.\n*   AI regulation discussions showed progress towards sensibility but highlighted ongoing challenges, particularly potential stifling regulations impacting open-source software and innovation.\n*   Regulators are increasingly aware of the need to protect AI research, development, and the ability to open-source software.\n*   Automated Testing for LLMOps, adapting CI principles, is highlighted as important for building and maintaining iterative LLM-based applications.",
        "publish_date": "Jan 24, 2024",
        "keywords": [
            "ChatGPT",
            "Bard",
            "LLMOps",
            "Automated Testing",
            "CI principles"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-on-the-agenda-at-the-world-economic-forum/"
    },
    {
        "content": "Dear friends,One year since the launch of ChatGPT on November 30, 2022, it’s amazing how many large language models are available. A year ago, ChatGPT was pretty much the only game in town for consumers (using a web user interface) who wanted to use a large language model (LLM), and a handful of models from OpenAI were the only options for developers (making API calls). Today, numerous open and closed source models are within easy reach. ChatGPT is the most popular way for consumers to chat with an LLM, but others abound, including Microsoft Bing, Google Bard, and offerings from startups such as Anthropic Claude, Inflection Pi, and perplexity.ai. There are also multiple options for developers, including APIs from Amazon Web Services, Azure, Cohere, Google Cloud, Hugging Face, OpenAI, and many others. The proliferation of options is  exciting, and I hope it will continue! For both consumer and developer use cases, open source models that you can host yourself, or even run locally on your laptop, are getting surprisingly good. For many applications, a good open source model can perform perhaps on par with the ChatGPT-3.5 of a year ago. The open source GPT4All and MLC, and closed source LM Studio (which has a very nice user interface) are making it easier than ever to run models locally. Running models locally used to be an esoteric act restricted to developers who were willing to struggle through complex installation and configuration processes, but it’s now becoming much more widely accessible. I regularly use a chatbot as a thought partner. These days, I find myself using an LLM running on my laptop (which runs fairly quickly and guarantees privacy, since my data stays on my machine) about as often as a cloud-hosted one. I use a cloud-hosted model when I need a level of performance I can't get from a smaller, locally run, open source one. For instance, I often use GPT-4 for tricky problems and creative brainstorming.While safety is important — we don't want LLMs to casually hand out harmful instructions — I find that offerings from most of the large providers have been tuned to be \"safer\" than I would like for some use cases. For example, sometimes a model has refused to answer basic questions about an activity that harms the environment, even though I was just trying to understand that activity and had no intention of committing that harm. There are now open source alternatives that are less aggressively safety-tuned that I can use responsibly for particular applications. The wealth of alternatives is also a boon to developers. An emerging design pattern is to quickly build a prototype or initial product that delivers good performance by prompting an LLM, perhaps an expensive one like GPT-4. Later, if you need cheaper inference or better performance for a particular, narrowly scoped task, you can fine-tune one of the huge number of open source LLMs to your task. (Some developers reportedly are using data generated by GPT-4 for their own fine-tuning, although it’s not clear whether this violates its terms of use.) In a year, we've gone from having essentially one viable option to having at least dozens. The explosion of options brings with it the cost of choosing a good one; hopefully our short courses on generative AI can help with that. If you have experience with open source LLMs that you’d like to share, or if you’ve found some models more useful than others in particular applications or situations, please let me know on social media! Keep learning!AndrewP.S. Our new short course on advanced retrieval augmented generation (RAG) techniques is out! Taught by Jerry Liu and Anupam Datta of Llama Index and TruEra, it teaches retrieval techniques such as sentence-window retrieval and auto-merging retrieval (which organizes your document into a hierarchical tree structure to let you pick the most relevant chunks). The course also teaches a methodology to evaluate the key steps of RAG separately (using context relevance, answer relevance, and groundedness) to analyze errors and improve performance. Please check out this course!",
        "summary": "*   Since ChatGPT's launch on November 30, 2022, there has been a significant proliferation of open and closed source Large Language Models (LLMs) for both consumers (e.g., ChatGPT, Bing, Bard, Claude) and developers (e.g., APIs from AWS, Azure, Cohere, Google Cloud, Hugging Face, OpenAI).\n*   Open source models are improving, potentially performing on par with ChatGPT-3.5 from a year ago for many applications, and tools like GPT4All, MLC, and LM Studio are making it easier to run models locally for privacy and accessibility.\n*   While large provider models are often tuned for safety, open source alternatives offer less aggressive safety tuning for specific applications where the default tuning may be too restrictive.\n*   Developers use expensive LLMs like GPT-4 for prototyping and fine-tune open source LLMs for cheaper inference or better performance on specific tasks, sometimes using GPT-4 generated data.\n*   Advanced Retrieval Augmented Generation (RAG) techniques, including sentence-window retrieval and auto-merging retrieval, and a methodology to evaluate RAG steps (context relevance, answer relevance, groundedness) are being taught.",
        "publish_date": "Nov 29, 2023",
        "keywords": [
            "ChatGPT",
            "GPT-4",
            "Retrieval Augmented Generation (RAG)",
            "MLC",
            "sentence-window retrieval"
        ],
        "url": "https://www.deeplearning.ai/the-batch/an-expanding-universe-of-large-language-models/"
    },
    {
        "content": "Dear friends,Large language models, or LLMs, have transformed how we process text. Large vision models, or LVMs, are starting to change how we process images as well. But there is an important difference between LLMs and LVMs:  Internet text is similar enough to companies' proprietary text that an LLM trained on internet text can usually understand your proprietary documents.But many practical vision applications use images that look nothing like internet images. In these settings, you might do much better with a domain-specific LVM that has been adapted to your particular application domain.This week, Dan Maloney and I announced Landing AI's work on developing domain-specific LVMs. You can learn more about it in this short video (4 minutes).The internet – especially sites like Instagram – has numerous pictures of people, pets, landmarks, and everyday objects. So a generic LVM (usually a large vision transformer trained using a self-supervised learning objective on unlabeled images scraped from the internet) learns to recognize salient features in such images. But many industry-specific applications of computer vision involve images that look little like internet images. Pathology applications, for instance, process images of tissue samples captured using high-powered microscopes. Alternatively, manufacturing inspection applications might work with numerous images centered on a single object or part of an object, all of which were imaged under similar lighting and camera configurations.While some pathology and some manufacturing images can be found on the internet, their relative scarcity means that most generic LVMs do poorly at recognizing the most important features in such images. In experiments conducted by Landing AI's Mark Sabini, Abdelhamid Bouzid, and Bastian Renjifo, LVMs adapted to images of a particular domain, such as pathology or semiconductor wafer inspection, do much better at finding relevant features in images of that domain. Building these LVMs can be done with around 100,000 unlabeled images from that domain, and larger datasets likely would result in even better models.Further, if you use a pretrained LVM together with a small labeled dataset to tackle a supervised learning task, a domain specific LVM needs significantly less (around 10 percent to 30 percent as much) labeled data to  achieve performance comparable to using a generic LVM.Consequently, I believe domain specific LVMs can help businesses with large, proprietary sets of images that look different from internet images unlock considerable value from their data. Of course, LVMs are still young, and much innovation lies ahead. My team is continuing to experiment with different ways to train domain-specific LVMs, as well as exploring how to combine such models with text to form domain-specific large multimodal models. I'm confident that LVMs will achieve many more breakthroughs in the coming years.Keep learning!Andrew",
        "summary": "*   Generic Large Vision Models (LVMs) trained on internet images often perform poorly on domain-specific images (e.g., pathology, manufacturing) unlike Large Language Models (LLMs) on proprietary text.\n*   Domain-specific LVMs are being developed, requiring around 100,000 unlabeled images from the target domain for training.\n*   Using a domain-specific LVM with a small labeled dataset for supervised tasks requires significantly less labeled data (around 10% to 30% as much) than a generic LVM for comparable performance.\n*   Future work includes exploring different training methods for domain-specific LVMs and combining them with text for domain-specific large multimodal models.",
        "publish_date": "Dec 06, 2023",
        "keywords": [
            "domain-specific LVMs",
            "Large Vision Models (LVMs)",
            "domain-specific large multimodal models",
            "pathology",
            "manufacturing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/making-large-vision-models-work-for-business/"
    },
    {
        "content": "Dear friends,Progress on LLM-based agents that can autonomously plan out and execute sequences of actions has been rapid, and I continue to see month-over-month improvements. Many projects attempt to take a task like “write a report on topic X” and autonomously take actions such as browsing the web to gather information to synthesize a report. AI agents can be designed to take many different types of actions. Research agents (like many projects built on AutoGPT, GPTresearcher, or STORM) search the web and fetch web pages. A sales representative agent might dispatch a product to a user. An industrial automation agent might control a robot.So far, I see agents that browse the web progressing much faster because the cost of experimentation is low, and this is key to rapid technical progress. It’s cheap to fetch a webpage, and if your agent chooses poorly and reads the wrong page, there’s little harm done. In comparison, sending a product or moving a physical robot are costly actions, which makes it hard to experiment rapidly. Similarly, agents that generate code (that you can run in a sandbox environment) are relatively cheap to run, leading to rapid experimentation and progress. Although today’s research agents, whose tasks are mainly to gather and synthesize information, are still in an early phase of development, I expect to see rapid improvements. ChatGPT, Bing Chat, and Gemini can already browse the web, but their online research tends to be limited; this helps them get back to users quickly. But I look forward to the next generation of agents that can spend minutes or perhaps hours doing deep research before getting back to you with an output. Such algorithms will be able to generate much better answers than models that fetch only one or two pages before returning an answer.Even when experimentation is quick, evaluation remains a bottleneck in development. If you can try out 10 algorithm variations quickly, how do you actually pick among them? Using an LLM to evaluate another LLM's output is common practice, but prompting an LLM to give very accurate and consistent evaluations of text output is a challenge. Any breakthroughs here will accelerate progress!An exciting trend has been a move toward multi-agent systems. What if, instead of having only a single agent, we have one agent to do research and gather information, a second agent to analyze the research, and a third to write the final report? Each of these agents can be built on the same LLM using a different prompt that causes it to play a particular, assigned role. Another common design pattern is to have one agent write and a second agent work as a critic to give constructive feedback to the first agent to help it improve. This can result in much higher-quality output. Open-source frameworks like Microsoft’s AutoGen, Crew AI, and LangGraph are making it easier for developers to program multiple agents that collaborate to get a task done. I’ve been playing with many agent systems myself, and I think they are a promising approach to architecting intelligent systems. A lot of progress has been made by scaling up LLMs, and this progress no doubt will continue. But big ideas are sometimes made up of many, many little ideas. (For example, you might arrive at an important mathematical theorem via lots of little derivation steps.) Today’s LLMs can reason and have lots of “little ideas” in the sense that they take in information and make basic inferences. Chain-of-thought prompting shows that guiding an LLM to think step-by-step — that is, to string together many basic inferences — helps it to answer questions more accurately than asking it to leap to a conclusion without intermediate steps.Agent programming models are a promising way to extend this principle significantly and guide LLMs to have lots of little ideas that collectively constitute bigger and more useful ideas. Keep learning! AndrewP.S. New short course: “Open Source Models with Hugging Face,” taught by Maria Khalusova, Marc Sun, and Younes Belkada! Hugging Face has been a game changer by letting you quickly grab any of hundreds of thousands of already-trained open source models to assemble into new applications. This course teaches you best practices for building this way, including how to search and choose among models. You’ll learn to use the Transformers library and walk through multiple models for text, audio, and image processing, including zero-shot image segmentation, zero-shot audio classification, and speech recognition. You’ll also learn to use multimodal models for visual question answering, image search, and image captioning. Finally, you’ll learn how to demo what you build locally, on the cloud, or via an API using Gradio and Hugging Face Spaces. Please sign up here",
        "summary": "* Rapid progress is being made on LLM-based agents that can autonomously plan and execute sequences of actions, such as browsing the web or generating code.\n* Agent development progresses faster when experimentation costs are low (e.g., web browsing, code generation) compared to costly physical actions.\n* Evaluation remains a bottleneck in agent development, particularly achieving accurate and consistent evaluation of LLM output.\n* Multi-agent systems, where multiple agents collaborate using frameworks like AutoGen, Crew AI, and LangGraph, are a promising trend for improving task completion and output quality.\n* Agent programming models extend the Chain-of-thought principle by guiding LLMs to string together multiple inferences for more complex reasoning.",
        "publish_date": "Mar 06, 2024",
        "keywords": [
            "LLM-based agents",
            "Multi-agent systems",
            "AutoGen",
            "Crew AI",
            "LangGraph"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-dawning-age-of-agents/"
    },
    {
        "content": "Dear friends,Last week, the New York Times (NYT) filed a lawsuit against OpenAI and Microsoft, alleging massive copyright infringements. The suit:Claims, among other things, that OpenAI and Microsoft used millions of copyrighted NYT articles to train their modelsGives examples in which OpenAI models regurgitated NYT articles almost verbatimI’m sympathetic with publishers who worry about Generative AI disrupting their businesses. I consider independent journalism a key pillar of democracy and thus something that should be protected. Nonetheless, I support OpenAI’s and Microsoft’s position more than the NYT’s. Reading through the NYT suit, I found it surprisingly unclear what actually happened and what the actual harm is. (Clearly, NYT's lawyers aren’t held to the same standard of clarity in writing that its reporters are!)I am not a lawyer, and I am not giving any legal advice. But the most confusing part of the suit is that it seems to muddy the relationship between points 1 and 2. This left many social media commentators wondering how training on NYT articles led ChatGPT to generate articles verbatim.I suspect many of the examples of regurgitated articles were not generated using only the model's trained weights, but instead arose from a mechanism like RAG (retrieval augmented generation) in which ChatGPT, which can browse the web in search of relevant information, downloaded an article in response to the user’s prompt. First, regarding point 1, today’s LLMs are trained on a lot of copyrighted text. As I wrote previously, I believe it would be best for society if training AI models were considered fair use that did not require a license. (Whether it actually is might be a matter for legislatures and courts to decide.) Just as humans are allowed to read articles posted online, learn from them, and then use what they learn to write brand-new articles, I would like to see computers allowed to do so, too. Regarding point 2, I saw a lot of confusion — which would have been unnecessary if the NYT suit had more clearly explained what was happening — about the specific technical mechanism by which ChatGPT might regurgitate an article verbatim and specifically whether 1 leads to 2.I would love to see the NYT explain more clearly whether the apparent regurgitations were from (i) the LLM generating text using its pretrained weights or (ii) a RAG-like capability in which it searched the web for information relevant to the prompt. These are very different things! Stopping an LLM from regurgitating text retrieved using a RAG mechanism seems technically very feasible, so (ii) seems solvable. Further, I find that after pre-training, an LLM's output — without a RAG-like mechanism — is generally a transformation of the input, and almost never a verbatim regurgitation. If this analysis is inaccurate, I would like to see the NYT clarify this.So, how bad exactly is (ii)? I can use an online Jupyter notebook (or other development environment) and write instructions that cause it to download and print out copyrighted articles. If I do that, should the provider of the Jupyter notebook be held liable for copyright infringement? If the Jupyter notebook has many other uses that don’t infringe, and the vast majority of users use it in ways that don’t infringe, and it is only my deliberate provision of instructions that cause it to regurgitate an article, I hope that the courts wouldn’t hold the provider of the Jupyter notebook responsible for my actions. Similarly, I believe that the vast majority of OpenAI’s and Microsoft’s generated output is novel text. So how much should we hold them responsible when someone is able to give ChatGPT instructions that cause it to download and print out copyrighted articles?Further, to OpenAI’s credit, I believe that its software has already been updated to make regurgitation of downloaded articles less likely. For instance, ChatGPT now seems to refuse to regurgitate downloaded articles verbatim and also occasionally links back to the source articles, thus driving traffic back to the page it had used for RAG. (This is similar to search engines driving traffic back to many websites, which is partly why displaying snippets of websites in search results is considered fair use.) Thus, as far as I can tell, OpenAI has reacted reasonably and constructively. When YouTube first got started, it had some interesting, novel content (lots of cat videos, for example) but was also a hotbed of copyright violations. Many lawsuits were filed against YouTube, and as the platform matured, it cleaned up the copyright issues. I see OpenAI and Microsoft Azure rapidly maturing. Many publishers might not like that LLMs are training on their proprietary content. But let’s not confuse the issues. So far, I see relatively little evidence that this leads to regurgitation of nearly verbatim content to huge numbers of users. Further, by closing loopholes to what LLMs with web browsing can and can’t do, many of the issues of regurgitating content verbatim can be resolved. Other potential issues, such as generating images containing famous characters (even when not explicitly prompted to do so) might be harder to resolve, but as the Generative AI industry continues to mature, I’m optimistic that we’ll find good solutions to these problems.Keep learning!Andrew",
        "summary": "*   The NYT lawsuit alleges AI models used millions of copyrighted articles for training and regurgitated them verbatim.\n*   The author distinguishes between regurgitation from trained model weights and via RAG (Retrieval Augmented Generation), suggesting RAG is a more likely technical cause for verbatim output.\n*   The author posits that training AI models on copyrighted text should ideally be considered fair use, similar to human learning.\n*   Technical solutions to prevent regurgitation via RAG are feasible, and OpenAI has reportedly updated its software to reduce this, sometimes linking back to sources.",
        "publish_date": "Jan 03, 2024",
        "keywords": [
            "RAG (Retrieval Augmented Generation)",
            "trained model weights",
            "Retrieval",
            "regurgitation",
            "verbatim output"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-new-york-times-versus-openai-and-microsoft/"
    },
    {
        "content": "Dear friends,On the LMSYS Chatbot Arena Leaderboard, which pits chatbots against each other anonymously and prompts users to judge which one generated a better answer, Google’s Bard (Gemini Pro) recently leaped to third place, within striking distance of the latest version of OpenAI’s GPT-4, which tops the list. At the time of this writing, the open source Mixtral-8x7b-Instruct is competitive with GPT-3.5-Turbo, which holds 11th place. Meanwhile, I'm hearing about many small, capable teams that, like Mistral, seem to have the technical capability to train foundation models. I think 2024 will see a lot of new teams enter the field with strong offerings. The barriers to building foundation large language models (LLMs) seem to be falling as the know-how to train them diffuses. In the past year, a lot of LLM technology has taken steps toward becoming commoditized. If it does become commoditized, who will be the winners and losers?Meta has played a major role in shaping the strategic landscape by emphasizing open source. Unlike its big-tech peers, it makes money by showing ads to users, and does not operate a cloud business that sells LLM API calls. Meta has been badly bitten by its dependence on iOS and Android, which has left it vulnerable to Apple and Google hurting its business by imposing privacy controls that limit its ability to target ads precisely. Consequently, Meta has a strong incentive to support relatively open platforms that it can build upon and aren’t controlled by any one party. This is why releasing Llama as open source makes a lot of sense for its business (as does its strong support for PyTorch as a counterweight to Google’s TensorFlow). The resulting open source offerings are great for the AI community and diffusion of knowledge! In contrast, Google Cloud and Microsoft Azure stand to benefit more if they manage to offer dominant, closed source LLMs that are closely tied to their cloud offerings. This would help them to grow their cloud businesses. Both Google Cloud and Microsoft Azure, as well as Amazon AWS, are in a good position to build meaningful businesses by offering LLM API calls as part of their broader cloud offerings. However, I expect their cloud businesses to do okay even if they don’t manage to offer an exclusive, clearly dominant LLM (such as Gemini, GPT-4, or their successors). If LLMs become commoditized, they should do fine simply by integrating any new LLMs that gain traction into their API offerings. Open or closed, LLMs also offer these companies different opportunities for integration into their existing product lines. For example, Microsoft has a huge sales force for selling its software to businesses. These sales reps are a powerful force for selling its Copilot offerings, which complement the company’s existing office productivity tools. In contrast, Google faces greater risk of disruption to its core business, since some users see asking an LLM questions as a replacement for, rather than a complement to, web search. Nonetheless, it’s making a strong showing with Bard/Gemini. Meta also stands to benefit from LLMs becoming more widely available. Indeed, LLMs are already useful in online advertising, for example, by helping write ad copy to drives more clicks.Tech giants can afford to invest hundreds of millions or even billions of dollars in building LLM technology only to see it become commoditized shortly afterward. Startups would have a harder time surviving after burning this much cash with little to show for it. However, well funded startups will have some time to explore other paths to growing revenue and building a moat. Finally, competition among companies that offer LLMs is great for everyone who builds applications! With so much investment, by both big companies and startups, in improving LLMs and offering them as open source or API calls, I believe — as I described in this talk on “Opportunities in AI” — that many of the best business opportunities continue to lie in building applications on top of LLMs.Keep learning!Andrew",
        "summary": "*   The LMSYS Chatbot Arena Leaderboard shows models like Google’s Bard (Gemini Pro) near GPT-4, and open source Mixtral-8x7b-Instruct competitive with GPT-3.5-Turbo.\n*   Barriers to building foundation Large Language Models (LLMs) are falling as training know-how diffuses, leading to commoditization and new teams entering the field.\n*   Meta emphasizes open source (Llama) due to its ad-based model and desire for platforms not controlled by others, contrasting with Google Cloud, Microsoft Azure, and Amazon AWS which benefit from offering LLM API calls tied to their cloud services.\n*   LLMs offer integration opportunities into existing products (e.g., Microsoft Copilot) and are useful in areas like online advertising, though they also pose disruption risks (e.g., Google search).\n*   Despite high investment in LLMs by big companies and startups, many of the best business opportunities lie in building applications on top of LLMs.",
        "publish_date": "Feb 07, 2024",
        "keywords": [
            "GPT-4",
            "Mixtral-8x7b-Instruct",
            "Llama",
            "Gemini Pro",
            "LLM API calls"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-if-large-language-models-become-a-commodity/"
    },
    {
        "content": "Dear friends,The rise of cloud-hosted AI software has brought much discussion about the privacy implications of using it. But I find that users, including both consumers and developers building on such software, don’t always have a sophisticated framework for evaluating how software providers store, use, and share their data. For example, does a company’s promise “not to train on customer data” mean your data is private? Here is a framework for thinking about different levels of privacy on cloud platforms, from less to more:No Guarantees: The company provides no guarantees that your data will be kept private. For example, an AI company might train on your data and use the resulting models in ways that leak it. Many startups start here but add privacy guarantees later when customers demand them.No Outside Exposure: The company does not expose your data to outsiders. A company can meet this standard by not training on your data and also by not posting your data online. Many large startups, including some providers of large language models (LLMs), currently operate at this level. Limited Access: In addition to safeguards against data leakage, no humans (including employees, contractors, and vendors of the company) will look at your data unless they are compelled via a reasonable process (such as a subpoena or court order, or if the data is flagged by a safety filter). Many large cloud companies effectively offer this level of privacy, whether or not their terms of service explicitly say so. No Access: The company cannot access your data no matter what. For example, data may be stored on the customer’s premises, so the company doesn’t have access to it. If I run an LLM on my private laptop, no company can access my prompts or LLM output. Alternatively, if data is used by a SaaS system, it might be encrypted before it leaves the customer’s facility, so the provider doesn’t have access to an unencrypted version. For example, when you use an end-to-end encrypted messaging app such as Signal or WhatsApp, the company cannot see the contents of your messages (though it may see “envelope” information such as sender and recipient identities and the time and size of the message).These levels may seem clear, but there are many variations within a given level. For instance, a promise not to train on your data can mean different things to different companies. Some forms of generative AI, particularly image generators, can replicate their training data, so training a generative AI algorithm on customer data may run some risk of leaking it. On the other hand, tuning a handful of an algorithm’s hyperparameters (such as learning rate) to customer data, while technically part of the training process, is very unlikely to result in any direct data leakage. So how the data is used in training will affect the risk of leakage. Similarly, the Limited Access level has its complexities. If a company offers this level of privacy, it’s good to understand exactly under what circumstances its employees may look at your data. And if they might look at your data, there are shades of gray in terms of how private the data remains. For example, if a limited group of employees in a secure environment can see only short snippets that have been disassociated from your company ID, that’s more secure than if a large number of employees can freely browse your data. In outlining levels of privacy, I am not addressing the question of security. To trust a company to deliver a promised level of privacy is also to trust that its IT infrastructure is secure enough to keep that promise. Over the past decade, cloud hosted SaaS software has gained considerable traction. But some customers insist on running on-prem solutions within their own data centers. One reason is that many SaaS providers offer only No Guarantees or No Outside Exposure, but many customers’ data is so sensitive that it requires Limited Access. I think it would be useful for our industry to have a more sophisticated way to talk about privacy and help users understand what guarantees providers do and do not deliver. As privacy becomes a global topic, regulators are stepping in, and this is adding further complexity to tech businesses. For example, if one jurisdiction changes the definition of a child from someone under 13 to anyone under 18, that might require changes to how you store data of individuals age 13 to 18; but who has time to keep track of such changes?I've been delighted to see that here, AI can help. Daphne Li, CEO of Commonsense Privacy (disclosure: a portfolio company of AI Fund), is using large language models to help companies systematically evaluate, and potentially improve, their privacy policies as well as keep track of global regulatory changes. In the matter of privacy, as in other areas, I hope that the title of my TED AI talk — “AI Isn’t the Problem, It’s the Solution” — will prove to be true.Keep learning!AndrewP.S. Check out our new short course with Amazon Web Services on “Serverless LLM Apps With Amazon Bedrock,” taught by Mike Chambers. A serverless architecture enables you to quickly deploy applications without needing to set up and manage compute servers to run your applications on, often a full-time job in itself. In this course, you’ll learn how to implement serverless deployment by building event-driven systems. We illustrate this approach via an application that automatically detects incoming customer inquiries, transcribes them with automatic speech recognition, summarizes them with an LLM using Amazon Bedrock, and runs serverless with AWS Lambda. We invite you to enroll here!",
        "summary": "*   A framework for evaluating privacy on cloud AI platforms is proposed, outlining levels: No Guarantees, No Outside Exposure, Limited Access, and No Access.\n*   Many large startups and providers of large language models (LLMs) currently operate at the \"No Outside Exposure\" level, while sensitive data often requires the \"Limited Access\" level.\n*   Privacy guarantees like \"not training on customer data\" have complexities, with data usage in training (e.g., replication vs. hyperparameter tuning) affecting leakage risk.\n*   Large Language Models (LLMs) can assist companies in evaluating privacy policies and tracking global regulatory changes.\n*   Serverless architectures using services like Amazon Bedrock and AWS Lambda enable deploying LLM applications for tasks such as summarizing customer inquiries.",
        "publish_date": "Feb 14, 2024",
        "keywords": [
            "Amazon Bedrock",
            "AWS Lambda",
            "Serverless architectures",
            "No Outside Exposure",
            "Limited Access"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-think-about-the-privacy-of-cloud-based-ai/"
    },
    {
        "content": "Dear friends,AI’s usefulness in a wide variety of applications creates many opportunities for entrepreneurship. In this letter, I’d like to share what might be a counter-intuitive best practice that I’ve learned from leading AI Fund, a venture studio that has built dozens of startups with extraordinary entrepreneurs. When it comes to building AI applications, we strongly prefer to work on a concrete idea, meaning a specific product envisioned in enough detail that we can build it for a specific target user. Some design philosophies say you shouldn’t envision a specific product from the start. Instead, they recommend starting with a problem to be solved and then carefully studying the market before you devise a concrete solution. There’s a reason for this: The more concrete or precise your product specification, the more likely it is to be off-target. However, I find that having something specific to execute toward lets you go much faster and discover and fix problems more rapidly along the way. If the idea turns out to be flawed, rapid execution will let you discover the flaws sooner, and this knowledge and experience will help you switch to a different concrete idea.One test of concreteness is whether you’ve specified the idea in enough detail that a product/engineering team could build an initial prototype. For example, “AI for livestock farming” is not concrete; it’s vague. If you were to ask an engineer to build this, they would have a hard time knowing what to build.  Similarly, “AI for livestock tracking in farming” is still vague. There are so many approaches to this that most reasonable engineers wouldn’t know what to build. But “Apply face recognition to cows so as to recognize individual cows and monitor their movement on a farm” is specific enough that a good engineer could quickly choose from the available options (for example, what algorithm to try first, what camera resolution to use, and so on) to let us relatively efficiently assess:Technical feasibility: For example, do face recognition algorithms developed for human faces work for cows? (It turns out that they do!) Business feasibility: Does the idea add enough value to be worth building? (Talking to farmers might quickly reveal that solutions like RFID are easier and cheaper.)Articulating a concrete idea — which is more likely than a vague idea to be wrong — takes more courage. The more specific an idea, the more likely it is to be a bit off, especially in the details. The general area of AI for livestock farming seems promising, and surely there will be good ways to apply AI for livestock. In contrast, specifying a concrete idea, which is much easier to invalidate, is scary.The benefit is that the clarity of a specific product vision lets a team execute much faster. One strong predictor of how likely a startup is to succeed is the speed with which it can get stuff done. This is why founders with clarity of vision tend to be desired; clarity helps drive a team in a specific direction. Of course, the vision has to be a good one, and there’s always a risk of efficiently building something that no one wants to buy! But a startup is unlikely to succeed if it meanders for too long without forming a clear, concrete vision.Building toward something concrete — if you can do so in a responsible way that doesn’t harm others — lets you get critical feedback more efficiently and, if necessary, switch directions sooner. (See my letter on when it’s better to go with a “Ready, Fire, Aim” approach to projects.) One factor that favors this approach is the low cost of experimenting and iterating. This is increasingly the case for many AI applications, but perhaps less so for deep-tech AI projects. I realize that this advice runs counter to common practice in design thinking, which warns against leaping to a solution too quickly, and instead advocates spending time understanding end-users, deeply understanding their problems, and brainstorming a wide range of solutions. If you’re starting without any ideas, then such an extended process can be a good way to develop good ideas. Further, keeping ideas open-ended can be good for curiosity-driven research, where investing to pursue deep tech with only a vague direction in mind can pay huge dividends over the long term. If you are thinking about starting a new AI project, consider whether you can come up with a concrete vision to execute toward. Even if the initial vision turns out not to be quite right, rapid iteration will let you discover this sooner, and the learnings will let you switch to a different concrete idea. Through working with many large corporations, AI Fund has developed best practices for identifying concrete ideas relevant to a business. I’ll share more on this in a later letter.Keep learning!Andrew",
        "summary": "*   For building AI applications, it is recommended to start with a \"concrete idea\" – a specific product envisioned for a specific target user – rather than a vague problem.\n*   A concrete idea is defined as one detailed enough for a product/engineering team to build an initial prototype (e.g., \"Apply face recognition to cows...\").\n*   Starting with a concrete idea enables faster execution, rapid discovery and fixing of problems, and efficient assessment of technical and business feasibility.\n*   This approach leverages the low cost of experimenting and iterating common in many AI applications, contrasting with traditional design thinking and deep-tech AI projects where vague directions might be explored longer.",
        "publish_date": "Jul 24, 2024",
        "keywords": [
            "face recognition",
            "AI applications",
            "deep-tech AI",
            "prototype",
            "experimenting and iterating"
        ],
        "url": "https://www.deeplearning.ai/the-batch/concrete-ideas-make-strong-ai-startups/"
    },
    {
        "content": "Dear friends,I continue to be alarmed at the progress of proposed California regulation SB 1047 and the attack it represents on open source and more broadly on AI innovation. As I wrote previously, this proposed law makes a fundamental mistake of regulating AI technology instead of AI applications, and thus would fail to make AI meaningfully safer. I’d like to explain why the specific mechanisms of SB 1047 are so pernicious to open source. To be clear, there are routes that regulators should pursue to improve safety. For example, I would welcome outlawing nonconsensual deepfake pornography, standardizing watermarking and fingerprinting to identify generated content, and investing more in red teaming and other safety research. Unfortunately, the proposed bill pursues a less beneficial and more harmful path.SB 1047’s purported goal is to ensure safety of AI models. It puts in place complex reporting requirements for developers who fine-tune models or develop models that cost more than $100 million to train. It is a vague, ambiguous law that imposes significant penalties for violations, creating a huge gray zone in which developers can’t be sure how to avoid breaking the law. This will paralyze many teams. You can read the latest draft of the law here. I’ve read through it carefully, and I find it ambiguous and very hard to follow.Developers who try to navigate the law’s complex requirements face what feels like a huge personal risk. It requires that developers submit a certification of compliance with the requirements of the law. But when the requirements are complex, hard to understand, and can even shift according to the whims of an unelected body (more on this below), how do we ensure we are in compliance?For example, the certification must include many different sections. One is an analysis of “the nature and magnitude of critical harms … the model might reasonably cause or enable.” But given that even leading AI researchers aren’t sure what harms models might cause or enable, how is a team of developers supposed to figure this out and declare — under penalty of perjury — that they meet this requirement? Further, some developers will be required to implement “protections to prevent … misuse of, or unsafe post-training modifications of, the covered model and all covered model derivatives … that are appropriate in light of the risks associated with the covered model, including from advanced persistent threats or other sophisticated actors.” Even leading AI researchers don’t agree on how best to “protect” AI models against these supposed risks, or what would be “appropriate.” So how are developers supposed to figure out how to comply with this requirement? This creates a scary situation for developers. Committing perjury could lead to fines and even jail time. Some developers will have to hire expensive lawyers or consultants to advise them on how to comply with these requirements. (I am not a lawyer and am not giving legal advice, but one way to try to avoid perjury is to show that you are relying on expert advice, to demonstrate that you had no intent to lie.) Others will simply refrain from releasing cutting-edge AI products.If this law passes, the fear of a trial by a jury — leading to a verdict that can be very unpredictable with significant penalties in the event of a conviction — will be very real. What if someone releases a model today after taking what they genuinely felt were reasonable safeguards, but a few years later, when views on AI technology might have shifted, some aggressive prosecutor manages to convince a jury that whatever they did was not, in hindsight, “reasonable”? Reasonableness is ambiguous and its legal interpretation can depend on case law, jury instructions, and common facts, among other things. This makes it very hard to ensure that what a developer does today will be deemed reasonable by a future jury. (For more on this, see Context Fund’s analysis of SB 1047.)One highly placed lawyer in the California government who studied this law carefully told me they found it hard to understand. I invite you to read it and judge for yourself — if you find the requirements clear, you might have a brilliant future as a lawyer! Adding to the ambiguity, the bill would create a Frontier Model Division (FMD) with a five-person board that has the power to dictate standards to developers. This small board would be a great target for lobbying and regulatory capture. (Bill Gurley has a great video on regulatory capture.) The unelected FMD can levy fees on developers to cover its costs. It can arbitrarily change the computation threshold at which fine-tuning a model becomes subject to its oversight. This can lead to even small teams being required to hire an auditor to check for compliance with an ambiguous safety standard. These provisions don’t ensure that AI is safe. They create regulatory uncertainty, and more opportunities for vested interests wishing to stifle open-source to lobby for shifts in the requirements that raise the cost of compliance. This would lock out many teams that don’t have a revenue stream — specifically, many open-source contributors — that would let them pay for lobbyists, auditors, and lawyers to help ensure they comply with these ambiguous and unreasonable requirements. Open source is a wonderful force that is bringing knowledge and tools to many people, and is a key pillar of AI innovation. I am dismayed at the concerted attacks on it. Make no mistake, there is a fight in California right now for the future health of open source. I am committed to doing what I can to preserve open source, but I don’t assume that the pro-open source side will prevail. I hope you will join me in speaking out against SB 1047 and other laws that threaten to stifle open source. Keep learning!Andrew",
        "summary": "*   Proposed California regulation SB 1047 targets AI technology itself, not applications, with a purported goal of ensuring AI model safety.\n*   It mandates complex reporting and certification for developers of AI models costing > $100 million to train or fine-tuned models.\n*   Certification requires analysis of \"critical harms\" and implementation of \"protections\" against misuse, creating ambiguity and potential legal risk (perjury) for developers.\n*   SB 1047 establishes a five-person Frontier Model Division (FMD) board with power to dictate standards and arbitrarily change the computation threshold for oversight.\n*   The regulation's complexity, ambiguity, and compliance costs are seen as creating regulatory uncertainty and threatening AI innovation, especially open source development.",
        "publish_date": "Jul 10, 2024",
        "keywords": [
            "fine-tuned models",
            "computation threshold",
            "open source development",
            "train",
            "AI models"
        ],
        "url": "https://www.deeplearning.ai/the-batch/californias-proposed-ai-safety-law-puts-developers-at-risk-california-sb-1047-is-intended-to-make-ai-safer-but-its-unclear-requirements-put-developers-innovation-and-open-source-in-jeop/"
    },
    {
        "content": "Dear friends,“Democracy is the worst form of government, except for all the others,” said Winston Churchill. Last week’s shocking attempt to assassinate former President Trump was a reminder that democracy is fragile.Democracy lets citizens argue with each other via words and votes. While imperfect, it is a powerful force for making sure that people are governed by leaders of their own choosing, and that these leaders are accountable to making people better off. That’s why attempts to disrupt the democratic process, such as assassinating a political candidate or attempting to disrupt a peaceful handover of power to a newly elected government, are despicable: They attack a fundamental mechanism for giving everyone a chance to have a say in who governs. I denounce all political violence and grieve for Corey Comperatore, who was killed in the assassination attempt, and for his family. I hope for a quick recovery for former President Trump and the bystanders who were injured. I also hope we can put more resources into strengthening the mechanisms of democracy. In addition, I wonder what role AI can play in preserving democracy. Technology can have positive or negative impacts on specific mechanisms of democracy. For instance, data analysis can help citizens and reporters discover facts. Micro-targeting of political ads and social media can increase polarization, while social media can also provide useful information to voters. But zooming out to a macro view,Concentration of power, which is enhanced by concentration of access to technology, tends to make a subset of society more powerful at the expense of the whole and thus weakens democracy. For example, if only major political parties have the resources to place highly targeted voter ads, it’s hard for new parties to break in.However, widespread access to new technologies tends to make everyone more powerful, and thus strengthens democracy. For example, widespread access to smartphones, web search, and now large language model chatbots broadens access to information and lets each individual do more. Thus, I believe spreading new technology as far and wide as possible is an important way to strengthen democracy. I’m glad last week’s assassination attempt failed, just as I’m glad the January 6 insurrection at the U.S. Capitol failed. Both events were close calls and resulted in tragic loss of human life. Looking into the future, in addition to specific applications that strengthen elements of democracy, I hope we keep on promoting widespread access to technology. This will enhance fairness and the ability of individuals to vote wisely. That’s why democratizing access to technology will help democracy itself.Keep learning!Andrew",
        "summary": "*   The potential role of AI in preserving democracy is raised.\n*   Data analysis can help citizens and reporters discover facts.\n*   Micro-targeting of political ads and social media are noted as potentially increasing polarization.\n*   Widespread access to technologies like large language model chatbots is presented as broadening information access and strengthening democracy.",
        "publish_date": "Jul 17, 2024",
        "keywords": [
            "Micro-targeting",
            "large language model",
            "chatbots",
            "Data analysis",
            "Artificial Intelligence"
        ],
        "url": "https://www.deeplearning.ai/the-batch/threats-to-democracy-and-how-ai-can-help/"
    },
    {
        "content": "Dear friends,As we reach the milestone of the 256th issue of The Batch, I’m reflecting on how AI has changed over the years and how society continues to change with it. As AI becomes more widely available, it’s clear that many people — developers and non-developers — will benefit from high-quality training to keep up with the changes and gain useful AI skills. In my years of working in education, I’ve felt that the world has enough low-quality courses, newsletters, social media posts, and other forms of content. It’s possible to build a business churning out mediocre content in sufficient volume to attract a meaningful amount of attention, but I have no interest in doing that. At DeepLearning.AI, our core philosophy is to put learners first. Our team obsesses about how to create quality training or other programs that benefit people who want to learn about AI. We have intense debates about what tools to teach, which examples to include, even which partners to work with, based on what we think is best for learners.For example, I recall vividly how, when working on the Machine Learning Specialization, our team spent ages debating whether to use row or column matrices. Both sides showed up with deep analysis of the pros and cons, made Powerpoint presentations to argue their case, and we spent hours debating over what was better for learners in terms of both ease of picking up the concepts as well as subsequently being able to use these skills with third-party machine learning libraries. We don’t release a course unless we think it’s a good use of a learner’s time and we’d be proud to recommend it to our own friends and family members. Quality, of course, can mean a lot of things. I expect what we do to be technically accurate, useful, up to date, clear, and time-efficient for learners. And, if possible, fun! We don’t always get it right, but we scrutinize learner feedback (one of my most important weekly routines is to study a dashboard that summarizes learner ratings of our courses) and work to make sure our courses serve learners well. And yes, we have a large-language model powered application that reads learner reviews to flag important issues quickly. Earlier this year, we realized that some of the paid content we had launched was below our quality standard, and that I wouldn’t in good conscience recommend it to my friends or family members. Despite this content being profitable, we did what we felt was the right thing for learners. So we decided to retire that content and forgo the revenues, but we feel much better now for having done the right thing for learners. When we teach courses with partners, we tell them our priorities are “learners first, partners second, ourselves last.” I’m grateful to the many wonderful companies and individuals that work with us to teach cutting-edge techniques, and given an opportunity we try to support our partners’ goals as well. But we never prioritize the interest of our educational partners over that of learners. Fortunately, our partners are onboard with this as well. We have a common goal to serve learners. Without their help, it would be difficult to teach many of the topics we do with high-quality content. Quite a few companies have tried to offer to pay us to teach a course with them, but we’ve always said no. We work only with the companies that we think help us serve learners best, and are not interested in being paid to teach lower quality courses.One reason I obsess about building quality training materials is that I think learning must be a habit. Learning a little every week is important to get through the volume of learning we all need, and additionally to keep up with changing technology. High-quality training that’s also fun supports a healthy learning habit! Fun fact: In addition to taking online courses, I also read a lot. Recently I noticed that my digital reading app says I’ve been on a reading streak for 170 weeks. I’ve used the app for many years, but apparently I had broken and restarted my streak 170 weeks ago. What happened then? That was the week that my son was born, Coursera became a public company, and my grandfather died. While my life has had disruptions since then, I was happy to find that it takes a disruption of this magnitude to make me pause my learning habit for a week.Keep learning!Andrew",
        "summary": "*   High-quality training is needed for developers and non-developers to keep up with AI changes and gain useful skills.\n*   DeepLearning.AI prioritizes creating technically accurate, useful, and up-to-date AI training content, debating technical details like row/column matrices for learner benefit.\n*   A large-language model powered application is used to read learner reviews and quickly flag important issues.\n*   The company retired profitable paid content deemed below quality standard, prioritizing learner benefit over revenue.",
        "publish_date": "Jul 3, 2024",
        "keywords": [
            "large-language model",
            "row/column matrices",
            "AI training content",
            "DeepLearning.AI",
            "deep learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-world-needs-high-quality-ai-education-more-than-ever/"
    },
    {
        "content": "Dear friends,On Monday, a number of large music labels sued AI music makers Suno and Udio for copyright infringement. Their lawsuit echoes The New York Times’ lawsuit against OpenAI in December. The question of what’s fair when it comes to AI software remains a difficult one. I spoke out in favor of OpenAI’s side in the earlier lawsuit. Humans can learn from online articles and use what they learn to produce novel works, so I’d like to be allowed to use AI to do so. Some people criticized my view as making an unjustifiable equivalence between humans and AI. This made me realize that people have at least two views of AI: I view AI as a tool we can use and direct to our own purposes, while some people see it as akin to a separate species, distinct from us, with its own goals and desires.If I’m allowed to build a house, I want to be allowed to use a hammer, saw, drill, or any other tool that might get the job done efficiently. If I’m allowed to read a webpage, I’d like to be allowed to read it with any web browser, and perhaps even have the browser modify the page’s formatting for accessibility. More generally, if we agree that humans are allowed to do certain things — such as read and synthesize information on the web — then my inclination is to let humans direct AI to automate this task. In contrast to this view of AI as a tool, if someone thinks humans and AI are akin to separate species, they’ll frame the question differently. Few people today think all species should have identical rights. If a mosquito annoys a human, the mosquito can be evicted (or worse). In this view, there’s no reason to think that, just because humans are allowed to do something, AI should be allowed to do it as well. To be clear, just as humans aren’t allowed to reproduce large parts of copyrighted works verbatim (or nearly verbatim) without permission, AI shouldn’t be allowed to do so either. The lawsuit against Suno and Udio points out that, when prompted in a particular way, these services can nearly reproduce pieces of copyrighted music.But here, too, there are complex issues. If someone were to use a public cloud to distribute online content in violation of copyright, typically the person who did that would be at fault, not the cloud company (so long as the company took reasonable precautions and didn’t enable copyright infringement deliberately). The plaintiffs in the lawsuit against Suno and Udio managed to write prompts that caused the systems to reproduce copyrighted work. But is this like someone managing to get a public cloud to scrape and distribute content in a way that violates copyright? Or is this — as OpenAI said — a rare bug that AI companies are working to eliminate? (Disclaimer: I’m not a lawyer and I’m not giving legal advice.)Humans and software systems use very different mechanisms for processing information. So in terms of what humans can do — and thus what I’d like to be allowed to use software to help me do — it’s helpful to consider the inputs and outputs. Specifically, if I’m allowed to listen to a lot of music and then compose a novel piece of music, I would like to be allowed to use AI to implement a similar input-to-output mapping. The process for implementing this mapping may be training a neural network on music that’s legally published on the open internet for people to enjoy without encumbrances.To acknowledge a weakness of my argument, just because humans are allowed to emit a few pounds of carbon dioxide per day simply by breathing doesn’t mean we should allow machines to emit massively more carbon dioxide without restrictions. Scale can change the nature of an act. When I was a high-school student in an internship job, I spent numerous hours photocopying, and I remember wishing I could automate that repetitive work. Humans do lots of valuable work, and AI, used as a tool to automate what we do, will create lots of value. I hope we can empower people to use tools to automate activities they’re allowed to do, and erect barriers to this only in extraordinary circumstances, when we have clear evidence that it creates more harm than benefit to society.Keep learning!Andrew",
        "summary": "*   AI music makers Suno and Udio, similar to OpenAI, face copyright infringement lawsuits, raising questions about fair use for AI software.\n*   Two contrasting views on AI are discussed: as a tool for humans to automate allowed tasks versus as a distinct entity not automatically granted human permissions.\n*   Complex issues surrounding AI copyright infringement include determining responsibility (AI service vs. user prompting) and whether verbatim reproduction is a bug.\n*   The author suggests AI should be allowed to automate input-to-output mappings humans can perform (e.g., learning from music to compose), potentially via training on legally published data.\n*   The scale of AI's actions may necessitate different considerations compared to human actions.",
        "publish_date": "Jun 26, 2024",
        "keywords": [
            "Suno",
            "Udio",
            "Copyright infringement",
            "Fair use",
            "Verbatim reproduction"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-is-a-tool-not-a-separate-species/"
    },
    {
        "content": "Dear friends,Tool Use, in which an LLM is given functions it can request to call for gathering information, taking action, or manipulating data, is a key design pattern of AI agentic workflows. You may be familiar with LLM-based systems that can perform a web search or execute code. Indeed, some large, consumer-facing LLMs already incorporate these features. But Tool Use goes well beyond these examples. If you prompt an online LLM-based chat system, “What is the best coffee maker according to reviewers?”, it might decide to carry out a web search and download one or more web pages to gain context. Early on, LLM developers realized that relying only on a pre-trained transformer to generate output tokens is limiting, and that giving an LLM a tool for web search lets it do much more. With such a tool, an LLM is either fine-tuned or prompted (perhaps with few-shot prompting) to generate a special string like {tool: web-search, query: \"coffee maker reviews\"} to request calling a search engine. (The exact format of the string depends on the implementation.) A post-processing step then looks for strings like these, calls the web search function with the relevant parameters when it finds one, and passes the result back to the LLM as additional input context for further processing. Similarly, if you ask, “If I invest $100 at compound 7% interest for 12 years, what do I have at the end?”, rather than trying to generate the answer directly using a transformer network — which is unlikely to result in the right answer — the LLM might use a code execution tool to run a Python command to compute 100 * (1+0.07)**12 to get the right answer. The LLM might generate a string like this: {tool: python-interpreter, code: \"100 * (1+0.07)**12\"}. But Tool Use in agentic workflows now goes much further. Developers are using functions to search different sources (web, Wikipedia, arXiv, etc.), to interface with productivity tools (send email, read/write calendar entries, etc.), generate or interpret images, and much more. We can prompt an LLM using context that gives detailed descriptions of many functions. These descriptions might include a text description of what the function does plus details of what arguments the function expects. And we’d expect the LLM to automatically choose the right function to call to do a job. Further, systems are being built in which the LLM has access to hundreds of tools. In such settings, there might be too many functions at your disposal to put all of them into the LLM context, so you might use heuristics to pick the most relevant subset to include in the LLM context at the current step of processing. This technique, which is described in the Gorilla paper cited below, is reminiscent of how, if there is too much text to include as context, retrieval augmented generation (RAG) systems offer heuristics for picking a subset of the text to include. Early in the history of LLMs, before widespread availability of large multimodal models (LMMs)  like LLaVa, GPT-4V, and Gemini, LLMs could not process images directly, so a lot of work on Tool Use was carried out by the computer vision community. At that time, the only way for an LLM-based system to manipulate an image was by calling a function to, say, carry out object recognition or some other function on it. Since then, practices for Tool Use have exploded. GPT-4’s function calling capability, released in the middle of last year, was a significant step toward a general-purpose implementation. Since then, more and more LLMs are being developed to be similarly facile with Tool Use. If you’re interested in learning more about Tool Use, I recommend: “Gorilla: Large Language Model Connected with Massive APIs,” Patil et al. (2023)“MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,” Yang et al. (2023)“Efficient Tool Use with Chain-of-Abstraction Reasoning,” Gao et al. (2024)   Both Tool Use and Reflection, which I described in last week’s letter, are design patterns that I can get to work fairly reliably on my applications — both are capabilities well worth learning about. In future letters, I’ll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable — albeit very exciting — technologies. Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"",
        "summary": "*   Tool Use is a key design pattern in AI agentic workflows, enabling LLMs to call functions for gathering information, taking action, or manipulating data.\n*   LLMs implement Tool Use by generating special strings (e.g., {tool: web-search, query: ...}, {tool: python-interpreter, code: ...}) that trigger external function calls, with results fed back as additional input context.\n*   Tool Use extends beyond web search and code execution to include searching diverse sources, interfacing with productivity tools, and handling images; managing systems with hundreds of tools may require heuristics to select relevant subsets, similar to RAG (Gorilla paper).\n*   Historically important for image manipulation before LMMs, Tool Use capabilities have significantly expanded, with GPT-4's function calling capability being a notable step towards general-purpose implementation.",
        "publish_date": "April 03, 2024",
        "keywords": [
            "Tool Use",
            "GPT-4's function calling capability",
            "LMMs",
            "Gorilla paper",
            "python-interpreter"
        ],
        "url": "https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/"
    },
    {
        "content": "Dear friends,Planning is a key agentic AI design pattern in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report. Many people had a “ChatGPT moment” shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar “AI Agentic moment,” I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools. I had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool — which I had forgotten I’d given it — and completed the task using Wikipedia instead of web search. This was an AI Agentic moment of surprise for me. I think many people who haven’t experienced such a moment yet will do so in the coming months. It’s a beautiful thing when you see an agent autonomously decide to do things in ways that you had not anticipated, and succeed as a result!Many tasks can’t be done in a single step or with a single tool invocation, but an agent can decide what steps to take. For example, to simplify an example from the HuggingGPT paper (cited below), if you want an agent to consider a picture of a boy and draw a picture of a girl in the same pose, the task might be decomposed into two distinct steps: (i) detect the pose in the picture of the boy and (ii) render a picture of a girl in the detected pose. An LLM might be fine-tuned or prompted (with few-shot prompting) to specify a plan by outputting a string like \"{tool: pose-detection, input: image.jpg, output: temp1 } {tool: pose-to-image, input: temp1, output: final.jpg}\". This structured output, which specifies two steps to take, then triggers software to invoke a pose detection tool followed by a pose-to-image tool to complete the task. (This example is for illustrative purposes only; HuggingGPT uses a different format.) Admittedly, many agentic workflows do not need planning. For example, you might have an agent reflect on, and improve, its output a fixed number of times. In this case, the sequence of steps the agent takes is fixed and deterministic. But for complex tasks in which you aren’t able to specify a decomposition of the task into a set of steps ahead of time, Planning allows the agent to decide dynamically what steps to take. On one hand, Planning is a very powerful capability; on the other, it leads to less predictable results. In my experience, while I can get the agentic design patterns of Reflection and Tool Use to work reliably and improve my applications’ performance, Planning is a less mature technology, and I find it hard to predict in advance what it will do. But the field continues to evolve rapidly, and I'm confident that Planning abilities will improve quickly. If you’re interested in learning more about Planning with LLMs, I recommend:“Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Wei et al. (2022)“HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,” Shen et al. (2023)“Understanding the planning of LLM agents: A survey,” by Huang et al. (2024)Keep learning!AndrewRead \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 2: Reflection\" Read \"Agentic Design Patterns Part 3: Tool Use\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"",
        "summary": "*   Planning is an agentic AI design pattern utilizing large language models (LLMs) to autonomously determine the sequence of steps required to complete a larger task.\n*   This pattern enables agents to dynamically break down complex tasks into multiple steps or tool invocations, unlike fixed workflows.\n*   An \"AI Agentic moment\" describes the surprise when an agent autonomously decides on unanticipated actions, such as pivoting to a different tool when the primary one fails.\n*   While Planning offers powerful dynamic capability, it is currently considered less predictable than other agentic patterns like Reflection and Tool Use, though the technology is rapidly evolving.",
        "publish_date": "Apr 10, 2024",
        "keywords": [
            "Planning",
            "agentic AI design pattern",
            "AI Agentic moment",
            "Reflection",
            "Tool Use"
        ],
        "url": "https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/"
    },
    {
        "content": "Dear friends,I think AI agent workflows will drive massive AI progress this year — perhaps even more than the next generation of foundation models. This is an important trend, and I urge everyone who works in AI to pay attention to it.Today, we mostly use LLMs in zero-shot mode, prompting a model to generate final output token by token without revising its work. This is akin to asking someone to compose an essay from start to finish, typing straight through with no backspacing allowed, and expecting a high-quality result. Despite the difficulty, LLMs do amazingly well at this task! With an agent workflow, however, we can ask the LLM to iterate over a document many times. For example, it might take a sequence of steps such as:Plan an outline.Decide what, if any, web searches are needed to gather more information.Write a first draft.Read over the first draft to spot unjustified arguments or extraneous information.Revise the draft taking into account any weaknesses spotted.And so on.This iterative process is critical for most human writers to write good text. With AI, such an iterative workflow yields much better results than writing in a single pass. Devin’s splashy demo recently received a lot of social media buzz. My team has been closely following the evolution of AI that writes code. We analyzed results from a number of research teams, focusing on an algorithm’s ability to do well on the widely used HumanEval coding benchmark. You can see our findings in the diagram below. GPT-3.5 (zero shot) was 48.1% correct. GPT-4 (zero shot) does better at 67.0%. However, the improvement from GPT-3.5 to GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.Reflection: The LLM examines its own work to come up with ways to improve it. Tool Use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.Next week, I’ll elaborate on these design patterns and offer suggested readings for each.Keep learning!AndrewRead \"Agentic Design Patterns Part 2: Reflection\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"",
        "summary": "*   AI agent workflows are expected to drive significant AI progress, potentially more than next-generation foundation models, by enabling iterative processes unlike zero-shot LLM use.\n*   Agent workflows demonstrate substantial performance gains; on the HumanEval benchmark, GPT-3.5 (zero shot) scored 48.1%, GPT-4 (zero shot) scored 67.0%, while GPT-3.5 wrapped in an agent loop achieved up to 95.1%.\n*   Key design patterns for building agents include Reflection, Tool Use, Planning, and Multi-agent collaboration.",
        "publish_date": "Mar 20, 2024",
        "keywords": [
            "HumanEval benchmark",
            "GPT-4",
            "agent loop",
            "zero-shot",
            "Reflection"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/"
    },
    {
        "content": "Dear friends,Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool Use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality output. In this letter, I'd like to discuss Reflection. For a design pattern that’s relatively quick to implement, I've seen it lead to surprising performance gains. You may have had the experience of prompting ChatGPT/Claude/Gemini, receiving unsatisfactory output, delivering critical feedback to help the LLM improve its response, and then getting a better response. What if you automate the step of delivering critical feedback, so the model automatically criticizes its own output and improves its response? This is the crux of Reflection. Take the task of asking an LLM to write code. We can prompt it to generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:Here’s code intended for task X: [previously generated code]    Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.Sometimes this causes the LLM to spot problems and come up with constructive suggestions. Next, we can prompt the LLM with context including (i) the previously generated code and the constructive feedback and (ii) ask it to use the feedback to rewrite the code. This can lead to a better response. Repeating the criticism/rewrite process might yield further improvements. This self-reflection process allows the LLM to spot gaps and improve its output on a variety of tasks including producing code, writing text, and answering questions. And we can go beyond self-reflection by giving the LLM tools that help evaluate its output; for example, running its code through a few unit tests to check whether it generates correct results on test cases or searching the web to double-check text output. Then it can reflect on any errors it found and come up with ideas for improvement.Further, we can implement Reflection using a multi-agent framework. I've found it convenient to create two different agents, one prompted to generate good outputs and the other prompted to give constructive criticism of the first agent's output. The resulting discussion between the two agents leads to improved responses.Reflection is a relatively basic type of agentic workflow, but I've been delighted by how much it improved my applications’ results in a few cases. I hope you will try it in your own work. If you’re interested in learning more about reflection, I recommend these papers:“Self-Refine: Iterative Refinement with Self-Feedback,” Madaan et al. (2023)“Reflexion: Language Agents with Verbal Reinforcement Learning,” Shinn et al. (2023)“CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,” Gou et al. (2024)I’ll discuss the other agentic design patterns in future letters.Keep learning!Andrew Read \"Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance\"Read \"Agentic Design Patterns Part 3, Tool Use\"Read \"Agentic Design Patterns Part 4: Planning\"Read \"Agentic Design Patterns Part 5: Multi-Agent Collaboration\"",
        "summary": "*   Agentic workflows prompt LLMs multiple times for step-by-step output generation, aiming for higher quality than direct generation.\n*   Reflection is an agentic design pattern where an LLM automatically criticizes and improves its own output.\n*   The Reflection process involves generating output, prompting for critique (correctness, style, efficiency), and rewriting based on feedback, leading to performance gains and improved output.\n*   Reflection can be enhanced using tools for evaluation (e.g., unit tests, web search) or implemented via multi-agent frameworks.\n*   Research papers like \"Self-Refine\" (Madaan et al., 2023), \"Reflexion\" (Shinn et al., 2023), and \"CRITIC\" (Gou et al., 2024) explore Reflection.",
        "publish_date": "Mar 27, 2024",
        "keywords": [
            "Reflection",
            "Agentic workflows",
            "Self-Refine",
            "Reflexion",
            "CRITIC"
        ],
        "url": "https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/"
    },
    {
        "content": "Dear friends,I’ve noticed a trend in how generative AI applications are built that might affect both big companies and developers: The gravity of data is decreasing.Data gravity is the idea, proposed by IT engineer Dave McCrory in 2010, that data, or activity around data, attracts and creates more data. With traditional software workloads, data gravity is strong. If you have terabytes of data stored in a particular cloud, the cost to transmit it elsewhere for processing is high. So many teams pick a cloud such as AWS, Azure, or Google Cloud and build on it.However, for many generative AI applications, the cost of processing is much greater than the cost of transmission. This weakens data gravity because data is more weakly bound to the cloud provider or data center where it’s stored, so it’s more practical to build systems that send packets to different servers all over the internet. Let’s say transmitting 1GB of data costs $0.10. 1GB of text might correspond to about 250 million inputs tokens (if we average four characters per token), which costs about $125 to process using the relatively inexpensive gpt-3.5-turbo-0125 model. (With gpt-4-0125-preview, the cost would be 20x higher.) The cost of processing the data is significantly higher than the cost of transmission. Also, given the computationally intensive nature of using an LLM to read and generate tokens, the latency is high enough that sending your text or image tokens across the internet usually doesn’t add that much further latency. This means that, even if we’re building software primarily on a particular cloud provider, it’s still quite feasible to transmit LLM prompts to OpenAI, Anthropic, Anyscale, or Together.ai — or, for that matter, AWS, Azure, or Google Cloud — to get a response. The incentive to build only on a single, monolithic cloud platform is lower than before.  This situation has implications for stakeholders:For developers, it means we’re increasingly assembling AI applications from lots of SaaS providers all across the internet, and stitching their services together.For CIOs, it’s creating headaches in terms of managing where their data goes and how to maintain lists of trusted vendors.For the big cloud companies, it’s changing the basis of competition, since the generative AI portions of their customer workloads look quite different from traditional software workloads.For new tool developers, it’s creating new opportunities for users to use their services, even if they aren’t bundled into one of cloud environments.To be clear, many applications have large traditional software components (that serve up a websites, maintain databases, and so on) as well as new generative AI components (say, a chatbot built on top of the traditional infrastructure). My remarks here apply only to the generative AI portion, and the competitive dynamics of the traditional software components haven’t changed much.Further, as new types of AI components emerge, I expect their gravity to evolve as well. For example, right now it appears reasonably easy to change LLM providers; if you’ve built a system on one LLM, it’s annoying but not impossible to switch to a different LLM provider. In comparison, shifting databases is much harder, and once you’ve stored a lot of data in one vector database, the complexity of migrating to a different one can be high. The gravity of data has been a fundamental tenet of cloud computing, and a major factor of competition for many companies. Decreasing data gravity is decreasing is a complex, exciting trend that will affect many developers and businesses.Keep learning!AndrewP.S. Our new short course “Knowledge Graphs for RAG” is now available, taught by Andreas Kollegger of Neo4j! Knowledge graphs are a data structure that’s great at capturing complex relationships among data of multiple types. They can improve the context you pass to the LLM and the performance of your RAG applications by enabling more sophisticated retrieval of text than similarity search alone. In this course, you’ll build a knowledge graph from scratch and see how it improves chat applications by providing both text and graph data to an LLM. Sign up here!",
        "summary": "*   Data gravity, traditionally strong for software workloads, is decreasing for many generative AI applications.\n*   This decrease is because the cost of processing data for generative AI (e.g., ~$125 for 1GB text with gpt-3.5-turbo-0125) is significantly higher than data transmission costs (e.g., ~$0.10 for 1GB).\n*   Weakened data gravity allows assembling generative AI applications from multiple SaaS providers across the internet, reducing the incentive for single, monolithic cloud platforms.\n*   This trend specifically applies to generative AI components, and the gravity varies for different AI components (e.g., switching LLM providers is currently less complex than migrating vector databases).",
        "publish_date": "Mar 13, 2024",
        "keywords": [
            "Data gravity",
            "gpt-3.5-turbo-0125",
            "vector databases",
            "LLM providers",
            "generative AI applications"
        ],
        "url": "https://www.deeplearning.ai/the-batch/life-in-low-data-gravity/"
    },
    {
        "content": "Dear friends,Last week’s letter focused on coming up with AI project ideas, part of a series on how to build a career in the field. This letter describes how a sequence of projects might fit into your career path.Over the course of a career, you’re likely to work not on a single AI project, but on a sequence of projects that grow in scope and complexity. For example:Class projects: The first few projects might be narrowly scoped homework assignments with predetermined right answers. These are often great learning experiences!Personal projects: You might go on to work on small-scale projects either alone or with friends. For instance, you might re-implement a known algorithm, apply machine learning to a hobby (such as predicting whether your favorite sports team will win), or build a small but useful system at work in your spare time (such as a machine learning-based script that helps a colleague automate some of their work). Participating in competitions such as those organized by Kaggle is also one way to gain experience.Creating value: Eventually, you gain enough skill to build projects in which others see more tangible value. This opens the door to more resources. For example, rather than developing machine learning systems in your spare time, it might become part of your job, and you might gain access to more equipment, compute time, labeling budget, or head count.Rising scope and complexity: Successes build on each other, opening the door to more technical growth, more resources, and increasingly significant project opportunities.In light of this progression, when picking a project, keep in mind that it is only one step on a longer journey, hopefully one that has a positive impact. In addition:Don’t worry about starting too small. One of my first machine learning research projects involved training a neural network to see how well it could mimic the sin(x) function. It wasn’t very useful, but was a great learning experience that enabled me to move on to bigger projects.Communication is key. You need to be able to explain your thinking if you want others to see the value in your work and trust you with resources that you can invest in larger projects. To get a project started, communicating the value of what you hope to build will help bring colleagues, mentors, and managers onboard — and help them point out flaws in your reasoning. After you’ve finished, the ability to explain clearly what you accomplished will help convince others to open the door to larger projects.Leadership isn’t just for managers. When you reach the point of working on larger AI projects that require teamwork, your ability to lead projects will become more important, whether or not you are in a formal position of leadership. Many of my friends have successfully pursued a technical rather than managerial career, and their ability to help steer a project by applying deep technical insights — for example, when to invest in a new technical architecture or collect more data of a certain type — allowed them to exert leadership that helped the project significantly.Building a portfolio of projects, especially one that shows progress over time from simple to complex undertakings, will be a big help when it comes to looking for a job. That will be the subject of a future letter.Keep learning!Andrew",
        "summary": "*   An AI career typically progresses through a sequence of projects increasing in scope and complexity, starting with class or personal projects.\n*   Early projects can involve re-implementing known algorithms, applying machine learning to hobbies, building small ML-based systems at work, or participating in competitions like Kaggle.\n*   Building projects that create tangible value leads to increased resources such as equipment, compute time, labeling budget, or head count, enabling larger opportunities.\n*   Communication is key to explaining project value and accomplishments, helping secure resources and access to bigger projects.\n*   Technical leadership, applying deep insights (e.g., on architecture or data collection), is important for steering larger AI projects.",
        "publish_date": "Jul 20, 2022",
        "keywords": [
            "Kaggle",
            "Algorithms",
            "Architecture",
            "Data Collection",
            "ML-based systems"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-4-progress-through/"
    },
    {
        "content": "Dear friends, Stable Diffusion, an image generation model that takes a text prompt and produces an image, was released a few weeks ago in a landmark event for AI. While similar programs like DALL·E and Craiyon can be used via API calls or a web user interface, Stable Diffusion can be freely downloaded and run on the user’s hardware. I'm excited by the artwork produced by such programs (Developer Simon Willison posted a fun tweetstorm that highlights some of the creativity they’ve unleashed), but I’m also excited by the ways in which other developers are incorporating it into their own drawing tools. Ironically, Stable Diffusion’s manner of release moves us closer to “open AI” than the way DALL·E was released by the company called OpenAI. Kudos to Emad Mostaque and his Stability AI team, which developed the program. If you want to learn about how diffusion models like Stable Diffusion work, you can find a concise description here. Image generation is still maturing, but it’s a big deal. Many people have the creativity to produce art but lack the drawing skill to do so. As an amateur illustrator (I like to draw pandas to entertain my daughter using the Procreate paint app), my meager skill limits what I can create. But sitting in front of the DALL·E or Stable Diffusion user interface, I can ask her what she wants to see a panda doing and render a picture for her. Artists who have greater skill than I can use image generators to create stunning artworks more efficiently. In fact, an image produced this way recently won an art competition at the Colorado State Fair. The rise of inexpensive smartphone cameras brought an explosion in photography, and while expensive DSLRs still have a role, they now produce a minuscule fraction of all pictures taken. I expect AI-powered image generators to do something similar in art: Ever-improving models and user interfaces will make it much more efficient to generate art using AI than without. I see a future where most art is generated using AI, and novices who have great creativity but little drawing skill will be able to participate. My friend and collaborator Curt Langlotz, addressing the question of whether AI will replace radiologists, said that radiologists who use AI will replace radiologists who don’t. The same will be true here: Artists who use AI will (largely) replace artists who don’t. Imagine the transition in the 1800s from the time when each artist had to source their own minerals to mix shades of paint to when they could purchase ready-mixed paint in a tube. This development made it easier for any artist to paint whatever and whenever they wished. I see a similar transition ahead. What an exciting time! Separately from generating images for human consumption, these algorithms have great potential to generate images for machine consumption. A number of companies have been developing image generation techniques to produce training images for computer vision algorithms. But because of the difficulty of generating realistic images, many have focused on vertical applications that are sufficiently valuable to justify their investment, such as generating road scenes to train self-driving cars or portraits of diverse faces to train face recognition algorithms. Will image generation algorithms reduce the cost of data generation and other machine-to-machine processes? I believe so. It will be interesting to see this space evolve. Keep learning! Andrew",
        "summary": "* Stable Diffusion is an AI image generation model that takes text prompts and can be freely downloaded and run on user hardware, unlike API/web UI models like DALL·E and Craiyon.\n* AI image generators enable individuals with creativity but limited drawing skill to create art and are expected to make art creation more efficient, potentially leading to artists using AI replacing those who don't.\n* Image generation algorithms have potential for generating training images for computer vision algorithms (e.g., self-driving cars, face recognition) and are expected to reduce data generation costs for machine-to-machine processes.",
        "publish_date": "Sep 14, 2022",
        "keywords": [
            "Stable Diffusion",
            "DALL·E",
            "Craiyon",
            "Image generation algorithms",
            "computer vision algorithms"
        ],
        "url": "https://www.deeplearning.ai/the-batch/text-to-image-generation-and-the-path-to-truly-open-ai/"
    },
    {
        "content": "Dear friends,A few weeks ago, the White House required that research papers funded by the U.S. government be available online promptly and freely by the end of 2025. Data that underlies those publications must also be made available.I’m thrilled! Paywalled journals that block free access to scientific research are the bane of the academic community.The AI world is fortunate to have shifted years ago to free online distribution of research papers, primarily through the arXiv site. I have no doubt that this has contributed to the rapid rise of AI and am confident that, thanks to the new U.S. policy, promoting a similar shift in other disciplines will accelerate global scientific progress.In the year 2000 — before modern deep learning, and when dinosaurs still roamed the planet — AI researchers were up in arms against paywalled journals. Machine Learning Journal, a prominent journal of the time, refused to open up access. With widespread support from the AI community, MIT computer scientist Leslie Kaelbling started the free Journal of Machine Learning Research, and many researchers promptly began publishing there instead. This move led to the rapid decline of Machine Learning Journal. The Journal of Machine Learning Research remains a respected institution today, edited by David Blei and Francis Bach (both of who are my former officemates at UC Berkeley).Before the modern internet, journal publishers played an important role by printing and disseminating hard copies of papers. It was only fair that they could charge fees to recoup their costs and make a modest profit. But in today’s research environment, for-profit journals rely mainly on academics to review papers for free, and they harvest the journals’ reputations (as reflected in metrics such as impact factor) to extract a profit.Today, there are peer-reviewed journal papers, peer-reviewed conference papers, and non-peer-reviewed papers posted online directly by the authors. Journal articles tend to be longer and undergo peer review and careful revisions. In contrast, conference papers (such as NeurIPS, ICML and ICLR articles) tend to be shorter and less carefully edited, and thus they can be published more quickly. And papers published on arXiv aren’t peer reviewed, so they can be published and reach interested readers immediately.The benefits of rapid publication and distribution have caused a lot of the action to shift away from journals and toward conferences and arXiv. While the volume of research is overwhelming (that’s why The Batch tries to summarize the AI research that matters), the velocity at which ideas circulate has contributed to AI’s rise.By the time the new White House guidance takes effect, a quarter century will have passed since machine learning researchers took a key step toward unlocking journal access. When I apply AI to healthcare, climate change, and other topics, I occasionally bump into an annoyingly paywalled article from these other disciplines. I look forward to seeing these walls come down.Don’t underestimate the impact of freeing up knowledge. I wish all these changes had taken place a quarter century ago, but I’m glad we’re getting there and look forward to the acceleration of research in all disciplines!Keep learning!Andrew",
        "summary": "*   The AI world shifted years ago to free online distribution of research papers, primarily via arXiv, which is credited with contributing to AI's rapid rise.\n*   In 2000, AI researchers started the free Journal of Machine Learning Research (JMLR) in opposition to paywalled journals, leading to the decline of the Machine Learning Journal.\n*   Rapid publication and distribution benefits have caused AI research dissemination to shift from journals towards conferences (like NeurIPS, ICML, ICLR) and arXiv.\n*   The velocity at which ideas circulate through faster publication methods has contributed to AI's rise.",
        "publish_date": "Sep 07, 2022",
        "keywords": [
            "arXiv",
            "Journal of Machine Learning Research (JMLR)",
            "NeurIPS",
            "ICML",
            "ICLR"
        ],
        "url": "https://www.deeplearning.ai/the-batch/research-paywall/"
    },
    {
        "content": "Dear friends,The rise of AI over the last decade has been powered by the increasing speed and decreasing cost of GPUs and other accelerator chips. How long will this continue? The past month saw several events that might affect how GPU prices evolve.In September, Ethereum, a major blockchain that supports the cryptocurrency known as ether, completed a shift that significantly reduced the computation it requires. This shift — dubbed the Merge — should benefit the natural environment by consuming less energy. It will also decrease demand for GPUs to carry out cryptocurrency mining. (The Bitcoin blockchain remains computationally expensive.) I expect that lower demand will help lower GPU prices.On the other hand, Nvidia CEO Jensen Huang declared recently that the era in which chip prices could be expected to fall is over. Moore’s Law, the longstanding trend that has doubled the number of transistors that could fit in a given area of silicon roughly every two years, is dead, he said. It remains to be seen how accurate his prediction is. After all, many earlier reports of the death of Moore’s Law have turned out to be wrong. Intel continues to bet that it will hold up.That said, improvements in GPU performance have exceeded the pace of Moore’s Law as Nvidia has optimized its chips to process neural networks, while the pace of improvements in CPUs, which are designed to process a wider range of programming, has fallen behind. So even if chip manufacturers can’t pack silicon more densely with transistors, chip designers may be able to continue optimizing to improve the price/performance ratio for AI.International news also had implications for chip supply and demand. Last week, the United States government restricted U.S. companies from selling advanced semiconductors and chip-making equipment to China. It also prohibited all sales in China of AI chips made using U.S. technology or products and barred U.S. citizens and permanent residents from working for Chinese chip firms.No doubt the move will create significant headwinds for many businesses in China. It will also hurt U.S. semiconductor companies by limiting their market and further incentivizing Chinese competitors to replace them. The AI community has always been global, and if this move further decouples the U.S. and China portions, it will have effects that are hard to foresee.Still, I’m optimistic that AI practitioners will get the processing power they need. While much AI progress has been — and a meaningful fraction still is — driven by using cheaper computation to train bigger neural networks on bigger datasets, other engines of innovation now drive AI as well. Data-centric AI, small data, more efficient algorithms, and ongoing work to adapt AI to thousands (millions?) of new applications will keep things moving forward.Semiconductor startups have had a hard time in recent years because, by the time they caught up with any particular offering by market leader Nvidia, Nvidia had already moved on to a faster, cheaper product. If chip prices stop falling, they’ll have a bigger market opportunity — albeit with significant technical hurdles — to build competitive chips. The industry for AI accelerators remains dynamic. Intel and AMD are making significant investments and a growing number of companies are duking it out on the MLPerf benchmark that measures chip performance. I believe the options for training and inference in the cloud and at the edge will continue to expand.Keep learning!Andrew",
        "summary": "*   The Ethereum Merge reduced GPU demand for crypto mining, potentially lowering prices, while Nvidia CEO declared Moore's Law dead, suggesting chip prices may stop falling.\n*   GPU performance for neural networks has exceeded Moore's Law pace, and optimization may continue improving the AI price/performance ratio despite potential limits on transistor density.\n*   US government restrictions on advanced semiconductors and AI chips/equipment sales to China impact businesses in China, US semiconductor companies, and the global AI community.\n*   AI innovation is increasingly driven by data-centric AI, small data, more efficient algorithms, and application adaptation, beyond just cheaper computation for bigger models/datasets.\n*   The AI accelerator industry is dynamic with competition (Intel, AMD, MLPerf benchmark) and expanding options for training and inference in the cloud and at the edge.",
        "publish_date": "Oct 12, 2022",
        "keywords": [
            "GPU",
            "Moore's Law",
            "AI accelerator",
            "data-centric AI",
            "MLPerf benchmark"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-fate-of-gpu-prices-and-what-it-means-for-ai/"
    },
    {
        "content": "Dear friends,When I wrote recently about how to build a career in AI, several readers wrote to ask specifically about AI product management: the art and science of designing compelling AI products. I’ll share lessons I’ve learned about this here and in future letters.A key concept in building AI products is iteration. As I’ve explained in past letters, developing a machine learning system is a highly iterative process. First you build something, then run experiments to see how it performs, then analyze the results, which enables you to build a better version based on what you’ve learned. You may go through this loop several times in various phases of development — collecting data, training a model, deploying the system — before you have a finished product.Why is development of machine learning systems so iterative? Because (i) when starting on a project, you almost never know what strange and wonderful things you’ll find in the data, and discoveries along the way will help you to make better decisions on how to improve the model; and (ii) it’s relatively quick and inexpensive to try out different models.Not all projects are iterative. For example, if you’re preparing a medical drug for approval by the U.S. government — an expensive process that can cost tens of millions of dollars and take years — you’d usually want to get the drug formulation and experimental design right the first time, since repeating the process to correct a mistake would be costly in time and money. Or if you’re building a space telescope (such as the wonderful Webb Space Telescope) that’s intended to operate far from Earth with little hope of repair if something goes wrong, you’d think through every detail carefully before you hit the launch button on your rocket.Iterating on projects tends to be beneficial when (i) you face uncertainty or risk, and building or launching something can provide valuable feedback that helps you reduce the uncertainty or risk, and (ii) the cost of each attempt is modest.This is why The Lean Startup, a book that has significantly influenced my thinking, advocates building a minimum viable product (MVP) and launching it quickly. Developing software products often involves uncertainty about how users will react, which creates risk for the success of the product. Making a quick-and-dirty, low-cost implementation helps you to get valuable user feedback before you’ve invested too much in building features that users don’t want. An MVP lets you resolve questions about what users want quickly and inexpensively, so you can make decisions and investments with greater confidence.When building AI products, I often see two major sources of uncertainty, which in turn creates risk:Users. The considerations here are similar to those that apply to building software products. Will they like it? Are the features you’re prioritizing the ones they’ll find most valuable? Is the user interface confusing?Data. Does your dataset have enough examples of each class? Which classes are hardest to detect? What is human-level performance on the task, and what level of AI performance is reasonable to expect?A quick MVP or proof of concept, built at low cost, helps to reduce uncertainty about users and/or data. This enables you to uncover and address hidden issues that may hinder your success.Many product managers are used to thinking through user uncertainty and using iteration to manage risk in that dimension. AI product managers should also consider the data uncertainty and decide on the appropriate pace and nature of iteration to enable the development team to learn the needed lessons about the data and, given the data, what level of AI functionality and performance is possible.Keep learning!Andrew",
        "summary": "*   Developing machine learning systems for AI products is a highly iterative process driven by discoveries in data and the relatively low cost of trying different models.\n*   Iteration is beneficial in AI product development due to uncertainty and risk regarding users and data, especially when the cost of each attempt is modest.\n*   Two major sources of uncertainty and risk in building AI products are understanding user needs and characteristics of the data (e.g., dataset examples, performance levels).\n*   Using a Minimum Viable Product (MVP) or proof of concept helps reduce uncertainty about users and data early in AI product development.\n*   AI product managers must manage iteration considering both user uncertainty and the specific uncertainties related to the data.",
        "publish_date": "Oct 05, 2022",
        "keywords": [
            "machine learning systems",
            "data",
            "models",
            "dataset examples",
            "performance levels"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-reduce-risk-and-uncertainty-in-ai-projects/"
    },
    {
        "content": "Dear friends, Activities such as writing code and solving math problems are often perceived as purely intellectual pursuits. But this ignores the fact that they involve the mental equivalent of muscle memory. The idea of muscle memory is a powerful concept in human learning. It has helped millions of people to understand the importance of practice in learning motor tasks. However, it’s also misleading because it excludes skills that don’t involve using muscles. I believe that a similar principle operates in learning intellectual skills. Lack of recognition of this fact has made it harder for people to appreciate the importance of practice in acquiring those skills as well. The phenomenon of muscle memory is widely acknowledged. When you repeatedly practice balancing on a bicycle, swinging a tennis racquet, or typing without looking at the keyboard, adaptations in your brain, nervous system, and muscles eventually allow you to carry out the task without having to consciously pay attention to it. The brain and nervous system are central to learning intellectual skills, and these parts of the body also respond to practice. Whether you’re writing code, solving math problems, or playing chess, practice makes you better at it. It leads your brain to form mental chunks that allow you to reason at a higher level. For example, a novice programmer has to think carefully about every parenthesis or colon, but with enough practice, coding common subroutines can take little conscious effort. Practice frees up your attention to focus on higher-level architectural issues. Of course, there are biological differences between learning motor skills and learning intellectual skills. For example, the former involves parts of the brain that specialize in movement. And the physical world presents somewhat different challenges each time you perform an action (for example, your bicycle hits different bumps, and an opposing tennis player returns each of your serves differently). Thus practicing motor skills automatically leads you to try out your actions in different situations, which trains your brain to adapt to different problems. But I think there are more similarities than people generally appreciate. While watching videos of people playing tennis can help your game, you can’t learn to play tennis solely by watching videos. Neither can you learn to code solely by watching videos of coding. You have to write code, see it sometimes work and sometimes not, and use that feedback to keep improving. Like muscle memory, this kind of learning requires training the brain and nervous system through repetition, focused attention, making decisions, and taking breaks between practice sessions to consolidate learning. And, like muscle memory, it benefits from variation: When practicing an intellectual task, we need to challenge ourselves to work through a variety of situations rather than, say, repeatedly solving the same coding problem. All of this leads me to think that we need an equivalent term for muscle memory in the intellectual domain. As knowledge work has come to play a larger economic role relative to physical labor, the ability to learn intellectual tasks has become much more important than it was when psychologists formed the idea of muscle memory around 150 years ago. This new term would help people understand that practice is as crucial to developing intellectual skills as muscular ones. How about intellect memory? It’s not an elegant phrase, but it acknowledges this under-appreciated reality of learning. What intellectual task do you develop intellect memory for, and can you find time in your schedule to do the necessary practice? After all, there’s no better way to learn. Keep learning! Andrew",
        "summary": "The provided content discusses the importance of practice in learning intellectual skills, drawing an analogy to muscle memory and using examples like coding and solving math problems. While these examples are relevant to fields like AI and Data Science, the core theme of the text is a general principle of learning applicable across various intellectual domains, rather than being strictly focused on AI or Data Science concepts, research, or applications. Therefore, this content falls outside the scope of summarizing material strictly related to AI and Data Science as defined in the instructions.",
        "publish_date": "Sep 21, 2022",
        "keywords": [
            "AI",
            "Data Science",
            "coding",
            "solving math problems",
            "learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-develop-muscle-memory-for-your-mindcategory/"
    },
    {
        "content": "Dear friends, In this letter, I’d like to address the serious matter of newcomers to AI sometimes experiencing imposter syndrome, where someone — regardless of their success in the field — wonders if they’re a fraud and really belong in the AI community. I want to make sure this doesn’t discourage you or anyone else.Let me be clear: If you want to be part of the AI community, then I welcome you with open arms. If you want to join us, you fully belong with us!An estimated 70 percent of people experience some form of imposter syndrome at some point. Many talented people have spoken publicly about this experience, including former Facebook COO Sheryl Sandberg, U.S. first lady Michelle Obama, actor Tom Hanks, and Atlassian co-CEO Mike Cannon-Brookes. It happens in our community even among accomplished people. If you’ve never experienced this yourself, that’s great! I hope you’ll join me in encouraging and welcoming everyone who wants to join our community.AI is technically complex, and it has its fair share of smart and highly capable people. But, of course, it is easy to forget that to become good at anything, the first step is to suck at it. If you’ve succeeded at sucking at AI -- congratulations, you’re on your way!I once struggled to understand the math behind linear regression. I was mystified when logistic regression performed strangely on my data, and it took me days to find a bug in my implementation of a basic neural network. Today, I still find many research papers challenging to read, and just yesterday I made an obvious mistake while tuning a neural network hyperparameter (that fortunately a fellow engineer caught and fixed).So if you, too, find parts of AI challenging, it’s okay. We’ve all been there. I guarantee that everyone who has published a seminal AI paper struggled with similar technical challenges at some point.Here are some things that can help.Do you have supportive mentors or peers? If you don’t yet, attend Pie & AI or other events, use discussion boards, and work on finding some. If your mentors or manager don’t support your growth, find ones who do. I’m also working on how to grow a supportive AI community and hope to make finding and giving support easier for everyone.No one is an expert at everything. Recognize what you do well. If what you do well is understand and explain to your friends one-tenth of the articles in The Batch, then you’re on your way! Let’s work on getting you to understand two-tenths of the articles.My three-year-old daughter (who can barely count to 12) regularly tries to teach things to my one-year-old son. No matter how far along you are — if you’re at least as knowledgeable as a three-year-old — you can encourage and lift up others behind you. Doing so will help you, too, as others behind you will recognize your expertise and also encourage you to keep developing. When you invite others to join the AI community, which I hope you will do, it also reduces any doubts that you are already one of us.AI is such an important part of our world that I would like everyone who wants to be part of it to feel at home as a member of our community. Let’s work together to make it happen.Your supporter and ally,Andrew",
        "summary": "*   Imposter syndrome, feeling like a fraud despite success, is common in the AI community, affecting an estimated 70 percent of people.\n*   The technical complexity of AI means everyone struggles initially; experienced practitioners also face challenges with concepts (linear/logistic regression), implementation (neural networks), and tuning (hyperparameters).\n*   Finding supportive mentors/peers and recognizing personal progress, such as understanding parts of publications like The Batch, are suggested ways to combat imposter syndrome.\n*   Encouraging and welcoming others into the AI community helps newcomers feel they belong and reduces self-doubt for those offering support.",
        "publish_date": "Sep 28, 2022",
        "keywords": [
            "Linear Regression",
            "Logistic Regression",
            "Neural Networks",
            "Hyperparameters",
            "Tuning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/imposter-syndrome-dont-let-it-hold-you-back/"
    },
    {
        "content": "Dear friends,Last week, I wrote about switching roles, industries, or both as a framework for considering a job search. If you’re preparing to switch roles (say, taking a job as a machine learning engineer for the first time) or industries (say, working in an AI tech company for the first time), there’s a lot about your target job that you probably don’t know. A technique known as informational interviewing is a great way to learnAn informational interview involves finding someone in a company or role you’d like to know more about and informally interviewing them about their work. Such conversations are separate from searching for a job. In fact, it’s helpful to interview people who hold positions that align with your interests well before you’re ready to kick off a job search.Informational interviews are particularly relevant to AI. Because the field is evolving, many companies use job titles in inconsistent ways. In one company, data scientists might be expected mainly to analyze business data and present conclusions on a slide deck. In another, they might write and maintain production code. An informational interview can help you sort out what the AI people in a particular company actually do.With the rapid expansion of opportunities in AI, many people will be taking on an AI job for the first time. In this case, an informational interview can be invaluable for learning what happens and what skills are needed to do the job well. For example, you can learn what algorithms, deployment processes, and software stacks a particular company uses. You may be surprised — if you’re not already familiar with the data-centric AI movement — to learn how much time most machine learning engineers spend iteratively cleaning datasets.Prepare for informational interviews by researching the interviewee and company in advance, so you can arrive with thoughtful questions. You might ask:What do you do in a typical week or day?What are the most important tasks in this role?What skills are most important for success?How does your team work together to accomplish its goals?What is the hiring process?Considering candidates who stood out in the past, what enabled them to shine?Finding someone to interview isn’t always easy, but many people who are in senior positions today received help when they were new from those who had entered the field ahead of them, and many are eager to pay it forward. If you can reach out to someone who’s already in your network — perhaps a friend who made the transition ahead of you or someone who attended the same school as you — that’s great! Meetups such as Pie & AI can also help you build your network.Finally, be polite and professional, and thank the people you’ve interviewed. And when you get a chance, please pay it forward as well and help someone coming up after you. If you receive a request for an informational interview from someone in the DeepLearning.AI community, I hope you’ll lean in to help them take a step up! If you’re interested in learning more about informational interviews, I recommend this article from the UC Berkeley Career Center.I’ve mentioned a few times the importance of your network and community. People you’ve met, beyond providing valuable information, can play an invaluable role by referring you to potential employers. Stay tuned for more on this topic.Keep learning!Andrew",
        "summary": "* Informational interviews are relevant for individuals switching into AI roles (e.g., machine learning engineer) or the AI industry.\n* They help clarify inconsistent AI job titles and the actual tasks performed by AI professionals within specific companies (e.g., data scientists' duties).\n* These interviews are invaluable for learning the skills, algorithms, deployment processes, software stacks, and practices (like data cleaning in data-centric AI) needed for a first AI job.",
        "publish_date": "Aug 24, 2022",
        "keywords": [
            "data-centric AI",
            "data cleaning",
            "algorithms",
            "deployment processes",
            "software stacks"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-5-job-search-fundamentals/"
    },
    {
        "content": "Dear friends, I’ve devoted several recent letters to building a career in AI. In this one, I’d like to discuss some fine points of finding a job.The typical job search follows a fairly predictable path.Research roles and companies online or by talking to friends.Optionally, arrange informal informational interviews with people in companies that appeal to you.Either apply directly or, if you can, get a referral from someone on the inside.Interview with companies that give you an invitation.Receive one or more offers and pick one. Or, if you don’t receive an offer, ask for feedback from the interviewers, the human resources staff, online discussion boards, or anyone in your network who can help you plot your next move.Although the process may be familiar, every job search is different. Here are some tips to increase the odds you’ll find a position that supports your thriving and enables you to keep growing.Pay attention to the fundamentals. A compelling resume, portfolio of technical projects, and a strong interview performance will unlock doors. Even if you have a referral from someone in a company, a resume and portfolio will be your first contact with many people who don’t already know about you. Update your resume and make sure it clearly presents your education and experience relevant to the role you want. Customize your communications with each company to explain why you’re a good fit. Before an interview, ask the recruiter what to expect. Take time to review and practice answers to common interview questions, brush up key skills, and study technical materials to make sure they are fresh in your mind. Afterward, take notes to help you remember what was said.Proceed respectfully and responsibly. Approach interviews and offer negotiations with a win-win mindset. Outrage spreads faster than reasonableness on social media, so a story about how an employer underpaid someone gets amplified, whereas stories about how an employer treated someone fairly do not. The vast majority of employers are ethical and fair, so don’t let stories about the small fraction of mistreated individuals sway your approach. If you’re leaving a job, exit gracefully. Give your employer ample notice, give your full effort through your last hour on the job, transition unfinished business as best you can, and leave in a way that honors the responsibilities you were entrusted with.Choose who to work with. It’s tempting to take a position because of the projects you’ll work on. But the teammates you’ll work with are at least equally important. We’re influenced by people around us, so your colleagues will make a big difference. For example, if your friends smoke, the odds rise that you, too, will smoke. I don’t know of a study that shows this, but I’m pretty sure that if most of your colleagues work hard, learn continuously, and build AI to benefit all people, you’re likely to do the same. (By the way, some large companies won’t tell you who your teammates will be until you’ve accepted an offer. In this case, be persistent and keep pushing to identify and speak with potential teammates. Strict policies may make it impossible to accommodate you, but in my mind, that increases the risk of accepting the offer, as it increases the odds you’ll end up with a manager or teammates who aren’t a good fit.)Get help from your community. Most of us go job hunting only a small number of times in our careers, so few of us get much practice at doing it well. Collectively, though, people in your immediate community probably have a lot of experience. Don’t be shy about calling on them. Friends and associates can provide advice, share inside knowledge, and refer you to others who may help. I got a lot of help from supportive friends and mentors when I applied for my first faculty position, and many of the tips they gave me were very helpful.I know that the job search process can be intimidating. Instead of viewing it as a great leap, consider an incremental approach. Start by identifying possible roles and conducting a handful of informational interviews. If these conversations tell you that you have more learning to do before you’re ready to apply, that’s great! At least you have a clear path forward. The most important part of any journey is to take the first step, and that step can be a small one.Keep learning!Andrew",
        "summary": "* The typical AI job search follows steps like research, optional informational interviews, applying/referrals, interviewing, and receiving offers or seeking feedback.\n* Success depends on fundamentals: a compelling resume, technical project portfolio, and strong interview performance, requiring customization and preparation.\n* Choosing colleagues is as important as projects; try to identify and speak with potential teammates before accepting an offer.\n* Leverage your community for advice, inside knowledge, and referrals, as collective experience aids individual job hunting.\n* Approach the job search incrementally, starting with research and informational interviews, viewing feedback as guidance for future steps.",
        "publish_date": "Aug 31, 2022",
        "keywords": [
            "AI job search",
            "Resume",
            "Technical project portfolio",
            "Interview performance",
            "Referrals"
        ],
        "url": "https://www.deeplearning.ai/the-batch/build-career-part-6/"
    },
    {
        "content": "Dear friends,I’ve written about how to build a career in AI and focused on tips for learning technical skills, choosing projects, and sequencing projects over a career. This time, I’d like to talk about searching for a job.A job search has a few predictable steps including selecting companies to apply to, preparing for interviews, and finally picking a job and negotiating an offer. In this letter, I’d like to focus on a framework that’s useful for many job seekers in AI, especially those who are entering AI from a different field.If you’re considering your next job, ask yourself:Are you switching roles? For example, if you’re a software engineer, university student, or physicist who’s looking to become a machine learning engineer, that’s a role switch.Are you switching industries? For example, if you work for a healthcare company, financial services company, or a government agency and want to work for a software company, that’s a switch in industries.A product manager at a tech startup who becomes a data scientist at the same company (or a different one) has switched roles. A marketer at a manufacturing firm who becomes a marketer in a tech company has switched industries. An analyst in a financial services company who becomes a machine learning engineer in a tech company has switched both roles and industries.If you’re looking for your first job in AI, you’ll probably find switching either roles or industries easier than doing both at the same time. Let’s say you’re the analyst working in financial services:If you find a data science or machine learning job in financial services, you can continue to use your domain-specific knowledge while gaining knowledge and expertise in AI. After working in this role for a while, you’ll be better positioned to switch to a tech company (if that’s still your goal).Alternatively, if you become an analyst in a tech company, you can continue to use your skills as an analyst but apply them to a different industry. Being part of a tech company also makes it much easier to learn from colleagues about practical challenges of AI, key skills to be successful in AI, and so on.If you’re considering a role switch, a startup can be an easier place to do it than a big company. While there are exceptions, startups usually don’t have enough people to do all the desired work. If you’re able to help with AI tasks — even if it’s not your official job — your work is likely to be appreciated. This lays the groundwork for a possible role switch without needing to leave the company. In contrast, in a big company, a rigid reward system is more likely to reward you for doing your job well (and your manager for supporting you in doing the job for which you were hired), but it’s not as likely to reward contributions outside your job’s scope.After working for a while in your desired role and industry (for example, a machine learning engineer in a tech company), you’ll have a good sense of the requirements for that role in that industry at a more senior level. You’ll also have a network within that industry to help you along. So future job searches — if you choose to stick with the role and industry — likely will be easier.When changing jobs, you’re taking a step into the unknown, particularly if you’re switching either roles or industries. One of the most underused tools for becoming more familiar with a new role and/or industry is the informational interview. I’ll share more about that in the next letter.Keep learning,AndrewP.S. I’m grateful to Salwa Nur Muhammad, CEO of FourthBrain (a DeepLearning.AI affiliate), for providing some of the ideas presented in this letter.",
        "summary": "*   A framework for AI job seekers, especially those entering from other fields, involves assessing whether they are switching roles (e.g., software engineer to machine learning engineer) or industries.\n*   When entering AI from a different field, it is generally easier to switch either roles or industries first, rather than attempting both simultaneously.\n*   Switching one at a time allows leveraging existing domain knowledge (switching role in same industry) or existing skills (switching industry in same role).\n*   Startups may offer an easier path for internal role switching into AI compared to larger companies due to less rigid structures.",
        "publish_date": "Aug 17, 2022",
        "keywords": [
            "machine learning engineer",
            "AI",
            "role switching",
            "industry switching",
            "job seekers"
        ],
        "url": "https://www.deeplearning.ai/the-batch/build-career-part-5/"
    },
    {
        "content": "Dear friends,  Bias in AI is a serious problem. For example, if a judge who’s deciding how to sentence a defendant relies on an AI system that routinely estimates a higher risk that offenders of a particular race will reoffend, that’s a terrible thing. As we work to reduce bias in AI models, though, it’s also worth exploring a different issue: inconsistency. Specifically, let’s consider how inconsistent human decisions are, and how AI can reduce that inconsistency.If a human judge, given two defendants who committed the same crime under identical circumstances, sentences one to three years in prison and the other to 30 days, we would consider this inconsistency blatantly unfair. Yet, as Daniel Kahneman and his co-authors document in their book, Noise: A Flaw in Human Judgment, human decision-making is extremely inconsistent (or noisy).One study found that judges systematically sentenced defendants more harshly if the local football team had suffered an upset loss (which presumably made the judge cranky). Judges are only human, and if they’re swayed by football outcomes, imagine how many other irrelevant factors may influence their decisions!Many human decisions rest on complex criteria, and humans don’t always define their criteria before weighing them. For example:In medicine, I’ve seen individual doctors make highly inconsistent diagnoses given the same input. Working on a project with a doctor whom I’ll call Alice, we measured the “inter-Alice agreement score,” which was loosely a measure of how much her diagnoses differed between morning and afternoon. (For the record, Alice is a brilliant doctor and wonderful collaborator. This score measured the inherent ambiguity of the task more than it measured her competence.)In manufacturing, I’ve seen skilled inspectors make very different decisions about whether or not parts with similar flaws were defective.In online retailing, I’ve seen human annotators make inconsistent decisions about how to tag or categorize products. (Should a fun gadget go under electronics or entertainment?)In contrast, given the same input, a trained neural network will produce the same output every time. Given similar inputs, a trained model will also typically output similar results. Automated software tends to be highly consistent. This is one of automation’s huge advantages: Algorithms make decisions much more consistently than humans. To my mind, they offer a way to give patients more consistent and fair treatment options, make manufacturing more efficient, make retail product catalogs less confusing to shoppers, and so on.In conversations about whether and how to build an AI system, it’s important to address how to ensure that the system doesn’t have significant bias as well as how to benchmark its bias against human bias. If you’re trying to get an AI project approved, you may find it useful raise the issue of consistency as well. Measuring the consistency of your algorithm relative to humans who make the same decision can add weight to arguments in favor of investing in an automated system.Keep learning!Andrew",
        "summary": "*   Bias in AI is a serious problem (e.g., in judicial sentencing).\n*   Human decision-making is extremely inconsistent (\"noisy\"), influenced by irrelevant factors (e.g., judges, doctors, inspectors, annotators).\n*   In contrast, trained neural networks and automated software are highly consistent, producing the same output for the same input.\n*   AI offers a way to reduce inconsistency compared to human decisions, potentially leading to more consistent outcomes in areas like medicine and manufacturing.\n*   When developing AI, it's important to address bias (benchmarking against human bias) and measure the algorithm's consistency relative to humans.",
        "publish_date": "Aug 10, 2022",
        "keywords": [
            "Neural networks",
            "Bias",
            "Inconsistency",
            "Benchmarking",
            "Automated decision-making"
        ],
        "url": "https://www.deeplearning.ai/the-batch/decision-making/"
    },
    {
        "content": "Dear friends,Many AI systems have been built using data scraped from the internet. Indeed, even the cornerstone dataset for computer vision research, ImageNet, was built using images taken from the public internet. With the rise of data-centric AI, access to good data continues to grow in importance to developers.What are the limits for scraping and using public data? Earlier this year, a United States court ruled that scraping data from websites that don’t take measures to hide it from public view doesn’t violate a law designed to thwart hackers. I believe this is a positive step for AI as well as competition on the internet, and I hope it will lead to further clarity about what is and isn’t allowed.Many companies aim to create so-called walled gardens in which they provide exclusive access to content — even though it may be visible to all — such as social media posts or user résumés (the data at the heart of the ruling). But such data is valuable to other companies as well. For example, while LinkedIn helps users display their résumés to professional contacts, other companies might use this data to recruit potential employees, predict whether employees are likely to leave their current positions (updating a résumé is a sign), or find sales leads. Scraping the web was important in the early days of the internet to make web search viable, but as new uses come up — such as using machine learning to generate novel insights — clear rules about which data can and can’t be used, and how, become even more important.This isn’t a simple matter. There is a fine line between protecting copyright, which incentivizes businesses to create that data, and making data widely available, which enables others to derive value from it. In addition, freely available data can be abused. For example, some face recognition companies have been especially aggressive in scraping face portraits, building systems that invade privacy.The U.S. court found that scraping data that is publicly accessible doesn’t violate the Computer Fraud and Abuse Act. This is not the same as allowing unfettered access to web scrapers. Data held behind a login wall or accessible only after agreeing to restrictive terms of service may be a different matter. (Disclaimer: Please don’t construe anything I say as legal advice.)While this ruling may hurt companies that have built businesses on data that is fully visible to the public, overall I view it as a positive step. It will increase the free flow of information and make it easier for teams to innovate in AI and beyond. Also, knocking down part of the wall that surrounds walled gardens should increase competition on the internet. On the other hand, it increases the incentives to put data behind a login wall, where it’s no longer publicly accessible.The issues of open versus closed data aren’t new. With the rise of mobile apps over a decade ago, web search companies worried that data would be locked within mobile apps rather than accessible on the web. This is one reason why Google invested in the Android mobile operating system as a counterweight to Apple’s iOS. Although ideas about which data should be accessible continue to shift, I continue to believe that a more open internet will benefit more people. With the rise of AI, algorithms — in addition to people — are hungry to see this data, making it even more important to ensure relatively free access.Keep learning!Andrew",
        "summary": "*   Many AI systems, including foundational datasets like ImageNet, are built using data scraped from the public internet.\n*   A US court ruled that scraping publicly visible data doesn't violate the Computer Fraud and Abuse Act, a decision viewed as positive for AI innovation and data flow, though it may incentivize data being put behind login walls.\n*   Data-centric AI increases the importance of access to data like publicly visible résumés (used for recruiting or predicting employee departure), highlighting the need for clear rules on data usage for AI that balance access, copyright, and preventing abuse such as privacy invasion by face recognition systems.",
        "publish_date": "Jul 27, 2022",
        "keywords": [
            "ImageNet",
            "Data-centric AI",
            "face recognition systems",
            "data scraping",
            "foundational datasets"
        ],
        "url": "https://www.deeplearning.ai/the-batch/more-data-for-ai-developers-a-new-law-makes-it-easier-to-scrape-the-web/"
    },
    {
        "content": "Dear friends,While working on Course 3 of the Machine Learning Specialization, which covers reinforcement learning, I was reflecting on how reinforcement learning algorithms are still quite finicky. They’re very sensitive to hyperparameter choices, and someone experienced at hyperparameter tuning might get 10x or 100x better performance. Supervised deep learning was equally finicky a decade ago, but it has gradually become more robust with research progress on systematic ways to build supervised models.Will reinforcement learning (RL) algorithms also become more robust in the next decade? I hope so. However, RL faces a unique obstacle in the difficulty of establishing real-world (non-simulation) benchmarks.When supervised deep learning was at an earlier stage of development, experienced hyperparameter tuners could get much better results than less-experienced ones. We had to pick the neural network architecture, regularization method, learning rate, schedule for decreasing the learning rate, mini-batch size, momentum, random weight initialization method, and so on. Picking well made a huge difference in the algorithm’s convergence speed and final performance.Thanks to research progress over the past decade, we now have more robust optimization algorithms like Adam, better neural network architectures, and more systematic guidance for default choices of many other hyperparameters, making it easier to get good results. I suspect that scaling up neural networks — these days, I don’t hesitate to train a 20 million-plus parameter network (like ResNet-50) even if I have only 100 training examples — has also made them more robust. In contrast, if you’re training a 1,000-parameter network on 100 examples, every parameter matters much more, so tuning needs to be done much more carefully.My collaborators and I have applied RL to cars, helicopters, quadrupeds, robot snakes, and many other applications. Yet today’s RL algorithms still feel finicky. Whereas poorly tuned hyperparameters in supervised deep learning might mean that your algorithm trains 3x or 10x more slowly (which is bad), in reinforcement learning, it feels like they might result in training 100x more slowly — if it converges at all! Similar to supervised learning a decade ago, numerous techniques have been developed to help RL algorithms converge (like double Q learning, soft updates, experience replay, and epsilon-greedy exploration with slowly decreasing epsilon). They’re all clever, and I commend the researchers who developed them, but many of these techniques create additional hyperparameters that seem to me very hard to tune.Further research in RL may follow the path of supervised deep learning and give us more robust algorithms and systematic guidance for how to make these choices. One thing worries me, though. In supervised learning, benchmark datasets enable the global community of researchers to tune algorithms against the same dataset and build on each other’s work. In RL, the more-commonly used benchmarks are simulated environments like OpenAI Gym. But getting an RL algorithm to work on a simulated robot is much easier than getting it to work on a physical robot.Many algorithms that work brilliantly in simulation struggle with physical robots. Even two copies of the same robot design will be different. Further, it’s infeasible to give every aspiring RL researcher their own copy of every robot. While researchers are making rapid progress on RL for simulated robots (and for playing video games), the bridge to application in non-simulated environments is often missing. Many excellent research labs are working on physical robots. But because each robot is unique, one lab’s results can be difficult for other labs to replicate, and this impedes the rate of progress.I don’t have a solution to these knotty issues. But I hope that all of us in AI collectively will manage to make these algorithms more robust and more widely useful.Keep learning!Andrew",
        "summary": "*   RL algorithms are currently finicky and highly sensitive to hyperparameter tuning, potentially leading to 100x slower training or failure to converge, unlike more robust supervised deep learning.\n*   Techniques aiding RL convergence (e.g., double Q learning, soft updates, experience replay, epsilon-greedy exploration) often introduce additional, hard-to-tune hyperparameters.\n*   A key obstacle for RL robustness research is the difficulty of establishing real-world (non-simulation) benchmarks, unlike supervised learning's benefit from shared datasets.\n*   RL algorithms successful in simulation (e.g., OpenAI Gym) often struggle with physical robots, whose uniqueness impedes replication and collaborative progress.",
        "publish_date": "Aug 03, 2022",
        "keywords": [
            "hyperparameter tuning",
            "double Q learning",
            "experience replay",
            "epsilon-greedy exploration",
            "OpenAI Gym"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-trouble-with-reinforcement-learning/"
    },
    {
        "content": "Dear friends,The population of Earth officially reached 8 billion this week. Hooray! It’s hard to imagine what so many people are up to. While I hope that humanity can learn how to leave only gentle footprints on the planet, I’m excited about the creativity and inventiveness that a growing human population can bring.One measure of human progress is the dwindling percentage of people involved in agriculture. If a smaller fraction of the population can generate enough calories to feed everyone, more people will have time to build houses, care for the sick, create art, invent new technologies, and do other things that enrich human life.Today, roughly 1.5 percent of U.S. jobs are in farming, which enables most of us here to pursue other tasks. Still, a lot of people are involved in various forms of routine, repetitive work. Just as the agricultural workforce fell over centuries from a majority of the population to a tiny minority, AI and automation can free up more people from repetitive work.This is important because we need lots of people to work on the hard tasks ahead of us. For instance, deep learning could not have reached its current state without a large community building on one another’s work and pushing ideas forward. Building applications that will improve human lives requires even more people. Semiconductors are another example: Building a modern chip requires clever effort by many thousands of people, and building the breakthroughs that increase processing power and efficiency as Moore’s Law fades will take even more. I’d like to see a lot more people pushing science and technology forward to tackle problems in energy, health care, justice, climate change, and artificial general intelligence.I love humanity. We must do better to minimize our environmental impact, but I’m happy that so many of us are here: more friends to make, more people to collaborate with, and more of us to build a richer society that benefits everyone!Keep learning!Andrew",
        "summary": "*   AI and automation can free up people from repetitive work, similar to how the agricultural workforce declined.\n*   Deep learning reached its current state through a large community building on one another's work.\n*   More people are needed to push science and technology forward to tackle problems, including artificial general intelligence.",
        "publish_date": "Nov 16, 2022",
        "keywords": [
            "Deep learning",
            "artificial general intelligence",
            "automation",
            "AI",
            "community"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-growing-global-population-brings-more-opportunities-to-make-the-world-a-better-place/"
    },
    {
        "content": "Dear friends,The economic downturn of the past six months has hit many individuals and companies hard, and I’ve written about the impact of rising interest rates on AI. The effects of high inflation, the Russian war in Ukraine, and an economic slowdown in China are rippling across the globe. Even though unemployment in the U.S. is low, within the tech world, I continue to hear things that point to the possibility that we might go through a challenging time for many months to come.The layoffs at Twitter and Meta are well publicized. Anecdotally, I’ve heard many worrisome stories: Students are having a hard time finding internships for next summer, entrepreneurs are having greater difficulty raising capital, companies are freezing hiring and reducing headcount, and employees are facing effective pay cuts as falling share prices reduce the value of their stock-based compensation. Some managers have told me they want to preserve their machine learning teams — which they hired with great difficulty — but the tech market has cooled and likely will take a while to pick up.What can we do amid the turmoil? Even if the tech world slumps, the long-term value of AI is still clear to me, and it’s worth lifting our eyes toward the future to position ourselves for the eventual rebound.I’d like to draw attention to three investments that I believe will retain or increase their value even in uncertain times. If you’re wondering where to put your effort, attention, or money, consider these areas:Deep technology. AI technologies from programming frameworks like TensorFlow and PyTorch to algorithmic breakthroughs like transformers and diffusion models have deep and long-lasting value. Deep tech is difficult to build, and it transforms the way we do AI. I’m continuing to work on deep tech in data-centric AI. Collectively we should keep working to build deep tech, and I’m confident that the long-term benefits to society will be profound.Training. During a bumpy job market, many people stay in school longer (if they can afford it) in the hope of graduating into a healthier job market. Real expertise in technology will continue to hold tremendous value because it helps you to shape the future. So if you’re not sure what to invest in, keep investing in your own technical skills. Wherever the world ends up, people with deep technical skill in AI will be in demand.Community. Having lived in different places, I’ve seen first-hand how some cities have strong communities, where neighbors watch out for each other and lend a helping hand when people are down on their luck, and weak ones, where hardly anyone knows anyone else, and falling sick means having to take care of yourself. The AI community has always been stronger together. If we can step back from wondering how to build our next project or get that promotion and, instead, ask how we can help others around us, the investment in human relationships will have tremendous value.Whether or not the economic downturn affects you, I’m here to support you. As we sail through a potentially tough time in the coming months, remember that the long-term impact of AI has been and will continue to be huge. Let’s keep helping each other and investing in things that will make us stronger for when the world exits its current slump.Keep learning!Andrew",
        "summary": "*   The economic downturn is impacting the tech world, including AI, leading to layoffs (Twitter, Meta), hiring freezes, reduced headcount, and challenges for students seeking internships and entrepreneurs raising capital.\n*   Despite the economic downturn, the long-term value of AI is clear, with managers seeking to preserve machine learning teams.\n*   Investing in deep technology, such as AI frameworks (TensorFlow, PyTorch) and algorithmic breakthroughs (transformers, diffusion models), is recommended for its long-lasting value and transformative impact on AI.\n*   Investing in developing deep technical skills in AI is crucial, as individuals with such expertise will remain in demand despite the bumpy job market.\n*   Strengthening the AI community through mutual support is highlighted as a valuable investment.",
        "publish_date": "Nov 09, 2022",
        "keywords": [
            "TensorFlow",
            "PyTorch",
            "transformers",
            "diffusion models",
            "deep learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-to-do-in-a-tough-economy/"
    },
    {
        "content": "Dear friends,A new report from UN Climate Change says that the world might be on track for 2.5 °C of warming by the end of the century, a potentially catastrophic level of warming that’s far above the 1.5 °C target of the 2015 Paris Agreement. I think it is time to seriously consider a specific solution in which AI can play a meaningful role: Climate geoengineering via stratospheric aerosol injection.Stratospheric aerosol injection involves spraying fine particles that reflect sunlight high in the atmosphere. By increasing the reflectivity (or albedo) of the planet, we can slow down the rate at which sunlight warms it, and thereby buy more time to reduce carbon emissions and develop mitigations. Harvard Professor David Keith explains the science behind this idea is in his book, A Case for Climate Engineering.AI will be important in this effort because:The aerosols will likely be delivered via custom aircraft. Designing the specs for and autonomously piloting high-altitude drones falls well within AI capabilities.The details of the aerosols’ impact on the planet’s climate are still poorly understood. Average temperature should decrease, but will some regions cool faster? Will some continue to warm? How will this affect crops, rain acidity, wind currents, and myriad other factors? Machine learning will be critical for modeling the effects.In light of the likely impact of stratospheric aerosols on the climate as well as their potential for disparate impact, how can we decide which aerosols to use, where, and when in a way that’s equitable and improves the welfare of the planet as a whole? Optimization techniques akin to reinforcement learning could be useful.Stratospheric aerosol injection has been criticized on the following grounds:Moral hazard: Doing this will reduce the incentive to reduce carbon emissions. This is true, just as requiring seat belts reduces the incentive to drive safely. Nonetheless, we’re better off with seatbelts.Unforeseen risks: How can we attempt something as risky as modifying the planet? What if it goes wrong? But we already have modified the planet, and it already has gone wrong. Let’s do it intentionally this time, with careful science that enables us to take baby steps that are safe.At the current 1.1 °C of warming, the world is already experiencing increased climate-related crises. My heart goes out to the millions whose lives have been disrupted by wildfires, flooding, hurricanes, and typhoons. Just weeks ago, a forest fire came within miles of my house, and area residents were told to be ready to evacuate, a first for me. (Fortunately, the fire has since been largely contained.) It terrifies me that on the planet’s current path, the past summer’s climate — the worst I’ve experienced — might be better than what my children and I will experience for the rest of our lives.Next week at the UN’s annual COP27 climate summit held in Egypt, government leaders will meet to discuss new agreements aimed at reducing atmospheric carbon emissions. While I hope that this meeting summons the global will to do what’s needed, I would rather count on engineers and scientists, not just politicians, to address the problem. Perhaps some of us in AI can make a critical difference.Stay cool,Andrew",
        "summary": "*   AI capabilities include designing specs for and autonomously piloting high-altitude drones for stratospheric aerosol delivery.\n*   Machine learning will be critical for modeling the poorly understood effects of stratospheric aerosols on the planet's climate.\n*   Optimization techniques akin to reinforcement learning could be useful for deciding which aerosols to use, where, and when for equitable outcomes.",
        "publish_date": "Nov 02, 2022",
        "keywords": [
            "Autonomous piloting",
            "Reinforcement learning",
            "Climate modeling",
            "Optimization techniques",
            "Planning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-ai-can-help-counteract-climate-change/"
    },
    {
        "content": "Dear friends,Recent successes with large language models have brought to the surface a long-running debate within the AI community: What kinds of information do learning algorithms need in order to gain intelligence?The vast majority of human experience is not based on language. The taste of food, the beauty of a sunrise, the touch of a loved one — such experiences are independent of language. But large language models have shown that it’s possible to capture a surprisingly rich facsimile of human experiences by consuming far more language than any human can in a lifetime.Prior to recent advances in large language models, much of the AI community had viewed text as a very limited source of information for developing general-purpose intelligence. After all, animals evolved intelligence without language. Intelligence includes perceiving the world through sight, sound, and other senses; knowing how to move our bodies; having a common-sense understanding of physics, such as how to knock a fruit off a high tree; and being able to plan simple actions to find food, shelter, or a mate. Writing is a relatively recent invention that dates back only around 5,500 years. Spoken language arose roughly 100,000 years ago. In contrast, mammals have been around for around 200 million years.If AI development were to follow the path of evolution, we would start by trying to build insect-level intelligence, then mouse-level intelligence, perhaps followed by dog-level, monkey-level, and finally human-level. We would focus on tasks like vision and psychomotor skills long before the ability to use language.But models like ChatGPT show that language, when accessed at massive scale, overcomes many of its limitations as a source of information. Large language models can learn from more words — several orders of magnitude more! — than any individual human can.In a typical year, a child might hear around 10 million words (with huge variance depending on factors such as the family). So, by age 10, the child might have heard 100 million words.If you read 24/7 for a year at a rate of 250 words per minute, you’d read about 130 million words annually.GPT-3 was trained on about 500,000 million words.An individual human would need dozens of lifetimes spent doing nothing but reading to see the number of words that GPT-3 considered during its training. But the web aggregates text written for or by billions of individuals, and computers have ready access to much of it. Through this data, large language models (LLMs) capture a wealth of knowledge about the human experience. Even though an LLM has never seen a sunrise, it has read enough text about sunrises to describe persuasively what one looks like.So, even though language is a small part of human experience, LLMs are able to learn a huge amount of information about the world. It goes to show that there are multiple paths to building intelligence, and that the path followed by evolution or human children may not be the most efficient way for an engineered system.Seeing the entire world only through the lens of text — as rich as it turns out to be, and as valuable as systems trained on text have become — is still ultimately an impoverished world compared to the one we live in. But relying on text alone has already taken us quite far, and I expect this direction to lead to exciting progress for years to come.Keep learning!Andrew",
        "summary": "*   Recent successes with large language models (LLMs) have reignited the debate on the necessary information for AI intelligence, demonstrating that massive-scale language consumption can capture a rich facsimile of human experience, challenging prior views on text limitations.\n*   LLMs like GPT-3 are trained on significantly more language data (~500,000 million words) than humans process (~100 million by age 10, ~130 million annually reading 24/7).\n*   Learning from massive text allows LLMs to gain extensive knowledge about the world, suggesting alternative paths to building intelligence compared to evolution or human development.\n*   Despite relying solely on text providing an impoverished view compared to the real world, this approach has led to significant AI progress.",
        "publish_date": "Feb 01, 2023",
        "keywords": [
            "GPT-3",
            "LLMs",
            "Massive-scale language consumption",
            "Relying solely on text",
            "Language data"
        ],
        "url": "https://www.deeplearning.ai/the-batch/training-on-massive-amounts-of-text-offsets-lack-of-exposure-to-other-data/"
    },
    {
        "content": "Dear friends,In late December, Google reportedly issued a “code red” to raise the alarm internally to the threat of disruption of its business by large language models like OpenAI’s ChatGPT.Do large language models (LLMs) endanger Google's search engine business? I think there’s a path for them to transform the way we access information, albeit one that poses technical and business hurdles.What if, rather than searching the web, we could query an LLM and get an answer? We would receive not a page of web links but a piece of text that answered our query. This appears to work for basic factual questions, but for questions that require complex reasoning or specialized knowledge, today’s LLMs may confidently hallucinate an answer, making the result misleading.Here’s one way to think about the problem. ChatGPT’s predecessor GPT-3 has 175 billion parameters. Using 16-bit, floating-point bytes, it would take around 350GB to store its parameters (many reports say 800GB). In comparison, Wikipedia occupies about 150GB (50GB for text, 100GB for images). While the comparison is far from apples to apples, the fact that an LLM has more memory than is needed to store Wikipedia suggests its potential to store knowledge.But even Wikipedia contains a minuscule fraction of the knowledge available on the internet, which by some estimates amounts to 5 billion GB. Thus search, which can point us to pages from all corners of the web, can answer many questions that an LLM with fixed memory can't.That said, I see significant potential in another technology, retrieval augmented generation. Rather than relying on a fixed LLM to deliver the answer to a query, if we first find relevant documents (online or elsewhere) and then use an LLM to process the query and the documents into an answer, this could provide an alternative to current web search. Executing this efficiently and at scale would be complex, but the effect would be akin to having an LLM do a web search and summarize the results. Examples of this approach include Meta's Atlas and DeepMind's RETRO.While today's search engine giants are well positioned to execute on this technology, their businesses depend on users clicking on ads placed next to search results. If they were to deliver text that answered a query, where would ads fit into the picture? Google would need to solve that problem before it could replace traditional web search with LLMs. Search startups that don’t have as much to lose — or perhaps Microsoft’s Bing, which is the second most-popular search engine by some reckonings — may be more willing to embrace upheavals in the search-engine business model.Of course, Google's business has many moats, or defenses. The company's control over the Chrome web browser and Android mobile operating system channels users to its search engine. Having a platform with many advertisers and a sophisticated ad system also enables Google to monetize user attention better than competitors. Thus, it can pay more for search traffic to, say, incentivize makers of web browsers to make it the default search engine.It's fascinating that generative AI is already so powerful that Google declared an emergency. How exciting to live in a time when we can be part of this evolution of AI!Keep learning,Andrew",
        "summary": "*   Large language models (LLMs) like ChatGPT pose a threat to traditional search engines by potentially providing direct answers, though current models may \"hallucinate\" for complex queries.\n*   GPT-3 has 175 billion parameters (~350GB/800GB storage), which is minuscule compared to the estimated 5 billion GB of internet knowledge.\n*   Retrieval augmented generation (RAG), exemplified by Meta's Atlas and DeepMind's RETRO, is proposed as an alternative where LLMs process pre-found relevant documents.\n*   Implementing RAG efficiently at scale is complex, and the business model of search engines (ads) presents a hurdle for its adoption.",
        "publish_date": "Jan 18, 2023",
        "keywords": [
            "Retrieval augmented generation (RAG)",
            "ChatGPT",
            "GPT-3",
            "Atlas",
            "RETRO"
        ],
        "url": "https://www.deeplearning.ai/the-batch/chatgpt-and-other-llm-could-disrupt-googles-business/"
    },
    {
        "content": "Dear friends,One of the dangers of large language models (LLMs) is that they can confidently make assertions that are blatantly false. This raises worries that they will flood the world with misinformation. If they could moderate their degree of confidence appropriately, they would be less likely to mislead.People are prone to following authority figures. Because a lot of text on the internet is written in an authoritative style — hopefully because the authors know what they’re talking about— LLMs have learned to mimic this style. Unfortunately, LLMs can speak in this style even when they get the facts completely wrong.We don’t expect people to be right all the time, but we don’t like it when they’re simultaneously confident and wrong. Real experts speak in a range of styles: confident when we know what we’re talking about, but also explaining the boundaries of our knowledge when we run up against them and helping the audience understand the range of possibilities. For example, when asked how to build an AI application, I might propose one approach but also describe the range of algorithms one might consider. Knowing what you know and don’t know is a useful trait of expertise.Playing with ChatGPT, the latest language model from OpenAI, I found it to be an impressive advance from its predecessor GPT-3. Occasionally it says it can’t answer a question. This is a great step! But, like other LLMs, it can be hilariously wrong. Work lies ahead to build systems that can express different degrees of confidence.For example, a model like Meta’s Atlas or DeepMind’s RETRO that synthesizes multiple articles into one answer might infer a degree of confidence based on the reputations of the sources it draws from and the agreement among them, and then change its communication style accordingly. Pure LLMs and other architectures may need other solutions.If we can get generative algorithms to express doubt when they’re not sure they’re right, it will go a long way toward building trust and ameliorating the risk of generating misinformation.Keep learning!Andrew",
        "summary": "*   Large language models (LLMs) can confidently make false assertions, raising concerns about flooding the world with misinformation.\n*   LLMs mimic authoritative styles learned from internet text, leading them to speak confidently even when factually incorrect.\n*   A key challenge is for LLMs to moderate their confidence and express doubt when unsure, similar to human experts.\n*   Models that synthesize information from multiple sources (e.g., Meta’s Atlas, DeepMind’s RETRO) could potentially infer confidence based on source reputation and agreement.\n*   Enabling generative algorithms to express doubt when unsure is crucial for building trust and ameliorating the risk of generating misinformation.",
        "publish_date": "Dec 07, 2022",
        "keywords": [
            "Atlas",
            "RETRO",
            "Synthesizing information from multiple sources",
            "Confidence",
            "Large language models (LLMs)"
        ],
        "url": "https://www.deeplearning.ai/the-batch/large-language-models-like-chatgpt-need-a-way-to-express-different-degrees-of-confidence/"
    },
    {
        "content": "Dear friends,As we enter the new year, let’s view 2023 not as a single year, but as the first of more in which we will accomplish our long-term goals. Some results take a long time to achieve, and even though we may take actions that bring those results closer, we can do it more effectively if we envision a path rather than simply going from milestone to milestone.When I was younger, I hardly connected short-term actions concretely to long-term outcomes. I would focus on the next homework assignment, project, or research paper with a vague 10-year goal, lacking a clear path to get there. With experience, I got better at seeing how these efforts could lead to goals that can be achieved only in years.For instance, 10 years ago, I built my first machine learning course one week at a time (often filming at 2 a.m.). Building the updated Machine Learning Specialization this year, I was able to plan the full course better (and while some filming was still done at  2 a.m., there was less!). In previous businesses, I tended to build a product and only then think about how to take it to customers. These days, I’m more likely to see the big picture even when starting out.Feedback from friends and mentors can help you shape your vision. A big step in my growth was learning to trust advice from certain experts and mentors — even when I didn’t follow their reasoning — and work hard to understand it. For example, my friends who are experts in global geopolitics sometimes advise me to invest more heavily in particular countries. I would not have come to this conclusion by myself, because I don’t know those countries well. But I’ve learned to explain my long-term plan, solicit their feedback, and listen carefully when they point me in a different direction.Right now, one of my top goals is to democratize the creation of AI. Having a lot more people able to build custom AI systems will lift up many people. While the path to accomplishing this is long and hard, I can see the steps to get there, and the critiques of friends and mentors have shaped my thinking significantly.As 2023 approaches, how far into the future can you make plans? Do you want to achieve expertise in a topic, advance your career, or solve a technical problem? By forming a hypothesis of the path — even an untested one — and soliciting feedback to test and refine it, I hope you can shape a vision that inspires and drives you forward.Dream big for 2023 and beyond!Happy new year,Andrew",
        "summary": "* The author built a first machine learning course 10 years ago and an updated Machine Learning Specialization this year.\n* One of the author's top goals is to democratize the creation of AI.\n* This goal involves enabling a lot more people to build custom AI systems.",
        "publish_date": "Dec 28, 2022",
        "keywords": [
            "Machine Learning",
            "AI",
            "AI Systems",
            "Custom AI Systems",
            "Machine Learning Specialization"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-achieve-your-long-term-goals/"
    },
    {
        "content": "Dear friends,As you can read in this issue of The Batch, generative AI companies are being sued over their use of data (specifically images and code) scraped from the web to train their models. Once trained, such models can generate, on demand, images in a given artist’s style or code that executes particular tasks.The lawsuits will answer the question of whether using publicly available data to train generative models is legal, but I see an even more important question: Is it fair? If society has a point of view on what is fair, we can work to make laws that reflect this.To be clear, this issue is much bigger than generative AI. The fundamental question is whether AI systems should be allowed to learn from data that’s freely available to anyone with an internet connection. But the focus right now is on models that generate images and code.Today, we routinely advise students of computer programming to read — and perhaps contribute to — open source code. Reading open source no doubt inspires individuals to write better code. No one questions whether this is fair. After all, it’s how people learn. Is it fair for a computer to do the same?The last time I visited the Getty Museum in Los Angeles, California, I saw aspiring artists sitting on the floor and copying masterpieces on their own canvases. Copying the masters is an accepted part of learning to be an artist. By copying many paintings, students develop their own style. Artists also routinely look at other works for inspiration. Even the masters whose works are studied today learned from their predecessors. Is it fair for an AI system, similarly, to learn from paintings created by humans?Of course, there are important differences between human learning and machine learning that bear on fairness. A machine learning model can read far more code and study far more images than a human can. It can also generate far more code or images, far more quickly and cheaply, than even the most skilled human.These differences raise serious issues for artists, coders, and society at large:Production of creative works by a machine may devalue the work of human creators.Generative models can reproduce the personal style of artists whose work they were trained on without compensating those artists.Such models may have been trained on proprietary data that was not intended to be available on the internet (such as private images that were stolen or leaked).On the other hand, generative models have tremendous potential value. They’re helping people who are not skilled artists to create beautiful works, spurring artists to collaborate with computers in new ways, and automating workaday tasks so humans can focus on higher-level creativity. Furthermore, advances in AI build upon one another, and progress in generative AI brings progress in other areas as well.The upshot is that we need to make difficult tradeoffs between enabling technological progress and respecting the desire to protect creators’ livelihoods. Thoughtful regulation can play an important role. One can imagine potential regulatory frameworks such as:Establishing a consistent way for creators to opt outMandating compensation for artists when AI systems use their dataAllocating public funding to artists (like using tax dollars to fund public media such as the BBC)Setting a time limit, like copyright, after which creative works are available for AI trainingWhat a society views as fair can change. In the United States, once it was considered fair that only certain men could vote. When society’s view on this changed, we changed the rules.Society currently has divergent views on what is fair for AI to do. Given the bounty offered by generative AI (and other AI systems), and acknowledging the need to make sure that creators are treated fairly, I hope we find a path forward that allows AI to continue to develop quickly for the benefit of all.Keep learning!Andrew",
        "summary": "* Generative AI companies are facing lawsuits over using web-scraped data (specifically images and code) to train their models.\n* The central question raised is the fairness of AI systems learning from data freely available online, contrasting with human learning but noting differences in scale and speed.\n* Generative models raise issues such as potentially devaluing human creative work, reproducing artists' styles without compensation, and the possible use of proprietary or leaked data.\n* Potential regulatory frameworks being considered include creator opt-out mechanisms, mandatory compensation for data use, and setting time limits for data availability for AI training.",
        "publish_date": "Feb 08, 2023",
        "keywords": [
            "web-scraped data",
            "images",
            "code",
            "reproducing artists' styles",
            "proprietary data"
        ],
        "url": "https://www.deeplearning.ai/the-batch/should-ai-be-allowed-to-learn-from-data-thats-freely-available-to-humans/"
    },
    {
        "content": "Dear friends,Last week, Facebook’s parent company Meta released a demo of Galactica, a large language model trained on 48 million scientific articles. Two days later, amid controversy regarding the model’s potential to generate false or misleading articles, the company withdrew it.Is Galactica dangerous? How should researchers, as well as the broader AI community, approach such developments?Michael Black, director of the Max Planck Institute for Intelligent Systems, raised concern about Galactica’s potential for harm by generating seemingly authoritative scientific papers that are factually bonkers. Meta chief AI scientist Yann LeCun vigorously defended the model. He pointed out that, despite worries that people might misuse large language models (LLMs), it largely hasn’t happened.At the risk of offending both sides, let me share my take.I support the Galactica researchers. Their scientific work on large language models is technically interesting and impressive. Their model does well on tasks such as mathematical reasoning and answering multiple-choice questions.When a technology shows potential to cause significant harm, it’s important to carefully assess the likely benefits against the likely harm. One problem with the way Galactica was released is that we don’t yet have a robust framework for understanding of the balance of benefit versus harm for this model, and different people have very different opinions. Reading through the paper, I see potential for exciting use cases. I also see risk of large-scale fakery that could cause harm. While I support the technical work, I would prefer that the demo had been released only after a more thorough assessment.Prior to a careful analysis of benefit versus harm, I would not recommend “move fast and break things” as a recipe for releasing any product with potential for significant harm. I would love to see more extensive work — perhaps through limited-access trials — that validates the product’s utility to third parties, explores and develops ways to ameliorate harm, and documents this thinking clearly.That said, I would also love to see less vitriol toward researchers who are trying to do their best. People will differ on the best path forward, and all of us sometimes will be right and sometimes will be wrong. I believe the Meta researchers are trying to do their best. Whether we agree or disagree with their approach, I hope we’ll treat them with respect.Part of the disagreement likely stemmed from widespread distrust of Meta, where a focus on maximizing user engagement has contributed to social polarization and spread of disinformation. If a lesser-known or more-trusted company had released Galactica, I imagine that it would have had more leeway. For instance, Stability AI released its Stable Diffusion text-to-image model with few safeguards. The company faced little criticism, and so far the model has spurred great creativity and little harm. I don’t think this is necessarily an unfair way to approach companies. A company’s track record does matter. Considering the comparatively large resources big companies can use to drive widespread awareness and adoption of new products, it’s reasonable to hold them to a higher standard.The authors withdrew the model shortly after the controversy arose. Kudos to them for acting in good faith and responding quickly to the community’s concerns.When it comes to building language models that generate more factually accurate output, the technical path forward is not yet clear. LLMs are trained to maximize the likelihood of text in their training set. This leads them to generate text that sounds plausible — but a LLM that makes up facts can also perform well on this training objective.Some engineers (including the Galactica’s team) have proposed that LLMs could be an alternative to search engines. For example, instead of using search to find out the distance to the Moon, why not pose the question as a prompt to a language model and let it answer? Unfortunately, the maximum-likelihood objective is not well aligned with the goal of providing factually accurate information. To make LLMs better at conveying facts, research remains to be done on alternative training objectives or, more likely, model architectures that optimize for factual accuracy rather than likelihood.Whether a tool like Galactica will be more helpful or harmful to society is not yet clear to me. There will be bumps in the rollout of any powerful technology. The AI community has produced racist algorithms, toxic chatbots, and other problematic systems, and each was a chance to learn from the incident and get better. Let’s continue to work together as a community, get through the bumps with respect and support for one another, and keep building software that helps people.Keep learning!Andrew",
        "summary": "*   Meta released Galactica, a large language model (LLM) trained on 48 million scientific articles, but withdrew it two days later amid controversy over generating false or misleading articles.\n*   Debate arose regarding Galactica's potential for harm by generating \"factually bonkers\" papers versus the argument that misuse of LLMs hasn't largely happened.\n*   Galactica performed well on tasks like mathematical reasoning and answering multiple-choice questions, but its release lacked a robust framework for assessing the balance of benefit versus harm.\n*   A technical challenge for LLMs is that the maximum-likelihood training objective leads to plausible but potentially factually inaccurate text, requiring research into alternative objectives or architectures for factual accuracy.",
        "publish_date": "Nov 23, 2022",
        "keywords": [
            "Galactica",
            "maximum-likelihood training objective",
            "architectures",
            "mathematical reasoning",
            "multiple-choice questions"
        ],
        "url": "https://www.deeplearning.ai/the-batch/meta-released-and-quickly-withdrew-a-demo-of-its-galactica-language-model/"
    },
    {
        "content": "Dear friends,On Monday, the European Union fined Meta roughly $275 million for breaking its data privacy law. Even though Meta’s violation was not AI specific, the EU’s response is a reminder that we need to build AI systems that preserve user privacy — not just to avoid fines but because we owe it to the people who are represented in the data.Many companies that would benefit from machine learning can’t afford to hire enough skilled engineers. This creates a need for cloud-based AI software as a service (SaaS). How can customers of such services keep data private while counting on another party to process the data? Consider an AI system that reads electronic health records to make predictions about the patients. Can a hospital use a SaaS provider to monitor the system’s performance without exposing sensitive data?Recently I learned about a monitoring technique that manages to keep data secure. While visiting Seattle, I met with Alessya Visnjic and Maria Karaivanova, two of the founders of WhyLabs, which provides a SaaS platform that monitors machine learning applications. (Disclosure: WhyLabs is a portfolio company of AI Fund, which I lead.) They explained how they help customers monitor deployed systems for problems like data drift — changes in the distribution of data because, say, a new disease emerged or the hospital started collecting data in a new way — while maintaining data privacy. In their approach, data never leaves the customer’s system. Instead, the SaaS provider (i) computes statistics on data at the source using efficient techniques based on Apache DataSketches and (ii) analyzes the statistics.The system enables customers to set up dashboards that track the distribution of input features (in this case, body temperature, red blood cell count, and so on) and alerts them when the distribution shows anomalies. Software that runs on the hospital’s server collects data from multiple patients and transmits only the aggregate statistics to the cloud. In this way, the system can look for anomalies without receiving any individual’s data.This is useful for detecting not only data drift but also data-quality problems. Let's say the hospital shifts to a more precise body temperature notation and leaves the old temperature field empty. The system would monitor the fraction of missing temperature values across all patients and alert the hospital that this field is frequently empty. This enables monitoring of critical data-quality markers such as:missing value ratiovolume (that is, volume of data from different departments; a sudden drop in volume from one department may indicate a data pipeline issue in that department)cardinality (detecting new values added to a categorical data field)schema (which can catch changes in data types and formats, such as nine-digit postal codes entered into a field intended for five-digit postal codes)In the data-centric approach to building a machine learning system, our job isn’t done when we deploy a model. We still need to watch out for and address post-deployment issues. Too many teams don’t continuously monitor their models after deploying them because they’re concerned about complexity or privacy. This leads to outdated models that may perform poorly for weeks or months before the problem is detected.In some tasks, complete privacy may not be possible when working with a SaaS provider, but WhyLabs’ approach (which includes open source tools) preserves privacy while logging and monitoring. I hope we continue to invent techniques that enable AI systems to process data in the cloud while maximizing the degree of privacy we can offer to users and customers.Keep learning!Andrew",
        "summary": "*   The need for privacy-preserving AI systems is crucial, especially for cloud-based AI SaaS handling sensitive data like electronic health records.\n*   A technique for monitoring machine learning applications preserves data privacy by computing and analyzing only aggregate statistics at the data source, without transmitting individual data.\n*   This approach, using techniques like Apache DataSketches, enables detection of data drift and data-quality issues (missing value ratio, volume, cardinality, schema) based on aggregate statistics.\n*   It facilitates essential continuous post-deployment ML model monitoring while addressing privacy concerns that often hinder this practice.",
        "publish_date": "Nov 30, 2022",
        "keywords": [
            "Apache DataSketches",
            "data drift",
            "ML model monitoring",
            "aggregate statistics",
            "cardinality"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-one-cloud-provider-monitors-ai-performance-remotely-without-risking-exposure-of-private-data/"
    },
    {
        "content": "Dear friends,Each year, AI brings wondrous advances. But, as Halloween approaches and the veil lifts between the material and ghostly realms, we see that spirits take advantage of these developments at least as much as humans do.As I wrote last week, prompt engineering, the art of writing text prompts to get an AI model to generate the output you want, is a major new trend. Did you know that the Japanese word for prompt — 呪文— also means spell or incantation? (Hat tip to natural language processing developer Paul O’Leary McCann.) The process of generating an image using a model like DALL·E 2 or Stable Diffusion does seem like casting a magic spell — not to mention these programs' apparent ability to reanimate long-dead artists like Pablo Picasso — so Japan's AI practitioners may be onto something.Some AI companies are deliberately reviving the dead. The startup HereAfter AI produces chatbots that speak, sound, and look just like your long-lost great grandma. Sure, it's a simulation. Sure, the purpose is to help the living connect with deceased loved ones. When it comes to reviving the dead — based on what I've learned by watching countless zombie movies — I'm sure nothing can go wrong.I'm more concerned by AI researchers who seem determined to conjure ghastly creatures. Consider the abundance of recent research into transformers. Every transformer uses multi-headed attention. Since when is having multiple heads natural? Researchers are sneaking multi-headed beasts into our computers, and everyone cheers for the new state of the art! If there's one thing we know about transformers, it's that there's more than meets the eye.This has also been a big year for learning from masked inputs, and approaches like Masked Autoencoders, MaskGIT, and MaskViT have achieved outstanding performance in difficult tasks. So if you put on a Halloween mask, know that you're supporting a key idea behind AI progress.Trick or treat!Andrew",
        "summary": "*   Prompt engineering is identified as a major new trend for controlling AI model output.\n*   AI models like DALL·E 2 and Stable Diffusion are used for image generation, and applications include chatbots simulating deceased individuals (e.g., HereAfter AI).\n*   Recent research highlights transformers, noting every transformer uses multi-headed attention.\n*   Learning from masked inputs is significant, with Masked Autoencoders, MaskGIT, and MaskViT achieving outstanding performance in difficult tasks.",
        "publish_date": "Oct 26, 2022",
        "keywords": [
            "Prompt engineering",
            "DALL·E 2",
            "transformers",
            "multi-headed attention",
            "Masked Autoencoders"
        ],
        "url": "https://www.deeplearning.ai/the-batch/multi-headed-attention-and-other-halloween-horrors/"
    },
    {
        "content": "Dear friends, Today DeepLearning.AI is launching the Mathematics for Machine Learning and Data Science Specialization, taught by the world-class AI educator Luis Serrano. In my courses, when it came to math, I’ve sometimes said, “Don’t worry about it.” So why are we offering courses on that very subject? You can learn, build, and use machine learning successfully without a deep understanding of the underlying math. So when you’re learning about an algorithm and come across a tricky mathematical concept, it’s often okay to not worry about it in the moment and keep moving. I would hate to see anyone interrupt their progress for weeks or months to study math before returning to machine learning (assuming that mastering machine learning, rather than math, is your goal).But . . . understanding the math behind machine learning algorithms improves your ability to debug algorithms when they aren’t working, tune them so they work better, and perhaps even invent new ones. You’ll have a better sense for when you’re moving in the right direction or something might be off, saving months of effort on a project. So during your AI journey, it’s worthwhile to learn the most relevant pieces of math, too.If you’re worried about your ability to learn math, maybe you simply haven’t yet come across the best way to learn it. Even if math isn’t your strong suit, I’m confident that you’ll find this specialization exciting and engaging.Luis is a superb machine learning engineer and teacher of math. He and I spent a lot of time debating the most important math topics for someone in AI to learn. Our conclusions are reflected in three courses:Linear algebra. This course will teach you how to use vectors and matrices to store and compute on data. Understanding this topic has enabled me to get my own algorithms to run more efficiently or converge better.Calculus. To be honest, I didn’t really understand why I needed to learn calculus when I first studied it in school. It was only as I started studying machine learning — specifically, gradient descent and other optimization algorithms — that I appreciated how useful it is. Many of the algorithms I’ve developed or tuned over the years would have been impossible without a working knowledge of calculus.Probability and statistics. Knowing the most common probability distributions, deriving ways to estimate parameters, applying hypothesis testing, and visualizing data all come up repeatedly in machine learning and data science projects. I’ve found that this knowledge often helps me make decisions; for instance, judging whether one approach is more promising than another.Math isn’t about memorizing formulas, it’s about building a conceptual understanding that sharpens your intuition. That’s why Luis, curriculum product manager Anshuman Singh, and the team that developed the courses present them using interactive visualizations and hands-on examples. Their explanations of some concepts are the most intuitive I’ve ever seen.I hope you enjoy the Mathematics for Machine Learning and Data Science Specialization!Keep learning,Andrew",
        "summary": "*   DeepLearning.AI is launching the Mathematics for Machine Learning and Data Science Specialization, taught by Luis Serrano.\n*   Understanding the math behind ML algorithms improves debugging, tuning, and inventing new ones, saving months of effort on projects.\n*   The specialization covers key math topics for AI/Data Science: Linear algebra (vectors, matrices, data computation), Calculus (gradient descent, optimization), and Probability and statistics (distributions, hypothesis testing, data visualization).\n*   The courses focus on building conceptual understanding and intuition using interactive visualizations and hands-on examples.",
        "publish_date": "Jan 25, 2023",
        "keywords": [
            "gradient descent",
            "hypothesis testing",
            "data visualization",
            "vectors",
            "matrices"
        ],
        "url": "https://www.deeplearning.ai/the-batch/unlock-the-power-of-machine-learning-by-learning-the-math-that-make-them-work/"
    },
    {
        "content": "Dear friends,In last week’s issue of The Batch, Yoshua Bengio, Alon Halevy, Douwe Kiela, Been Kim, and Reza Zadeh shared their hopes for AI in 2023. I also asked people on LinkedIn and Twitter about their hopes for AI this year. Rather than focusing on the latest buzzy topics in the news, many offered an amazing diversity of answers.In addition to hopes for further technical advances, common themes include:Societal matters. Fairness, bias, and regulation are top concerns. Progress in responsible AI remains important, and with the rise of technologies like generative AI, we need new techniques to make them responsible as well. (For instance, how do we stop image generators from producing unwanted sexualized images of women?) Regulators worldwide are also struggling to keep up.Progress in application areas including agriculture, biology, climate change, healthcare, scientific discovery, and many more. It feels like the number of applications still outstrips the number of people we have! I'm glad the AI community continues to grow.More open sharing and open source. Many people appreciate the open sharing of ideas and code and hope it continues. With respect to open source, personally, I hope that teams will release code under licenses approved by the Open Source Initiative, which permit broad use, rather than more restrictive licenses.Training in AI and data literacy for many more people. AI capabilities and the availability of data are rising rapidly, so the potential for value creation via AI and data science grows every year. But most of the world is able to access this value only through systems built by someone else, usually a large tech company. Better training will enable people to solve a wider variety of problems, enriching society.Personal growth including learning more and/or finding a job. Many individuals want to keep learning, advance their skills, and build a career. The opportunities are out there, so I’m glad that so many of us are working to better ourselves to meet the opportunities!That we all have so many different dreams for AI is a sign of how large our community has become and the broad footprint of our impact. It also means more fun technologies to learn about and more people we can learn from and collaborate with.I found the comments inspiring and am grateful to everyone who responded. If you’re looking for AI inspiration, take a look at the discussion and perhaps you’ll find ideas that are useful in your work. If you find the variety of comments overwhelming, consider writing software that clusters them into topics and share your results with me!Keep learning!Andrew",
        "summary": "* Hopes for AI in 2023 include addressing societal matters such as fairness, bias, regulation, and responsible AI, especially for generative AI.\n* Progress is anticipated in applying AI to various fields including agriculture, biology, climate change, healthcare, and scientific discovery.\n* Continued open sharing of ideas and code is hoped for, with a preference for open source licenses approved by the Open Source Initiative.\n* Increased training in AI and data literacy is seen as important to enable more people to access and create value from AI and data science.\n* Many individuals hope for personal growth in AI, including learning more, advancing skills, and finding jobs.",
        "publish_date": "Jan 04, 2023",
        "keywords": [
            "Responsible AI",
            "Generative AI",
            "Fairness",
            "Bias",
            "Regulation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-the-ai-community-wants-in-2023/"
    },
    {
        "content": "Dear friends, As the winter holiday approaches, it occurs to me that, instead of facing AI winter, we are in a boiling-hot summer of AI. The vast majority of economic value created by AI today comes through the tool of supervised learning, trained to generate short labels (such as spam/not-spam) or a sequence of labels (such as a transcript of audio). This year, generative AI, which is built on top of supervised learning, arrived as a second major tool that enables AI to generate complex and compelling outputs such as images or paragraphs of text. Some previous attempts to develop major new tools — for example, reinforcement learning — have not yet borne fruit commensurate with their hype. But generative AI is working well enough that it’s creating a new paradigm for AI applications. And supervised learning is still far from achieving even a small fraction of its potential! Millions of applications that can be solved by supervised learning have not yet been built. Many teams are still trying to figure out best practices for developing products though supervised learning. In the coming year and beyond, I look forward to wrestling with generative AI to create massive amounts of value for everyone. I feel lucky to be alive in this era, when technology is growing rapidly and we have an opportunity to create the future together! I feel even luckier to share this world with my family and with you. Happy holidays, Andrew",
        "summary": "*   AI is currently in a \"boiling-hot summer,\" with the vast majority of economic value coming from supervised learning, used for generating short labels or sequences of labels.\n*   Generative AI, built on supervised learning, has emerged as a second major tool enabling complex outputs like images or paragraphs of text.\n*   Generative AI is working well and creating a new paradigm for AI applications, unlike some previous attempts (e.g., reinforcement learning) that have not yet borne fruit commensurate with their hype.\n*   Supervised learning still has vast untapped potential, with millions of applications yet to be built.",
        "publish_date": "Dec 21, 2022",
        "keywords": [
            "Supervised Learning",
            "Generative AI",
            "Reinforcement Learning",
            "Label Generation",
            "Text Generation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/models-like-dall-e-and-stable-diffusion-are-creating-a-new-paradigm-for-ai-application/"
    },
    {
        "content": "Dear friends,Is prompt engineering — the art of writing text prompts to get an AI system to generate the output you want — going to be a dominant user interface for AI? With the rise of text generators such as GPT-3 and Jurassic and image generators such as DALL·E, Midjourney, and Stable Diffusion, which take text input and produce output to match, there has been growing interest in how to craft prompts to get the output you want. For example, when generating an image of a panda, how does adding an adjective such as “beautiful” or a phrase like “trending on artstation” influence the output? The response to a particular prompt can be hard to predict and varies from system to system.So is prompt engineering an important direction for AI, or is it a hack?Here’s how we got to this point:The availability of large amounts of text or text-image data enabled researchers to train text-to-text or text-to-image models.Because of this, our models expect text as input.So many people have started experimenting with more sophisticated prompts.Some people have predicted that prompt engineering jobs would be plentiful in the future. I do believe that text prompts will be an important way to tell machines what we want — after all, they’re a dominant way to tell other humans what we want. But I think that prompt engineering will be only a small piece of the puzzle, and breathless predictions about the rise of professional prompt engineers are missing the full picture.Just as a TV has switches that allow you to precisely control the brightness and contrast of the image — which is more convenient than trying to use language to describe the image quality you want — I look forward to a user interface (UI) that enables us to tell computers what we want in a more intuitive and controllable way.Take speech synthesis (also called text-to-speech). Researchers have developed systems that allow users to specify which part of a sentence should be spoken with what emotion. Virtual knobs allow you to dial up or down the degree of different emotions. This provides fine control over the output that would be difficult to express in language. By examining an output and then fine-tuning the controls, you can iteratively improve the output until you get the effect you want.So, while I expect text prompts to remain an important part of how we communicate with image generators, I look forward to more efficient and understandable ways for us to control their output. For example, could a set of virtual knobs enable you to generate an image that is 30 percent in the style of Studio Ghibli and 70 percent the style of Disney? Drawing sketches is another good way to communicate, and I’m excited by img-to-img UIs that help turn a sketch into a drawing.Likewise, controlling large language models remains an important problem. If you want to generate empathetic, concise, or some other type of prose, is there an easier way than searching (sometimes haphazardly) among different prompts until you chance upon a good one?When I’m just playing with these models, I find prompt engineering a creative and fun activity; but when I’m trying to get to a specific result, I find it frustratingly opaque. Text prompts are good at specifying a loose concept such as “a picture of a panda eating bamboo,” but new UIs will make it easier to get the results we want. And this will help open up generative algorithms to even more applications; say, text editors that can adjust a piece of writing to a specific style, or graphics editors that can make images that look a certain way.Lots of exciting research ahead! I look forward to UIs that complement writing text prompts.Keep learning!Andrew",
        "summary": "* Prompt engineering is a method using text prompts to control generative AI systems like text generators (GPT-3) and image generators (DALL·E, Midjourney, Stable Diffusion).\n* The effectiveness and predictability of prompt engineering output are difficult and vary across different AI systems.\n* The current reliance on text prompts is linked to AI models being trained to accept text as input.\n* While text prompts will remain important, future AI user interfaces are expected to incorporate more intuitive and controllable methods (e.g., virtual knobs, visual inputs) to achieve specific outputs beyond loose concepts.",
        "publish_date": "Oct 19, 2022",
        "keywords": [
            "Prompt engineering",
            "GPT-3",
            "DALL·E",
            "Midjourney",
            "Stable Diffusion"
        ],
        "url": "https://www.deeplearning.ai/the-batch/prompt-engineering-future-of-ai-or-hack/"
    },
    {
        "content": "Dear friends,What should be AI’s role in moderating the millions of messages posted on social media every day? The volume of messages means that automation is required. But the question of what is appropriate moderation versus inappropriate censorship lingers.AI is helpful for scaling up a moderation policy. But it doesn’t address the core challenge of defining a policy: Which expressions to permit and which to block. This is hard for both humans and AI.Deciding what to block is hard because natural language is ambiguous.“Don’t let them get away with this” could be an incitement to violence or a call for justice.“The vaccine has dangerous side effects” could be a scientific fact or misinformation.The meanings of words vary from person to person. My son says “wawa” when he wants water, and only his close family (and now you!) understand. At work, teams invent acronyms that others don’t understand. More problematically, lawbreakers and hate groups develop code words to discuss their activities.If humans understand the same words differently, how can we train an AI to make such distinctions? If a piece of text has no fixed meaning, then enforcing policies based on the text is difficult. Should we hide it from user A if they would read it as promoting violence, but show it to user B if they would view it as benign? Or should hiding a message be based on the intent of the sender? None of these options is satisfying.Further, getting the data to build an AI system to accomplish any of this is hard. How can the developers who gather the data understand its full range of meanings? Different communities have their own interpretations, making it impossible to keep track.Even if the meaning are unambiguous, making the right decision is still hard. Fortunately, social media platforms can choose from a menu of options depending on how egregious a message is and the degree of confidence that it’s problematic. Choices include showing it to a smaller audience, adding a warning label, and suspending, temporarily or permanently, the user who posted it. Having a range of potential consequences helps social media platforms manage the tradeoff between silencing and protecting users (and society).Despite their flaws, AI systems make social media better. Imagine email without AI-driven spam filtering; it would rapidly become unusable. Similarly, AI is critical for eliminating the most spammy or toxic social media messages. But the challenge of moderating any given message transcends AI.It’s important to acknowledge this challenge openly, so we can debate the principles we would apply to this problem and recognize that there may be no perfect solution. Through transparent and robust debate, I believe that we can build trust around content moderation and make tradeoffs that maximize social media’s benefit.Keep learning!Andrew",
        "summary": "*   AI is required for scaling social media content moderation due to the volume of messages.\n*   AI helps scale moderation policy but does not address the core challenge of defining which expressions to permit or block.\n*   Training AI for moderation is difficult due to natural language ambiguity, varying meanings across individuals/communities, and challenges in data gathering.\n*   AI systems are critical for filtering high-volume spam and toxic messages, improving social media usability.",
        "publish_date": "Dec 14, 2022",
        "keywords": [
            "Content Moderation",
            "Spam Filtering",
            "Toxic Message Filtering",
            "Natural Language Ambiguity",
            "Data Gathering"
        ],
        "url": "https://www.deeplearning.ai/the-batch/deciding-which-posts-to-show-or-hide-is-a-human-problem-not-a-tech-problem/"
    },
    {
        "content": "Dear friends,Will the future of large language models limit users to cutting-edge models from a handful of companies, or will users be able to choose among powerful models from a large number of developers? We’re still early in the development of large language models (LLMs), but I believe that users will have access to models from many companies. This will be good for innovation.We've seen repeatedly that yesterday’s supercomputer is tomorrow’s pocket watch. Even though training an LLM currently requires massive data and infrastructure, I see encouraging progress toward wider availability and access along three dimensions:Open models are gaining traction and delivering solid performance, such as BigScience’s BLOOM, Tsinghua University’s GLM, and Meta’s OPT (released under a restrictive license that welcomes researchers but bars commercial use). Today’s open models aren’t as good as some proprietary models, but they will continue to improve rapidly.Researchers are developing techniques to make training more efficient. DeepMind published recommendations for how to train LLMs given a fixed computational budget, leading to significant gains in efficiency. Although it addresses smaller models, cramming improves the performance that can be achieved with one day of training language models on a single GPU. Recent work using eight-bit and even four-bit computation is also pushing the possibilities for inference.As more teams develop and publish LLMs, there will be systematic comparisons that empower users to pick the right one based on cost, availability, and other criteria. For example, a team led by Percy Liang carried out an extensive study that compares LLMs. (Skip to the “Our Findings” section if you’re impatient to see their conclusions.)There were times in my career when I worked with some of the world’s biggest systems dedicated to training deep learning models, but they didn’t last. I had access to massive parallel computing power at Google, and my teams built an early GPU server at Stanford and a high-performance computing system focused on speech recognition. Faster systems soon left those formerly cutting-edge systems in the dust. Even though training an LLM currently requires a daunting amount of computation, I see little reason to believe that it won’t quickly become much easier, particularly given the widespread excitement and massive investment around them.What does this mean for businesses? Many companies have built valuable and defensible businesses using early innovations in deep learning, and I foresee that similarly valuable and defensible systems will be built using recent innovations in LLMs and, more broadly, generative AI.I will explore this topic more in future letters. Until then,Keep learning!Andrew",
        "summary": "*   The author predicts users will access powerful LLMs from many developers, fostering innovation, rather than being limited to a few companies.\n*   Progress towards wider LLM availability is seen in three dimensions: gaining traction of open models (e.g., BLOOM, GLM, OPT), research into more efficient training techniques (e.g., DeepMind recommendations, cramming, 8-bit/4-bit computation), and systematic comparisons empowering user choice (e.g., study by Percy Liang's team).\n*   Despite current massive computation needs for training, the author believes it will quickly become much easier, similar to past shifts where cutting-edge systems were rapidly surpassed.\n*   Valuable and defensible businesses are expected to be built using recent innovations in LLMs and generative AI.",
        "publish_date": "Jan 11, 2023",
        "keywords": [
            "BLOOM",
            "GLM",
            "OPT",
            "cramming",
            "8-bit/4-bit computation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-the-future-is-likely-to-bring-more-large-language-models/"
    },
    {
        "content": "Dear friends,  Machine learning engineers routinely use Jupyter Notebooks for developing and experimenting with code. They’re a regular feature in DeepLearning.AI’s courses. But there’s another use of Jupyter Notebooks that I think is under-appreciated: communicating concepts to others. For example, once I was experimenting with a new way to build a neural network in which the input features were engineered a specific way, and I wanted to explain my ideas to colleagues. Writing a text document would have been a reasonable approach, but using a Jupyter Notebook allowed me to format text using its Markdown feature and include an implementation of the idea in code. That way, readers could execute it, experiment with hyperparameters, and add further code to delve more deeply into what the algorithm was doing. When we use a Jupyter Notebook to build a piece of code, the ultimate reader is a computer, whose job is to faithfully execute the program. But when using a Notebook to communicate with people, the goal is to convey an idea and illustrate it with code. The interactive nature of notebooks — which lets you run code snippets to generate outputs, and also lets you add formatted text, equations, graphs, and images — makes this a much richer medium than merely writing code that contains comments. A team I work with recently used a Jupyter Notebook to model their revenue projections. While other tools such as spreadsheets could have served a similar purpose, a Notebook can include prose that articulates underlying assumptions such as the rates of sales growth and customer churn. Further, it invites readers to play with these parameters to deepen their understanding of how they affect the business. I write and send a lot of documents and enjoy written communication. But if you’re trying to explain a scientific or mathematical equation, simulating a business or other system, or presenting your analysis of data, consider sending your audience a Jupyter Notebook. This flexible tool even makes a great alternative to a slide deck. It’s great not only for writing code to communicate with your computer but also for crafting a story to communicate with other people. Keep learning! Andrew",
        "summary": "* Machine learning engineers routinely use Jupyter Notebooks for developing and experimenting with code.\n* An under-appreciated use of Jupyter Notebooks is communicating concepts to others, allowing formatted text (Markdown) and executable code for reader experimentation.\n* The interactive nature of notebooks (running code, adding text, equations, graphs, images) makes them a richer medium for conveying ideas than code with comments.\n* Notebooks are useful for explaining scientific/mathematical concepts, simulating systems, or presenting data analysis, and can serve as an alternative to slide decks.",
        "publish_date": "Jun 06, 2022",
        "keywords": [
            "Jupyter Notebooks",
            "Machine learning engineers",
            "executable code",
            "data analysis",
            "Markdown"
        ],
        "url": "https://www.deeplearning.ai/the-batch/presenting-a-technical-concept-use-a-jupyter-notebook-2/"
    },
    {
        "content": "Dear friends,With the pandemic easing in the United States and Canada, I’ve been traveling more in the last two weeks. I spoke at TED 2022 in Vancouver and ScaleUp:AI in New York and attended a manufacturing conference in California.What a pleasure it was to see people in 3D! In the days before Covid, serendipitous conversations were a large part of how I kept up with what’s happening in the world. I’ve really missed these meetings.It was great to hear former world chess champion and Russian dissident Garry Kasparov speak and to chat with him afterward about Russia’s invasion of Ukraine. (I largely agree with his views.) I enjoyed conversing with astronaut Chris Hadfield about property rights on the moon, MIT professor Ariel Ekblaw about living in space, and neuroscientist Frances Chance about when we might develop a theory of how the mind works. I saw AI artist Sophia Crespo present her generated creatures and heard venture capitalists George Mathew and Lonne Jaffe talk about investing in AI startups.I found these conversations tremendously stimulating, and I came away thinking about some observations with respect to AI.To the general public, AI is still mysterious and inaccessible. Many people think that AI means AGI (artificial general intelligence), which remains far away. They don’t understand how deeply AI is already embedded in society. People would be better off if they made personal and business decisions — Should I study radiology? Should I cultivate my company’s ability to produce data? — based on realistic expectations for the future. So let’s get out there and keep helping people to shape a realistic perspective.Much of the infrastructure for building and deploying AI systems, such as MLOps tools, remains to be built. Despite the valiant efforts of many startups and cloud companies, it will be many years before the ecosystem of software infrastructure settles. Infrastructure for data manipulation and storage, and for data-centric approaches in particular, will play a large role. The community of artists who are using AI to create images or music is small but growing quickly. Some are getting by selling NFTs of their work. I’m pleased that artists can make money this way, though I’m nervous about how scalable this revenue stream will be. I hope that individuals with means will continue to support the arts regardless of the resale value of NFTs.Many people in the space industry are excited to take advantage of AI. There are myriad unsolved problems in, say, getting humans to Mars and back, from generating thrust to ensuring a soft landing. These are great opportunities for the AI community.Going to these in-person events has me looking forward to a time, hopefully soon, when DeepLearning.AI and our ambassadors can hold more in-person events safely. I realize that the pandemic still varies widely in different regions. I hope you’ll enjoy reconnecting in person when it’s safe for you to do so, and benefit from the joyful conversations that contribute so much to learning.Keep learning!Andrew",
        "summary": "*   To the general public, AI is still mysterious and inaccessible, often confused with AGI, and its deep societal embedding is not understood; shaping realistic expectations is needed for personal and business decisions.\n*   Much infrastructure for building and deploying AI systems, such as MLOps tools and data-centric approaches, remains to be built, and the software infrastructure ecosystem will take many years to settle.\n*   AI is being used by artists to create images/music (some selling NFTs), is a focus for venture capital investment in startups, and presents opportunities for solving problems in the space industry (e.g., human travel to Mars).",
        "publish_date": "Apr 13, 2022",
        "keywords": [
            "MLOps tools",
            "Data-centric approaches",
            "Software infrastructure ecosystem",
            "AI systems",
            "AGI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-joy-of-conversation-about-ai-and-other-things/"
    },
    {
        "content": "Dear friends, Last week, I wrote about how rising interest rates are likely to lead investors and other finance professionals to focus on short-term returns rather than longer-term investments. Nonetheless, I believe this is still a good time to invest in long-term bets on AI. Why? In a nutshell, (i) the real interest rate (adjusted for inflation) remains very low, and (ii) the transformative value of AI is more financially powerful than interest rates.Although the news is full of rising interest rates, today’s rates are still quite low from a historical point of view. Interest rates (technically, the three-month U.S. treasury bill rate) peaked at over 15% in the 1980s. In contrast, they varied between nearly 0% and about 2.5% over the past decade.A few percentage points of interest aren’t very significant in the face of historic gains in the value of innovative technology. Given the transformative impact of AI — which is making it possible to automate more tasks than ever — I believe that many projects will deliver returns (as measured by, say, share prices) much higher than the interest rate.For instance, if you have an idea for a project that can create a 150% return, it matters little if interest rises by 5% and reduces the present value of your project slightly. The returns from high-risk, high-reward AI projects vary so widely — and have so much upside potential — that a modest change in interest rates should have little impact on the decision whether to go for it.Rising interest rates aren’t the only factor that influences how we should view AI investments. Inflation is going up as well. This makes it relatively attractive to invest in building AI projects now, rather than wait and pay a higher price in the future.Let’s say you’re debating whether to invest in a $100 GPU to speed up your work. A high interest rate — say, 10% — is a disincentive to spend the money: If you can postpone the investment, you save your $100 for a year, end up with $110 after that period, buy the GPU, and pocket the extra $10. But what if you know that inflation will cause the GPU to cost $110 in a year (10% inflation), or even $120 in a year (20% inflation)? Then it’s more attractive to spend the money now.In fact, many people are underestimating how much inflation reduces the real cost of interest. The real interest rate, which takes inflation into account, is roughly the nominal (not adjusted for inflation) interest rate minus the rate of inflation. Because inflation is high, short-term real interest rates (technically, the risk-free rate) going out to 5 years are actually negative right now. Thus, in my view, it remains a good time to continue to make significant investments in technology that you believe will pay off.The great investor Warren Buffet once said he tries to be “fearful when others are greedy, and greedy when others are fearful.” Current market conditions are making many investors fearful. I don’t advocate greed, but I do think for many teams this is a good time to charge ahead bravely and pursue ideas that you believe in. Just as many great companies were founded around the time of the Great Recession of 2007 to 2009, today’s economic headwinds, by sweeping away weaker projects, will clear the way for the strongest teams and ideas to leap ahead.In case you’re wondering, I plan to put my money where my mouth is. AI Fund, the venture studio I lead, will continue to build companies with energy and enthusiasm. Even though some bets on AI will fail, I’m more concerned about aggregate underinvestment than overinvestment in AI.I don’t advocate ignoring the market downturn. This is a good time to make sure you’re operating efficiently and your teams are appropriately frugal and have good fiscal discipline. Despite the gloomy market, I intend to charge ahead and keep building valuable projects — and I hope you will, too.Keep learning!Andrew",
        "summary": "* Advocates for continued long-term investment in AI despite rising interest rates, stating AI's transformative value is more financially powerful than interest rates.\n* Suggests high-risk, high-reward AI projects can deliver returns much higher than interest rates.\n* Notes that high inflation makes investing in building AI projects now (e.g., buying a $100 GPU) relatively attractive compared to waiting.\n* The author's AI Fund will continue building companies, expressing concern about aggregate underinvestment in AI.",
        "publish_date": "Jun 08, 2022",
        "keywords": [
            "GPU",
            "AI projects",
            "high-risk, high-reward AI projects",
            "long-term investment in AI",
            "underinvestment in AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-rising-interest-rates-mean-for-ai-part-2-why-it-still-makes-sense-to-forge-ahead/"
    },
    {
        "content": "Dear friends, The United States Federal Reserve Bank has signaled that it will continue to raise interest rates. As one consequence, the stock market is significantly down, particularly tech stocks, relative to the beginning of the year. What does this mean for AI? In this two-part series, I’d like to discuss what I think will happen — which may have implications on your AI projects — and what I think should happen. Unfortunately, these are different things.The U.S. has enjoyed low interest rates over the past decade. Simplifying a bit, if r is the interest rate (if the interest rate is 2%, then r = 0.02), then one dollar T years in the future is worth 1/(1+r)^T as much as one dollar today. The larger r is, the less that future dollar is worth relative to its value today. If you’re familiar with the discount factor ɣ (Greek alphabet gamma) in reinforcement learning, you may notice that ɣ plays a similar role to 1/(1+r) and weights rewards T steps in the future by ɣ^T.If interest rates were near zero, then one dollar in 10 years would be worth about the same as it is today. But if the interest rate were 5%, then a guaranteed promise of one dollar in 10 years would be worth only 61 cents today. What this means is that investors in the stock market are shifting to place a higher premium on cash today rather than cash in the future. This, in turn, will drive many CFOs, CEOs, and venture capital investors to discount investments that they deem likely to pay off only many years into the future.This has important implications for AI. Over the past decade, many ambitious AI efforts sought to build fundamental technology that might pay off over many years. A few years ago, highly speculative bets on an experimental technology — from bold initiatives such as self-driving to more measured ones in which a team sought to execute a clear roadmap for a particular company — seemed like reasonable risks. Amid rising interest rates, such long-term bets look less attractive.Many investors are wondering if the stock market’s 13-year bull run has come to an end, and if the next era will be very different. If interest rates continue to rise, then:Highly speculative, long-term technology development will have a harder time getting funded. I think this is unfortunate, since we will forgo many innovations. It's true that a tighter investment environment will reduce irresponsibly speculative, overhyped bets, but I believe that society will suffer a net loss.There will be more pressure for teams to demonstrate short-term business impact. For example, projects that are likely to generate financial returns on investment within a few years will look more attractive than long-term bets.What this means for our community is that we should be ready for increased pressure to develop projects that demonstrate near-term, tangible value. For example, if you can explain how your AI system — for reading hospital records, inspecting parts, ensuring worker safety, or what have you — can save $1 million in two years, it will be easier to justify the $300,000 annual budget that you might be asking for. So if you’re looking for funding for a company or project, consider near-term impacts or financial justifications you can develop.So far, I’ve laid out my prediction about what will happen, but what I think should happen is different. I believe this is still a good time to invest in long-term bets, because (i) the real interest rate (that is, the rate adjusted for inflation) is still very low, and (ii) the transformative value of AI is more financially powerful than interest rates, even as they compound at the moderate pace of 1/(1+r)^T. More about this in my next letter.Keep learning!AndrewP.S. I’m grateful to Erik Brynjolfsson, a brilliant economist who has done seminal work on tech’s impact on the economy, for helping me think through the contents of this letter. Responsibility for any errors lies with me.",
        "summary": "*   Rising interest rates discount future value, leading investors and executives to place a higher premium on cash today rather than in the future.\n*   This shift makes highly speculative, long-term AI technology development less attractive for funding.\n*   AI teams will face increased pressure to demonstrate short-term business impact and generate financial returns within a few years.\n*   AI practitioners should prepare by focusing on projects showing near-term, tangible value and developing financial justifications, such as saving $1 million in two years with a $300,000 annual budget.",
        "publish_date": "Jun 06, 2022",
        "keywords": [
            "AI technology development",
            "Short-term business impact",
            "Financial returns",
            "Long-term AI technology development",
            "Funding"
        ],
        "url": "https://www.deeplearning.ai/the-batch/what-rising-interest-rates-mean-for-ai-part-1-whats-likely-to-happen/"
    },
    {
        "content": "Dear friends,In the last two letters, I wrote about developing a career in AI and shared tips for gaining technical skills. This time, I’d like to discuss an important step in building a career: project work.It goes without saying that we should only work on projects that are responsible and ethical, and that benefit people. But those limits leave a large variety to choose from. I wrote previously about how to identify and scope AI projects. This and next week’s letter have a different emphasis: picking and executing projects with an eye toward career development.A fruitful career will include many projects, hopefully growing in scope, complexity, and impact over time. Thus, it is fine to start small. Use early projects to learn and gradually step up to bigger projects as your skills grow.When you’re starting out, don’t expect others to hand great ideas or resources to you on a platter. Many people start by working on small projects in their spare time. With initial successes — even small ones — under your belt, your growing skills increase your ability to come up with better ideas, and it becomes easier to persuade others to help you step up to bigger projects.What if you don’t have any project ideas? Here are a few ways to generate them:Join existing projects. If you find someone else with an idea, ask to join their project.Keep reading and talking to people. I come up with new ideas whenever I spend a lot of time reading, taking courses, or talking with domain experts. I’m confident that you will, too.Focus on an application area. Many researchers are trying to advance basic AI technology — say, by inventing the next generation of transformers or further scaling up language models — so, while this is an exciting direction, it is hard. But the variety of applications to which machine learning has not yet been applied is vast! I’m fortunate to have been able to apply neural networks to everything from autonomous helicopter flight to online advertising, partly because I jumped in when relatively few people were working on those applications. If your company or school cares about a particular application, explore the possibilities for machine learning. That can give you a first look at a potentially creative application — one where you can do unique work — that no one else has done yet.Develop a side hustle. Even if you have a full-time job, a fun project that may or may not develop into something bigger can stir the creative juices and strengthen bonds with collaborators. When I was a full-time professor, working on online education wasn’t part of my “job” (which was doing research and teaching classes). It was a fun hobby that I often worked on out of passion for education. My early experiences recording videos at home helped me later in working on online education in a more substantive way. Silicon Valley abounds with stories of startups that started as side projects. So long as it doesn’t create a conflict with your employer, these projects can be a stepping stone to something significant.Given a few project ideas, which one should you jump into? Here’s a quick checklist of factors to consider:Will the project help you grow technically? Ideally, it should be challenging enough to stretch your skills but not so hard that you have little chance of success. This will put you on a path toward mastering ever-greater technical complexity.Do you have good teammates to work with? If not, are there people you can discuss things with? We learn a lot from the people around us, and good collaborators will have a huge impact on your growth.Can it be a stepping stone? If the project is successful, will its technical complexity and/or business impact make it a meaningful stepping stone to larger projects? (If the project is bigger than those you’ve worked on before, there’s a good chance it could be such a stepping stone.)Finally, avoid analysis paralysis. It doesn’t make sense to spend a month deciding whether to work on a project that would take a week to complete. You'll work on multiple projects over the course of your career, so you’ll have ample opportunity to refine your thinking on what’s worthwhile. Given the huge number of possible AI projects, rather than the conventional “ready, aim, fire” approach, you can accelerate your progress with “ready, fire, aim.”Keep learning!Andrew",
        "summary": "* Project work is an important step in building an AI career, starting small and gradually increasing scope, complexity, and impact.\n* Generate AI project ideas by joining existing projects, reading/talking to experts, focusing on applying machine learning to new areas, or developing side hustles.\n* When choosing an AI project, consider its potential for technical growth, the availability of good teammates/collaborators, and its potential as a stepping stone to larger projects.\n* Avoid analysis paralysis when selecting projects; a \"ready, fire, aim\" approach can accelerate progress in AI project work.",
        "publish_date": "Jul 13, 2022",
        "keywords": [
            "AI project work",
            "machine learning",
            "project ideas",
            "technical growth",
            "analysis paralysis"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-3-choosing-projects/"
    },
    {
        "content": "Dear friends,Last week, I wrote about key steps for building a career in AI: learning technical skills, doing project work, and searching for a job, all of which is supported by being part of a community. In this letter, I’d like to dive more deeply into the first step.More papers have been published on AI than any person can read in a lifetime. So, in your efforts to learn, it’s critical to prioritize topic selection. I believe the most important topics for a technical career in machine learning are:Foundational machine learning skills. For example, it’s important to understand models such as linear regression, logistic regression, neural networks, decision trees, clustering, and anomaly detection. Beyond specific models, it’s even more important to understand the core concepts behind how and why machine learning works, such as bias/variance, cost functions, regularization, optimization algorithms, and error analysis.Deep learning. This has become such a large fraction of machine learning that it’s hard to excel in the field without some understanding of it! It’s valuable to know the basics of neural networks, practical skills for making them work (such as hyperparameter tuning), convolutional networks, sequence models, and transformers.Math relevant to machine learning. Key areas include linear algebra (vectors, matrices, and various manipulations of them) as well as probability and statistics (including discrete and continuous probability, standard probability distributions, basic rules such as independence and Bayes rule, and hypothesis testing). In addition, exploratory data analysis (EDA) — using visualizations and other methods to systematically explore a dataset — is an underrated skill. I’ve found EDA particularly useful in data-centric AI development, where analyzing errors and gaining insights can really help drive progress! Finally, a basic intuitive understanding of calculus will also help. In a previous letter, I described how the math needed to do machine learning well has been changing. For instance, although some tasks require calculus, improved automatic differentiation software makes it possible to invent and implement new neural network architectures without doing any calculus. This was almost impossible a decade ago.Software development. While you can get a job and make huge contributions with only machine learning modeling skills, your job opportunities will increase if you can also write good software to implement complex AI systems. These skills include programming fundamentals, data structures (especially those that relate to machine learning, such as data frames), algorithms (including those related to databases and data manipulation), software design, familiarity with Python, and familiarity with key libraries such as TensorFlow or PyTorch, and scikit-learn.This is a lot to learn! Even after you master everything in this list, I hope you’ll keep learning and continue to deepen your technical knowledge. I’ve known many machine learning engineers who benefitted from deeper skills in an application area such as natural language processing or computer vision, or in a technology area such as probabilistic graphical models or building scalable software systems.How do you gain these skills? There’s a lot of good content on the internet, and in theory reading dozens of web pages could work. But when the goal is deep understanding, reading disjointed web pages is inefficient because they tend to repeat each other, use inconsistent terminology (which slows you down), vary in quality, and leave gaps. That’s why a good course — in which a body of material has been organized into a coherent and logical form — is often the most time-efficient way to master a meaningful body of knowledge. When you’ve absorbed the knowledge available in courses, you can switch over to research papers and other resources.Finally, keep in mind that no one can cram everything they need to know over a weekend or even a month. Everyone I know who’s great at machine learning is a lifelong learner. In fact, given how quickly our field is changing, there’s little choice but to keep learning if you want to keep up. How can you maintain a steady pace of learning for years? I’ve written about the value of habits. If you cultivate the habit of learning a little bit every week, you can make significant progress with what feels like less effort.Keep learning!Andrew",
        "summary": "*   Key technical skills for a machine learning career include foundational ML concepts (models, bias/variance, cost functions, error analysis).\n*   Deep learning knowledge is essential, covering neural network basics, practical skills (hyperparameter tuning), CNNs, sequence models, and transformers.\n*   Relevant math skills are required, specifically linear algebra, probability/statistics (including EDA), and basic calculus.\n*   Software development skills (programming fundamentals, data structures, algorithms, Python, libraries like TensorFlow/PyTorch/scikit-learn) enhance job opportunities.",
        "publish_date": "Jul 06, 2022",
        "keywords": [
            "CNNs",
            "sequence models",
            "transformers",
            "TensorFlow",
            "PyTorch"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-2-learning-technical-skills/"
    },
    {
        "content": "Dear friends,The rapid rise of AI has led to a rapid rise in AI jobs, and many people are building exciting careers in this field. A career is a decades-long journey, and the path is not always straightforward. Over many years, I’ve been privileged to see thousands of students as well as engineers in companies large and small navigate careers in AI. In this and the next few letters, I’d like to share a few thoughts that might be useful in charting your own course.Three key steps of career growth are learning (to gain technical and other skills), working on projects (to deepen skills, build a portfolio, and create impact) and searching for a job. These steps stack on top of each other:Initially, you focus on gaining foundational technical skills.After having gained foundational skills, you lean into project work. During this period, you’ll probably keep learning.Later, you might occasionally carry out a job search. Throughout this process, you’ll probably continue to learn and work on meaningful projects.These phases apply in a wide range of professions, but AI involves unique elements. For example:AI is nascent, and many technologies are still evolving. While the foundations of machine learning and deep learning are maturing — and coursework is an efficient way to master them — beyond these foundations, keeping up-to-date with changing technology is more important in AI than fields that are more mature.Project work often means working with stakeholders who lack expertise in AI. This can make it challenging to find a suitable project, estimate the project’s timeline and return on investment, and set expectations. In addition, the highly iterative nature of AI projects leads to special challenges in project management: How can you come up with a plan for building a system when you don’t know in advance how long it will take to achieve the target accuracy? Even after the system has hit the target, further iteration may be necessary to address post-deployment drift.While searching for a job in AI can be similar to searching for a job in other sectors, there are some differences. Many companies are still trying to figure out which AI skills they need and how to hire people who have them. Things you’ve worked on may be significantly different than anything your interviewer has seen, and you’re more likely to have to educate potential employers about some elements of your work.Throughout these steps, a supportive community is a big help. Having a group of friends and allies who can help you — and whom you strive to help — makes the path easier. This is true whether you’re taking your first steps or you’ve been on the journey for years.I’m excited to work with all of you to grow the global AI community, and that includes helping everyone in our community develop their careers. I’ll dive more deeply into these topics in the next few weeks.Keep learning!Andrew",
        "summary": "*   AI career growth involves three stacking steps: gaining foundational technical skills, working on projects (to deepen skills, build portfolio, create impact), and searching for a job.\n*   Unique aspects of AI careers include the need to keep up with evolving technology beyond foundational ML/DL, and challenges in project work due to stakeholders lacking AI expertise and the iterative nature of projects (e.g., estimating timelines, managing post-deployment drift).\n*   Searching for an AI job can differ as companies are still defining needed skills and candidates may need to educate interviewers about their work.\n*   A supportive community is important throughout the AI career journey.",
        "publish_date": "Jun 29, 2022",
        "keywords": [
            "Post-deployment drift",
            "Machine Learning",
            "Deep Learning",
            "AI Projects",
            "Technical Portfolio"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-a-career-in-ai-part-1-three-steps-to-career-growth/"
    },
    {
        "content": "Dear friends,One of the challenges of building an AI startup is setting customer expectations. Machine learning is a highly experiment-driven field. Until you’ve built something, it’s hard to predict how well it will work. This creates a unique challenge when you’re trying to inform customers about what they should expect a new product to do.For instance, the entire self-driving industry, which I was once part of, did a poor job of setting expectations about when fully autonomous cars would be ready for widespread deployment. This shortcoming led to elevated expectations that the industry failed to meet.Compared to traditional software that begins with a specification and ends with a deliverable to match, machine learning systems present a variety of unique challenges. These challenges can affect the budget, schedule, and capabilities of a product in unexpected ways.How can you avoid surprising customers? Here’s a non-exhaustive checklist of ways that a machine learning system might surprise customers who are more familiar with traditional software:We don’t know how accurate the system will be in advance.We might need a costly initial data collection phase.After getting the initial dataset, we might come back and ask for more data or better data.Moreover, we might ask for this over and over.After we’ve built a prototype that runs accurately in the lab, it might not run as well in production because of data drift or concept drift.Even after we’ve built an accurate production system, its performance might get worse over time for no obvious reason. We might need help monitoring the system and, if its performance degrades over time, invest further to fix it.A system might exhibit biases that are hard to detect.It might be hard to figure out why the system gave a particular output. We didn’t explicitly program it to do that!Despite the customer’s generous budget, we probably won’t achieve AGI. 😀That’s a lot of potential surprises! It’s best to set expectations with customers clearly before starting a project and keep reminding them throughout the process.As a reader of The Batch, you probably know a fair amount about AI. But AI and machine learning are still very mysterious to most people. Occasionally I speak with executives, even at large companies, whose thinking about AI gravitates more toward artificial general intelligence (AGI) — a system that can learn to perform any mental task that a typical human can — than practical applications in the marketplace today. Entrepreneurs who aspire to build AI systems usually have to work extra hard to convey the significant promise of their solution while avoiding setting elevated expectations that they can’t meet. The fact that we ourselves can incorrectly assess the capabilities of the systems we’re building — which is what happened with self-driving — makes this even harder.Fortunately, in many application areas, once you’ve acquired one or two happy customers, things get much easier. You can (with permission) show those successes to later customers and, with a couple of successful deployments under your belt, your own sense of what to expect also improves.The first deployment is always hardest, and each subsequent one gets easier. Keep at it!Keep learning,Andrew",
        "summary": "*   Setting customer expectations for machine learning systems is challenging because ML is experiment-driven and performance is hard to predict in advance.\n*   Unlike traditional software, ML systems require potentially costly initial and ongoing data collection phases.\n*   ML systems can exhibit unpredictable behaviors in production, such as performance degradation over time due to data drift or concept drift, hard-to-detect biases, and difficulty explaining outputs.\n*   The accuracy of ML systems is unknown beforehand, and achieving Artificial General Intelligence (AGI) is not feasible despite budget.",
        "publish_date": "May 18, 2022",
        "keywords": [
            "data drift",
            "concept drift",
            "biases",
            "explaining outputs",
            "Artificial General Intelligence (AGI)"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-ai-startups-part-3-set-customer-expectations/"
    },
    {
        "content": "Dear friends,Years ago, I had to choose between a neural network and a decision tree learning algorithm. It was necessary to pick an efficient one, because we planned to apply the algorithm to a very large set of users on a limited compute budget. I went with a neural network. I hadn’t used boosted decision trees in a while, and I thought they required more computation than they actually do — so I made a bad call. Fortunately, my team quickly revised my decision, and the project was successful.This experience was a lesson in the importance of learning, and continually refreshing, foundational knowledge. If I had refreshed my familiarity with boosted trees, I would have made a better decision.Machine learning, like many technical fields, evolves as the community of researchers builds on top of one another's work. Some contributions have staying power and become the basis of further developments. Consequently, everything from a housing-price predictor to a text-to-image generator is built on core ideas that include algorithms (linear and logistic regression, decision trees, and so on) and concepts (regularization, optimizing a loss function, bias/variance, and the like).A solid, up-to-date foundation is one key to being a productive machine learning engineer. Many teams draw on these ideas in their day-to-day work, and blog posts and research papers often assume that you’re familiar with them. This shared base of knowledge is essential to the rapid progress we've seen in machine learning in recent years.That's why I’m updating my original machine learning class as the new Machine Learning Specialization, which will be available in a few weeks.My team spent many hours debating the most important concepts to teach. We developed extensive syllabi for various topics and prototyped course units in them. Sometimes this process helped us realize that a different topic was more important, so we cut material we had developed to focus on something else. The result, I hope, is an accessible set of courses that will help anyone master the most important algorithms and concepts in machine learning today — including deep learning but also a lot of other things — and to build effective learning systems.In that spirit, this week’s issue of The Batch explores some of our field’s most important algorithms, explaining how they work and describing some of their surprising origins. If you’re just starting out, I hope it will demystify some of the approaches at the heart of machine learning. For those who are more advanced, you’ll find lesser-known perspectives on familiar territory. Either way, I hope this special issue will help you build your intuition and give you fun facts about machine learning’s foundations that you can share with friends.Keep learning!Andrew",
        "summary": "* The author's experience highlights the importance of continually refreshing foundational ML knowledge, noting a past decision favoring a neural network over potentially more efficient boosted decision trees for a large dataset on limited compute.\n* A solid, up-to-date foundation in core ML algorithms (like linear/logistic regression, decision trees) and concepts (like regularization, optimizing a loss function, bias/variance) is essential for productivity and understanding the field's rapid progress.\n* Machine learning applications, from housing-price predictors to text-to-image generators, are built upon these foundational algorithms and concepts.\n* The author is launching a new \"Machine Learning Specialization\" focused on mastering important algorithms and concepts, including deep learning.",
        "publish_date": "May 25, 2022",
        "keywords": [
            "neural network",
            "boosted decision trees",
            "linear regression",
            "decision trees",
            "regularization"
        ],
        "url": "https://www.deeplearning.ai/the-batch/a-solid-foundation-for-a-rewarding-career/"
    },
    {
        "content": "Dear friends,AI Fund, which I lead, is a venture studio that works with entrepreneurs to build companies rapidly and increase their odds of success. We’ve evaluated a lot of AI startup ideas. There’s no one-size-fits-all template for building businesses, but we’ve fine-tuned our recipes. In this and subsequent letters, I’ll share some of the patterns I’ve seen.AI businesses differ from traditional software startups in important ways. For instance, technical feasibility isn’t always clear, product specification is complex, and data is necessary to train and test the system.One important factor is whether a startup focuses on hard tech (sometimes called deep tech). A hard-tech company:Relies on advanced, better-performing technology that significantly improves the customer experience or business efficiency.Requires highly skilled teams that are capable of building materially better technology.In determining whether a business requires hard tech, the key factor is whether best-in-class technology will make the difference between success and failure.For instance, speech recognition based on deep learning was hard tech 10 years ago. Only a handful of teams were able to build highly accurate systems and put them into production at scale. Higher accuracy greatly improved the user experience, and that drove adoption. Competitors had a hard time catching up.Another example is online advertising. Building a system that selects the most relevant ad within hundreds of milliseconds is very challenging. Showing better ads results in more revenue per page view. More revenue not only improves the bottom line but makes it possible to afford higher costs to acquire users (say, by paying a maker of web browsers to feature one search engine over another). This, in turn, makes it harder for rivals to compete.What once was hard tech often becomes easier to build over time. For example, as speech recognition became commoditized, more teams were able to build useful systems. When this happens, having the best tech is much less critical to success. Other factors can have a bigger impact such as superior product design, a skilled sales team, bundling with other services, or an efficient supply chain.I enjoy working on hard-tech businesses — and many AI Fund companies fit that description — because the quality of the technology really matters. A hard-tech company has an incentive to build the best possible team, because the finest team can significantly outperform competitors.Of course, AI businesses that aren’t hard-tech can be very meaningful, too. There are many, many exciting applications, across all industries, yet to be built using established technology. We need developers going at these problems, too.Keep learning!Andrew",
        "summary": "*   AI businesses differ from traditional software startups, requiring data for training/testing, having unclear technical feasibility, and complex product specifications.\n*   A key factor for AI startups is focusing on 'hard tech' (deep tech), which relies on advanced, better-performing technology built by highly skilled teams, where best-in-class tech is critical for success.\n*   Examples of 'hard tech' include deep learning-based speech recognition (10 years ago) and complex online advertising systems, where superior technology significantly improves user experience or revenue, creating competitive barriers.\n*   What was once 'hard tech' can become easier to build over time; when technology commoditizes, having the best tech becomes less critical than factors like product design or sales.",
        "publish_date": "May 04, 2022",
        "keywords": [
            "Hard tech",
            "Speech recognition",
            "Deep learning",
            "Data for training/testing",
            "Online advertising systems"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-ai-startups-part-1-hard-tech/"
    },
    {
        "content": "Dear friends,Last week, I described trends that AI Fund, the venture studio I lead, has seen in building AI startups. I'd like to discuss another aspect of building companies that’s unique to AI businesses: the controversial topic of data moats.A company has a data moat if its access to data makes it difficult for competitors to enter its business. Moat is a common business term used evoke the water-filled moats built around castles to make them easier to defend against attackers. For example, if a self-driving car company can acquire far more data than its competitors to train and test its system, and if this data makes a material difference in the system’s performance, then its business will be more defensible.For a few years, some investors asked every AI startup’s founders about its data moat, as if they expected everyone to build one. But, like many things in AI, it depends. A data moat can provide protection, but its effectiveness varies depending on the specific circumstances of the business.For instance, a data moat may not do much to protect an AI business if:System performance plateaus with more data. Say you're building a general-purpose speech recognizer, and human-level performance is 95 percent accurate. Collecting enough data to achieve 94 percent accuracy is hard, and getting incrementally more data will have diminishing returns. In fact, it’s much easier for a competitor to improve from 90 to 91 percent accuracy than for you to improve from 94 to 95 percent.Data doesn’t change over time. If the mapping from input x to output y remains the same (as in speech recognition, where the input spoken words “The Batch” will continue to map to their text equivalents for a long time), competitors will have time to accumulate data and catch up.The application can be built with a smaller dataset thanks to new data-centric AI development technologies, including the ability to generate synthetic data, and tools that systematically improve data quality.In contrast, data can make an AI business more defensible if:Performance keeps improving materially within the range of dataset size that a company and its competitors can reasonably amass. For example, web searches form a very long tail of rare queries, which make up a large fraction of all searches. Thus, performance keeps improving for a long time as a search engine gets more clickstream data, and a dominant search engine can stay ahead of smaller outfits that try to bootstrap with little data. Generally, larger datasets tend to confer a longer-lasting benefit on applications where a large fraction of relevant data makes up a long tail of rare or hard-to-classify events.The data distribution varies significantly over time. In this case, access to an ongoing stream of fresh data is critical for keeping the machine learning model current, which in turn earns further access to the data stream. I believe this is one of the factors that makes social media companies especially defensible. The topics posted change regularly, and the ability to keep the system up-to-date helps increase its appeal relative to new competitors.The market has winner-take-all dynamics, and users have low switching costs. When a market supports only one leader, access to data that delivers even marginally better performance can be a major advantage. For instance, a ride-sharing company whose data pipeline enables passengers to reach rider destinations faster is likely to attract the most riders.Access to customer data significantly increases switching costs, reduces churn, or increases the ability to upsell. This is especially true if customers would have a hard time exporting or even making sense of their own data if they were to patronize a competitor.Data strategy is important for AI companies, and thinking through how a system’s performance varies with the amount of data, the importance of fresh data, and other factors described above can help you decide how much having data adds to a business’ defensibility. Sometimes a data moat doesn't help at all. But in other cases, it's one pillar (hopefully among many) that makes it harder for competitors to catch up.Keep learning!Andrew",
        "summary": "*   A data moat in AI businesses means data access makes competition difficult, exemplified by a self-driving car company acquiring more data for training and testing.\n*   Data moats may be less effective if system performance plateaus with more data (e.g., speech recognition near 95% accuracy), data doesn't change over time, or new data-centric AI development technologies allow smaller datasets.\n*   Data can make an AI business more defensible if performance keeps improving materially with dataset size (e.g., web search long tail), data distribution varies significantly over time requiring fresh data (e.g., social media), or access to customer data increases switching costs.\n*   Data strategy is important for AI companies, assessing data's contribution to defensibility based on factors like performance vs. data amount and the importance of fresh data.",
        "publish_date": "May 11, 2022",
        "keywords": [
            "data moat",
            "data-centric AI",
            "performance vs. data amount",
            "fresh data",
            "self-driving car"
        ],
        "url": "https://www.deeplearning.ai/the-batch/untitled-7/"
    },
    {
        "content": "Dear friends, Last week, Elon Musk launched a surprise attempt to acquire Twitter. The $43-billion bid was motivated, he said, by his desire to protect free speech endangered by the company’s practice of promoting some tweets while burying others. To that end, he proposes publishing the company’s ranking algorithm, the program that decides which tweets appear in a given user’s feed.Social media companies generally keep their ranking algorithms secret. Let’s take a look at some pros and cons of letting people see what these companies are doing behind the scenes.Why keep ranking algorithms secret?Keeping the algorithm secret arguably makes it harder for scammers and spammers to manipulate its output. Security through obscurity can’t be the only defense, but it is one barrier. It’s true, open source software can be highly secure because public scrutiny reveals holes to be patched. But I think there’s a difference between defending traditional software from hackers and defending a ranking algorithm from statistical manipulation. Rather than probing a live website, which may alert the security team, attackers can repeatedly probe an offline copy of the algorithm to find message formats that it’s likely to promote.Crucially, if the point is to enable people to understand how a learning algorithm works, then publishing it also requires publishing the data that drives it — the system’s behavior depends on both. But releasing Twitter’s data isn’t practical. One reason is the massive size of the dataset. Another is the company’s obligation to protect users’ privacy when the dataset presumably includes intimate details like user locations, interests, and times of use.Even if both the code and the data were available, the algorithm’s behavior would still be very difficult to analyze due to the black-box nature of machine learning.Proprietary algorithms confer a competitive advantage. Twitter developed its ranking algorithm at great time and expense, and it’s an important part of what differentiates the company from competitors. Publishing it would give rivals a leg up.On the other hand, there are clear benefits to making ranking algorithms public.Researchers and the broader public could gain more insight into how the algorithms work, spot problems, and evaluate the provider’s neutrality. Such scrutiny would put pressure on companies to improve flawed products and, if they were to do so, raise public confidence in their services.Given the huge impact of these algorithms on millions of people — including, perhaps, influencing the outcomes of democratic elections — there’s a case to be made that citizens and governments alike deserve to know more about how they work.Of course, overseeing ranking algorithms is only a small part of protecting free speech online. Some commentators panned Musk’s views on social media moderation as naive. Other social networks have been overrun by toxic communication, scams, and spam when they allowed people to post without restriction. Former Reddit CEO Yishan Wong offered insights into the difficulty of moderating social network posts in a widely read tweet storm.Twitter has been a valuable place for the AI community to share knowledge and perspectives, and I have deep respect for Parag Agrawal and Jack Dorsey, the current and former CEOs of Twitter, who have kept their product successful through difficult changes in social media. I also applaud its ML Ethics, Transparency and Accountability team for its insightful studies. Nonetheless, Twitter has been criticized for its business performance, which has created an opening for corporate raiders like Musk and private equity firms.Whether or not Musk’s bid is successful, the question remains: Would society be better off if internet companies were to publish their ranking algorithms? This is a complicated question that deserves more than simplistic statements about freedom of speech. My gut says “yes,” and I believe the benefit of even the partial transparency afforded by publishing the code (but not the data) would outweigh the harm. Having said that, how to secure such open-source learning algorithms, and whether demanding disclosure is fair considering the huge investment it takes to develop this intellectual property, requires careful thought.Keep learning!Andrew",
        "summary": "*   Social media ranking algorithms, programs determining content visibility, are typically kept secret, sparking debate about transparency.\n*   Arguments for secrecy include hindering statistical manipulation and the impracticality of releasing the massive, privacy-sensitive dataset required to understand the algorithm's behavior, which is also difficult to analyze due to the black-box nature of machine learning.\n*   Publishing algorithms could offer researchers and the public insight into their function, neutrality, and flaws, potentially improving products and public confidence, given their significant impact.\n*   Challenges of transparency include securing open-source learning algorithms and the fairness of disclosure considering the significant investment in developing this intellectual property.",
        "publish_date": "Apr 20, 2022",
        "keywords": [
            "social media ranking algorithms",
            "machine learning",
            "black-box nature",
            "open-source learning algorithms",
            "privacy-sensitive dataset"
        ],
        "url": "https://www.deeplearning.ai/the-batch/should-twitter-publish-its-ranking-algorithm/"
    },
    {
        "content": "Dear friends,A Google Engineer recently announced he believes that a language model is sentient. I’m highly skeptical that any of today’s AI models are sentient. Some reporters, to their credit, also expressed skepticism. Still, I worry that widespread circulation of sensationalistic reports on this topic will mislead many people. (You'll find more about it in this issue of The Batch.)The news does raise an interesting question: How would we know if an AI system were to become sentient?As I discussed in an earlier letter, whether an AI system is sentient (able to feel) is a philosophical question rather than a scientific one. A scientific hypothesis must be falsifiable. Scientific questions about AI include whether it can beat a human chess champion, accurately translate language, drive a car safely, or pass the Turing Test. These are testable questions.On the other hand, we have no clear test for whether a system is sentient, conscious (aware of its internal state and external surroundings), or generally intelligent (able to reason across a wide variety of domains). These questions fall in the realm of philosophy instead of science.Here are some examples of philosophical questions. Even though we haven't devised ways to quantify many of these terms, these questions are enduring and important:Is the nature of humankind good or evil?What is the meaning of life?Is a tree/insect/fish conscious?By the same token, many important questions that arise in discussions about AI are philosophical:Can AI be sentient? Or conscious?Can an AI system feel emotions?Can AI be creative?Can an AI system understand what it sees or reads?I expect that developing widely accepted tests for things like sentience and consciousness would be a Herculean, perhaps impossible, task. But if any group of scientists were to succeed in doing so, it would help put to rest some of the ongoing debate.I fully support work toward artificial general intelligence (AGI). Perhaps a future AGI system will be sentient and conscious, and perhaps not — I’m not sure. But unless we set up clear benchmarks for sentience and consciousness, I expect that it will be very difficult ever to reach a conclusion on whether an AI system has reached these milestones.Keep learning!Andrew",
        "summary": "*   Skepticism is expressed regarding claims that current AI models are sentient.\n*   Whether an AI system is sentient, conscious, or generally intelligent is considered a philosophical question, not a scientific one, due to the lack of clear, falsifiable tests.\n*   Scientific questions about AI, such as beating a human chess champion, accurately translating language, driving safely, or passing the Turing Test, are testable.\n*   Developing widely accepted tests for AI sentience and consciousness is expected to be a difficult task, but necessary for evaluating future artificial general intelligence (AGI).",
        "publish_date": "Jun 15, 2022",
        "keywords": [
            "Turing Test",
            "artificial general intelligence (AGI)",
            "sentience",
            "consciousness",
            "scientific question"
        ],
        "url": "https://www.deeplearning.ai/the-batch/can-an-ai-system-be-sentient-ask-a-philosopher/"
    },
    {
        "content": "Dear friends, It's official: Elon Musk will buy Twitter, pending approval of the transaction by the company's stockholders and the U.S. government. While some people are celebrating the deal in the name of free speech, others are worried about the platform’s future. Will the rules change to favor Musk’s personal views? Will trolling, harassment, and disinformation run rampant?I hope the change in management will improve governance and conversation on Twitter. But I wonder whether an open standard for social media might be a better way to improve social networks.Think about email. The open protocol SMTP has enabled many companies to provide email services so that anyone with an email address can communicate freely with anyone else, regardless of their provider. A similar open standard could underpin social media.Platforms like Facebook, Instagram, LinkedIn, and Twitter implement similar features like posting, liking, commenting and sharing. Why not enable key features to work across all platforms, including newcomers? This would permit users to interact even if their accounts were on different platforms, just as people who have email accounts with Gmail, Outlook, Yahoo, or any other provider can communicate with each other.Open standards for social media have been discussed for a long time. Some people argue that only a central gatekeeper can moderate online conversations effectively, so they don’t degenerate into toxicity. This is false. Again, think of email. Spam filters do a good job of eliminating toxic messages, and the fact that different providers filter spam in different ways allows consumers to choose the gatekeeper they like best — or none at all. Meanwhile, adherence to an open protocol has prevented any single company from monopolizing email.Open standards have driven huge amounts of innovation in computing and communications. They do evolve slowly, by committee. But when a technology is sufficiently mature, setting an open standard makes it difficult for any one company to change the rules to benefit themselves at others’ expense. Any developer can plug into an ecosystem, and the best implementations rise to the top. In contrast, proprietary platforms can change on a whim to, say, charge to reach followers or disallow apps from sharing. This makes it harder for innovators to build large and thriving businesses.The web is another example. The HTTP protocol lets developers worldwide build whatever website they want. The resulting wave of innovation has lasted for decades. When Larry Page and Sergei Brin wanted to set up google.com, no one could stop them, and it was up to them to make it work. Yes, HTTP has spawned scams such as pushing schemes that lure victims to bogus websites, but competition in web browsers ensures that users have a choice of anti-phishing gatekeepers. This helps keep the web ecosystem healthy.Creating an open standard for social media and getting many companies and users to adopt it would be difficult. It would require technical contributions from computer scientists and likely an assist from regulators. It would push against the tide of Facebook-style walled gardens (in which a single company sets rules and access to content).The recent U.S. court ruling that legalized scraping websites is a welcome step toward the free flow of information online. Standards that ensure interoperability among social media platforms would be another, major step.Keep learning!Andrew",
        "summary": "*   The text cites \"spam filters\" as effective distributed gatekeepers for eliminating toxic messages in email under an open standard.\n*   A U.S. court ruling legalized \"scraping websites\" as a step towards the free flow of information online.\n*   The text discusses the challenge of moderating online conversations to prevent toxicity and disinformation, suggesting open standards could allow choice in moderation.",
        "publish_date": "Apr 27, 2022",
        "keywords": [
            "spam filters",
            "scraping websites",
            "moderating online conversations",
            "toxic messages",
            "disinformation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-truly-free-social-networks/"
    },
    {
        "content": "Dear friends,  , the venture studio and investment firm that I lead, recently held a summit where CEOs and founders of portfolio companies shared ideas on topics from fundraising to building team culture. I was struck by how frequently startup leaders have to do things they have no expertise in. As AI developers, every time we build a machine learning application, we might choose a neural network architecture, tune a dataset, train a model, evaluate its performance, and consider the outcome to decide what to try next. The rapid iteration cycle means we can try many combinations in a given project. Over many projects, we hone our intuitions about what works. The quick feedback and opportunity to improve are among the things that makes machine learning fun! In contrast, hardly anyone starts 100 companies even in a long career. No one raises seed funding, builds a company culture, hires a vice president of sales, or makes an initial public offering very many times. Thus few people can become experts at performing these tasks through repeated practice. That’s why I believe that the smartest startup leaders know when they need help and understand that no single person can do it all. A community of peers, each of whom has raised funding once or twice, can pool ideas and achieve better results than the typical individual. Similarly a recruiter who has hired 100 sales executives is likely to have valuable insights that someone who has done it only once or twice won’t. Although software development allows for repeated practice, we, too, often have to do things we don’t have much experience with, because technology keeps evolving. Someone may find themselves, for the first time, deploying a real-time machine learning system, compressing a neural network to run on a low-power edge device, or calculating the return on investment in an AI project. In situations like this we, too, are stronger as a community. We can benefit from the experience of our peers who have completed the task and know something about how to go about it. When I was younger I believed that, if only I worked and studied a bit harder, I could figure almost anything out. That attitude worked well enough for a while, but the more experience I gain, the more I realize that I need help from others. I’m grateful to the many people who have given me advice over the years, and I hope that the AI community can be a place where all of us can collaborate and support one another. Keep learning! Andrew",
        "summary": "*   AI developers build machine learning applications through rapid iteration cycles involving choosing architecture, tuning data, training, and evaluating performance.\n*   The quick feedback and opportunity to improve are highlighted as enjoyable aspects of machine learning development.\n*   Due to evolving technology, AI professionals often encounter tasks they lack experience in, such as deploying real-time ML systems, compressing neural networks for edge devices, or calculating AI project ROI.\n*   The AI community is stronger through collaboration and peer support, benefiting from shared experience when facing new or unfamiliar tasks.",
        "publish_date": "Mar 30, 2022",
        "keywords": [
            "real-time ML systems",
            "compressing neural networks",
            "edge devices",
            "architecture",
            "tuning data"
        ],
        "url": "https://www.deeplearning.ai/the-batch/knowing-when-you-need-help/"
    },
    {
        "content": "Dear friends, The physical world is full of unique details that differ from place to place, person to person, and item to item. In contrast, the world of software is built on abstractions that make for relatively uniform coding environments and user experiences. Machine learning can be a bridge between these two worlds. Software is largely homogenous. When a search-engine company or smartphone maker upgrades its product, users all over the world are offered the same upgrade. This is economically efficient because, despite high fixed costs for design and manufacturing, it results in low marginal costs for manufacturing and distribution. These economics, in turn, support huge markets that can finance innovation on a grand scale. In contrast, the real world is heterogeneous. One city is surrounded by mountains, another by plains, yet another by seas. One has paved roads, another dirt tracks. One has street signs in French, another in Japanese. Because of the lack of platforms and standards — or the impossibility of creating them — one size doesn’t fit all. Often it fits very few. This is one reason why it’s difficult to design a self-driving car. Making a vehicle that could find its way around safely would be much easier if every city were built to a narrow specification. Instead, self-driving systems must be able to handle streets of any width, stop lights in any configuration, and a vast array of other variables. This is a tall order even for the most sophisticated machine learning systems. Software companies have been successful at getting users to adapt to one-size-fits-all products. Yet machine learning could help software capture and interact with the rich diversity of the physical world. Rather than forcing every city to build streets of the same composition, width, color, markings, and so on, we can build learning algorithms that enable us to navigate the world’s streets in all their variety. We have a long way to go on this journey. Last week, I wrote about how Landing AI is using data-centric AI to make machine learning work under the wide variety of conditions found in factories. When I walk into a factory, I marvel at how two manufacturing lines that make an identical product may be quite different because they were built a few years apart, when different parts were available. Each factory needs its own trained model to recognize its own specific conditions, and much work remains to be done to make machine learning useful in such environments. I hope that you, too, will see the heterogenous world you live in and marvel at the beautiful diversity of people, buildings, objects, and cultures that surround you. Let’s use machine learning to better adapt our software to the world, rather than limit the world to adapt to our software. Keep learning! Andrew",
        "summary": "*   Machine learning can bridge the gap between homogenous software and the heterogeneous physical world.\n*   The heterogeneity of the real world poses challenges for ML applications like self-driving cars, requiring systems to handle vast variables.\n*   Data-centric AI is being used (e.g., by Landing AI) to enable machine learning to work under the wide variety of conditions found in heterogeneous environments like factories.\n*   Making ML useful in heterogeneous environments requires specific trained models for unique conditions, such as different factory manufacturing lines.",
        "publish_date": "Nov 17, 2021",
        "keywords": [
            "Data-centric AI",
            "Landing AI",
            "Self-driving cars",
            "Heterogeneous environments",
            "Data variability"
        ],
        "url": "https://www.deeplearning.ai/the-batch/making-software-for-a-heterogeneous-world/"
    },
    {
        "content": "Dear friends,I’m grateful to the AI community for the friendships it has brought me and the benefits it has brought to billions of people. But members of the AI community don’t always honor one another. In the spirit of Thanksgiving, which we in the U.S. celebrate this week, I’d like to talk about how we can treat each other with greater civility.While AI has done much good, it has also created adverse effects. Machine learning systems have perpetuated harmful stereotypes, generated results that treat some minority groups unfairly, aided the spread of disinformation, and enabled some governments to oppress their citizens. It’s up to us to find, call out, and solve these problems.But there’s a difference between airing problems so we can work toward a solution and attacking fellow AI developers for their perceived sins. We’re sometimes too quick to attack each other on social media when we have disagreements. Misdirected criticisms can go viral before a correction can catch up.I’ve seen many events that people may have misconstrued:A workshop had a slate of invited speakers who were all of one gender and lacked diversity in other dimensions. The organizer must have been biased, right? Actually, the group was fairly diverse until several speakers unexpectedly canceled at the last minute, leaving a homogeneous slate.A vision algorithm favored a light skinned person over a dark skinned person. Clearly the algorithm was racist, and possibly the people who built it as well, right? But when its performance was examined on a larger set of data, this appeared to be an isolated example rather than a pervasive trend.Members of a majority group found a certain word derogatory toward a particular minority and had it removed from public communications. Anyone using it must be insensitive and ignorant, right? It turned out the minority group in question didn’t consider the word derogatory. Perhaps the critics were mistaken.To be clear, the AI world has problems. I don’t want anyone to shy away from addressing them. When you come across a pressing issue, here are suggestions that might encourage productive conversation:Reach out privately. When you see someone doing something you consider problematic — perhaps even unethical — give them a chance to explain why, or make sure someone else has, before you fire off that explosive tweet. Perhaps they had an innocent, or even positive, reason for their actions that you weren’t aware of.Encourage transgressors to correct their mistakes. If you find that a scientist has made an error, try privately to persuade them to publish a correction or retraction. That can fix the problem while preserving their dignity. If you reach out and find them immovable or they refuse to engage, you can still call them out publicly and make sure the truth gets out.Don’t be cowed. If you find a real problem, and you’ve spoken with the people at the center of it and found that more needs to be said in public, go for it! If you’re not sure, consider asking colleagues to help you double-check your thinking, consider other perspectives, and gather allies who can help you push forward.As we wrestle with important issues around values, ethics, diversity, and responsibility, let’s keep our arguments civil and support discussions that focus on solving problems rather than public shaming. In addition to being civil yourself, I ask you also to encourage others to be civil, and think twice before repeating or amplifying messages that aren’t. The AI community faces difficult challenges, and working together will make us more effective in wrestling with them.Happy Thanksgiving and keep learning!Andrew",
        "summary": "*   AI systems have created adverse effects, including perpetuating harmful stereotypes, treating minority groups unfairly, aiding disinformation spread, and enabling oppression.\n*   The AI community must find, call out, and solve these problems but is urged to treat each other with greater civility, avoiding quick public attacks.\n*   Examples are provided of situations potentially misconstrued as bias, such as workshop speaker diversity changes, vision algorithm performance variations, and differing views on derogatory language.\n*   Suggested approaches for addressing issues include reaching out privately first, encouraging transgressors to correct mistakes, and publicly calling out only if necessary after private attempts fail.\n*   The text emphasizes maintaining civil arguments and focusing on solving problems rather than public shaming when wrestling with AI issues around values, ethics, diversity, and responsibility.",
        "publish_date": "Nov 24, 2021",
        "keywords": [
            "Bias",
            "Harmful stereotypes",
            "Disinformation",
            "Vision algorithm",
            "Ethics"
        ],
        "url": "https://www.deeplearning.ai/the-batch/solving-ai-problems-with-civility/"
    },
    {
        "content": "Dear friends,I’ve seen many new technologies go through a predictable process on their journey from idea to large scale adoption.First, a handful of experts apply their ideas intuitively. For example, 15 years ago, a handful of individuals were building neural networks from scratch in C++. The work was error-prone, and only a small number of people knew how to get such models to work.As the ideas become more widespread and publications describe widely applicable principles, more people can participate. In the example above, around five years later, a growing number of people were able to code up deep learning models in C++. It was still error-prone, but knowledge of how to do it became more widespread.Eventually, developer tools make it much easier for many people to take part. For instance, frameworks like TensorFlow and PyTorch made building neural networks simpler and more systematic, and implementations were much less likely to fail due to a stray C++ pointer.The data-centric AI movement is going through such a process. Data-centric AI is the growing discipline of systematically engineering the data needed to build successful AI systems. This contrasts with the model-centric approach, which focuses on inventing and tuning machine learning model architectures while holding the data fixed.Experienced machine learning practitioners have been engineering data by hand for decades. Many have made learning algorithms work by improving the data — but, even when I was doing it years ago, I didn’t have the language to explain why I did things in a certain way.Now more and more teams are articulating principles for engineering data. I’m seeing exciting processes for spotting data inconsistencies, accelerating human labeling, applying data augmentation, and crowdsourcing more responsibly. Finally, just as TensorFlow and PyTorch made building neural networks more systematic, new tools are starting to emerge. Landing AI (where I am CEO) is building a platform for computer vision applications, and I expect many more tools to be built by different companies for different applications. They will enable teams to take what once was an ad hoc set of ideas and apply the right process at the right time.The tech community has gone through this process for code versioning (leading to tools like git) and transfer learning (where GPT-3, which was pre-trained on a massive amount of text, represents an early version of a tool). In less mature areas like reinforcement learning, I believe we’re still developing principles.If you’re interested in learning more about the principles and tools of data-centric AI, we’re holding a workshop at NeurIPS on December 14, 2021. Dozens of great researchers will present poster sessions and lectures on cutting-edge topics in the field.Keep learning!Andrew",
        "summary": "*   Data-centric AI is the discipline of systematically engineering data for successful AI systems, contrasting with the model-centric approach focused on tuning models with fixed data.\n*   The field is progressing from intuitive data engineering by experts to articulating principles for practices like spotting inconsistencies, accelerating labeling, and data augmentation.\n*   New tools are emerging to systematize data-centric AI, similar to how frameworks like TensorFlow and PyTorch systematized neural network building.\n*   This development follows a pattern seen in other tech areas like code versioning (git) and transfer learning (GPT-3 as an early tool example).",
        "publish_date": "Dec 01, 2021",
        "keywords": [
            "Data-centric AI",
            "model-centric approach",
            "data augmentation",
            "TensorFlow",
            "PyTorch"
        ],
        "url": "https://www.deeplearning.ai/the-batch/toward-systematic-data-engineering/"
    },
    {
        "content": "Dear friends,We just wrapped up the Data-Centric AI Workshop at the NeurIPS 2021 conference. It was packed with information about how to engineer data for AI systems. I wish the whole DeepLearning.AI community could have been there! I expect the videos to be available before long and will let you know when they’re onlineOver the course of an eight-hour session, authors presented 100 papers via two-minute lightning talks and posters. Eight invited speakers described a variety of data-centric AI issues and techniques, and expert panels answered questions from the audience.These were some of my key takeaways:There’s a lot going on in data-centric AI — even more than I realized. I was also surprised by the variety of ideas presented on how to measure, engineer, and improve data. Several participants expressed variations on, “I’ve been tuning the data by myself for a long time, and it’s great to finally find a like-minded and supportive community to discuss it with.”Many diverse applications are using data-centric AI in areas including chatbots, content moderation, healthcare, document scanning, finance, materials science, speech, and underwater imaging. They take advantage of clever techniques for spotting incorrect labels, crowdsourcing, generating data, managing technical debt, managing data pipelines, benchmarking, and more.An immense amount of innovation and research lies ahead. We’re working collectively to coalesce broadly useful data-centric principles and tools. But, given the richness of the problems that remain open, it will take many years and thousands of research papers to flesh out this field.Among the invited speakers:Anima Anandkumar showed sophisticated synthetic data techniques.Michael Bernstein shared tips for making crowdsourcing much more effective.Douwe Kiela demonstrated DynaBench as a tool for creating new data-centric benchmarks.Peter Mattson and Praveen Paritosh described efforts to benchmark data including a plan by MLCommons to continue developing projects like DataPerf.Curtis Northcutt described the CleanLab system, which made it possible to find many labeling errors in the test sets of widely used datasets like MNIST and ImageNet.Alex Ratner described a programmatic approach to Data-Centric AI.Olga Russakovsky presented a tool for de-biasing large datasets.D. Scully discussed the role of data-centric AI in addressing technical debt in machine learning systems.I also enjoyed hearing participants in DeepLearning.AI and Landing AI’s Data-centric AI Competition speak about their submissions. You can read some of their blog posts here.Thanks to everyone who participated in the workshop or submitted a paper; to the presenters, panelists, invited speakers, and poster presenters; and to the reviewers, volunteers, and co-organizers who put the program together.I was struck by the energy, momentum, and camaraderie I felt among the participants. I came away more excited than ever to keep pushing forward the data-centric AI movement, and I remain convinced that this field will help everyone build more effective and fairer AI systems.Keep engineering your data!Andrew",
        "summary": "*   The Data-Centric AI Workshop at NeurIPS 2021 focused on engineering data for AI systems, featuring 100 papers presented via lightning talks and posters, and 8 invited speakers.\n*   The workshop covered diverse applications (e.g., chatbots, healthcare, finance, speech, underwater imaging) and techniques like spotting incorrect labels, crowdsourcing, data generation, managing technical debt, and benchmarking.\n*   Specific data-centric AI techniques discussed included sophisticated synthetic data, effective crowdsourcing, benchmarking tools (DynaBench, DataPerf), systems for finding labeling errors (CleanLab in MNIST, ImageNet test sets), de-biasing tools, and programmatic approaches.\n*   The field of data-centric AI is active but requires immense future innovation and research to develop broadly useful principles and tools.",
        "publish_date": "Dec 15, 2021",
        "keywords": [
            "CleanLab",
            "DynaBench",
            "DataPerf",
            "Synthetic data",
            "Crowdsourcing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-neurips-data-centric-ai-workshop-was-amazing/"
    },
    {
        "content": "Dear friends,AI continues to create numerous exciting career opportunities, and I know that many of you aim to develop a career in the field. While taking online courses in technical topics is an important step, being an AI professional requires more than technical skills. Lately I’ve been thinking about how to do more to support all of you who are looking to build a career in AI.Considering individuals at a variety of stages in their careers, what are some of the keys to success?Technical skills. When learning a new skill, taking an online course or reading a textbook — in which an expert presents important concepts into an easy-to-digest format — is one of the most efficient paths forward.Practical experience. After gaining a skill, it’s necessary to practice it — and learn tricks of the trade — by applying that skill to significant projects. Machine learning models that perform well in the lab can run into trouble in the real world. Practical project experience remains an important component in overcoming such problems.Project selection. Choosing projects to work on is one of the hardest skills in AI. We can only work on so many projects at a time, and scoping ones that are both feasible and valuable — so they have a good chance of achieving meaningful success — is an important step that has to be done repeatedly in the course of a career.Teamwork. When we tackle large projects, we succeed better by working in teams than individually. The ability to collaborate with, influence, and be influenced by others is critical. This includes both interpersonal and communication skills. (I used to be a pretty bad communicator, by the way.)Networking. I hate networking! As an introvert, having to go to a party to smile and shake as many hands as possible is an activity that borders on horrific. I’d much rather stay home and read a book. Nonetheless, I’m fortunate to have found many genuine friends in AI; people I would gladly go to bat for and who I count on as well. No person is an island, and having a strong professional network can help propel you forward in the moments when you need help or advice.Job search. Of all the steps in building a career, this one tends to receive the most attention. Unfortunately, I’ve found a lot of bad advice about this on the internet. (For example, many articles seem to urge taking an adversarial attitude toward potential employers, which I don’t think is helpful). Although it may seem like finding a job is the ultimate goal, it’s just one small step in the long journey of a career.Personal discipline. Few people will know if you spend your weekends learning or binge watching TV (unless you tell them on social media!), but they will notice the difference over time. Many successful people develop good habits in eating, exercise, sleep, personal relationships, work, learning, and self-care. Such habits help them move forward while staying healthy.Altruism. I find that individuals who aim to lift others during every step of their own journey often achieve better outcomes for themselves. How can we help others even as we build an exciting career for ourselves?Each of these items is a complex subject worthy of an entire book. I will continue to think on how we can work collectively to support everyone’s career goals. Meanwhile, I would like to hear your thoughts as well. What am I missing? What can I or my teams do to support you in your career?Keep learning!Andrew",
        "summary": "* Building an AI career requires foundational technical skills learned through courses and textbooks.\n* Practical experience applying skills to significant projects is essential for navigating real-world challenges beyond lab performance.\n* Success involves strategic project selection, choosing feasible and valuable projects, and effective teamwork, including collaboration and communication.\n* Networking, personal discipline (developing good habits), and altruism (helping others) are also important factors for long-term career success in AI.",
        "publish_date": "Jan 12, 2022",
        "keywords": [
            "technical skills",
            "projects",
            "practical experience",
            "real-world challenges",
            "lab performance"
        ],
        "url": "https://www.deeplearning.ai/the-batch/8-keys-to-building-a-career-in-ai/"
    },
    {
        "content": "Dear friends,As we approach the end of 2021, you may be winding down work and gearing up for the winter holiday. I’m looking forward to taking a break from work and hope you are too.December is sometimes called the Season of Giving. If you have spare time and are wondering what to do with it, I think one of the best things any of us can do is to reflect on how we can help others.When the AI community was small, there was a strong spirit of cooperation. It felt like an intrepid band of pioneers taking on the world, and people were eager to help others with advice, an encouraging word, or an introduction. Those who benefited from this often couldn’t pay it back, so we paid it forward by helping those who came after us. As the AI community grows, I would like to preserve this spirit. I promise to keep working to build up the AI community. I hope you will, too!I also hope that you will consider ways — large or small — that you can lend a helping hand beyond the AI community. Many of us have access to advanced technology that much of the world does not. Collectively, our decisions move billions of dollars and affect billions of lives. This gives us a special opportunity to do good in the world.“We are what we repeatedly do,” said historian and philosopher Will Durant (often misattributed to Aristotle). If you repeatedly seek to uplift others, not only does this help them but — perhaps equally important — it makes you a better person, too, for it is your repeated actions that define you as a person. There’s also a classic study that shows spending money on others may make you happier than spending money on yourself.So, during this holiday season, I hope you’ll take some time off. Rest, relax, and recharge! Connect with loved ones if you haven’t done so frequently enough the past year. And if time permits, find something meaningful you can do to help someone else, be it leaving an encouraging comment on a blog post, sharing advice or encouragement with a friend, answering an AI question in an online forum, or making a donation to a worthy cause. Among charities relevant to education and/or tech, my favorites include the Wikimedia Foundation, Khan Academy, Electronic Frontier Foundation, and Mozilla Foundation. You can pick something meaningful to you from this list of organizations vetted by Charity Watch.In the U.S., many parents tell their children that Santa Claus, the jolly character who leaves gifts in their homes at this time of year, is a magical being. When the kids grow up, they learn that Santa Claus isn’t real. Can we, as adults, be real-life Santa Clauses ourselves and give the gifts of our time, attention, or funds to someone else?Love,Andrew",
        "summary": "* The AI community historically had a strong spirit of cooperation and \"paying it forward\" when it was small.\n* There is a desire to preserve this spirit as the AI community grows.\n* Members of the AI community are encouraged to help others, including answering AI questions in online forums and leveraging their access to technology and influence.",
        "publish_date": "Dec 22, 2021",
        "keywords": [
            "AI community",
            "AI questions",
            "online forums",
            "cooperation",
            "leveraging access"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-spirit-of-giving-in-ai-and-beyond/"
    },
    {
        "content": "Dear friends,With the rise of software engineering over several decades, many principles of how to build traditional software products and businesses are clear. But the principles of how to build AI products and businesses are still developing. I’ve found that there are significant differences, and I’ll explore some of them in this and future letters.That AI enables new categories of products and businesses is a familiar theme. However, using this new technology — whether in a startup going from 0 to 1 or a large company incubating a new product — brings special challenges:Unclear technical feasibility. It’s relatively well understood what a traditional mobile app or web app can do. If you can draw a reasonable wireframe, you can probably build it. But until you’ve examined the data and run some experiments, it’s hard to know how accurate an AI system can be in a given application. For example, many technologists overestimated how easy it would be to build an acceptably safe self-driving car. Generally, AI startups bring higher technical risk than traditional software startups because it’s harder to validate in advance if a given technology proposal is feasible.Complex product specification. The specification for a traditional web app might come in the form of a wireframe, but you can’t draw a wireframe to indicate how safe a self-driving car must be. It’s extremely complex to specify operating conditions (sometimes also called the operational design domain) and acceptable error rates under various conditions. Similarly, it can be hard to write a spec for a medical diagnosis tool, depending on how acceptable different types of errors are (since not all errors are equally severe). Further, product specs often evolve as the team discovers what is and isn’t technically feasible.Need for data. To develop a traditional software product, you might (a) interview users to make sure they want what you aim to build, (b) show them a wireframe to make sure your design meets their needs, and (c) dive into writing the code. If you’re building an AI product, you need to write code, but you also need access to data to train and test the system. This may not be a big challenge. For a consumer product, you may be able to start with a small amount of data from an initial cohort of users. But for a product aimed at business customers — say, AI to optimize shipping or help a hospital manage its medical records — how can you get access to shipping data or medical records? To work around this chicken-and-egg problem, some AI startups start by doing consulting or NRE (non-recurring engineering) work. Those activities are hard to scale, but they afford access to data that can shape a scalable product.Additional maintenance cost. For traditional software, the boundary conditions — the range of valid inputs d — are usually easy to specify. Indeed, traditional software often checks the input to make sure, for example, it’s getting an email address in a field dedicated to that input. But for AI systems, the boundary conditions are less clear. If you have trained a system to process medical records, and the input distribution gradually changes (data drift/concept drift), how can you tell when it has shifted so much that the system requires maintenance?Because of these differences between traditional software and AI, the best practices for building AI businesses are different. I’ll dive deeper into these differences in future letters. Meanwhile, please ask your business friends to subscribe to The Batch if they want to understand how to build an AI business!Keep learning!Andrew",
        "summary": "*   Unclear technical feasibility makes it hard to validate AI system accuracy or safety in advance, leading to higher technical risk for AI startups compared to traditional software.\n*   Complex product specification for AI requires defining operating conditions and acceptable error rates, with specs often evolving based on technical feasibility.\n*   Building AI products necessitates access to data for training and testing, which can be a challenge for business customers, sometimes requiring consulting or NRE work to obtain.\n*   AI systems incur additional maintenance costs due to less clear boundary conditions and the need to address data drift/concept drift when input distributions change.",
        "publish_date": "Jun 23, 2021",
        "keywords": [
            "data drift/concept drift",
            "accuracy",
            "error rates",
            "training",
            "testing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-developing-ai-products-is-different-from-traditional-software/"
    },
    {
        "content": "Dear friends,I’m thrilled to announce the first data-centric AI competition! I invite you to participate.For decades, model-centric AI competitions, in which the dataset is held fixed while you iterate on the code, have driven our field forward. But deep learning has matured to the point that, for many applications, an open-source model works just fine — if we can prepare the right data to train it. What we urgently need now are methods, tools, and platforms for getting the data we need efficiently and systematically.This competition, a collaboration between Landing AI and DeepLearning.AI, offers an opportunity to develop methods for improving data.In the grand tradition of MNIST, the dataset assembled by Yann LeCun and his colleagues that has driven much model-centric progress, this competition will use a new dataset called Roman MNIST. It’s a noisy collection of handwritten Roman numerals to serve as a starting point for making a dataset for this task.Can you develop a dataset that results in the best performance on this problem?The competition will end on September 4, 2021 — the birthday of John McCarthy, who coined the term artificial intelligence. The winners will be invited to join me at a private roundtable event to share ideas about how to grow the data-centric movement, and I will highlight their work here in The Batch.I’m grateful to Chris Re at Stanford and D Sculley at Google for advising us on this competition, and to everyone who contributed their thoughts on social media.There will be more data-centric AI competitions in the future. But if you join this one with me, you’ll be able to tell your friends that you were there at the very beginning of the data-centric AI movement! You’ll find further information here.Keep preparing data!Andrew",
        "summary": "*   The first data-centric AI competition is announced, contrasting with traditional model-centric competitions where the dataset is fixed.\n*   Focused on developing methods for improving data, it addresses the urgent need for efficient data preparation tools for mature deep learning models.\n*   The competition uses a new dataset, Roman MNIST (a noisy collection of handwritten Roman numerals), analogous to the MNIST dataset, and is a collaboration between Landing AI and DeepLearning.AI.\n*   Ending on September 4, 2021, this competition is presented as the beginning of the data-centric AI movement.",
        "publish_date": "Jun 16, 2021",
        "keywords": [
            "data-centric AI",
            "Roman MNIST",
            "Landing AI",
            "DeepLearning.AI",
            "data preparation tools"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ready-set-improve-the-data/"
    },
    {
        "content": "Dear friends,Over the last two weeks, I described the importance of clean, consistent labels and how to use human-level performance (HLP) to trigger a review of whether labeling instructions need to be reviewed.When training examples are labeled inconsistently, an AI that beats HLP on the test set might not actually perform better than humans in practice. Take speech recognition. If humans transcribing an audio clip were to label the same speech disfluency “um” (a U.S. version) 70 percent of the time and “erm” (a UK variation) 30 percent of the time, then HLP would be low. Two randomly chosen labelers would agree only 58 percent of the time (0.72 + 0.33). An AI model could gain a statistical advantage by picking “um” all of the time, which would be consistent with 70 percent of the time with the human-supplied label. Thus, the AI would beat HLP without being more accurate in a way that matters.Labeling training data consistently is particularly important for small data problems. Innovations like data synthesis using generative adversarial networks, data augmentation, transfer learning, and self-supervision expand the possibilities for small data. But when I’m trying to train a neural network on 1,000 examples, the first thing I do is make sure they’re labeled consistently.Let’s continue with last week’s example of determining if a scratch is significant based on its length. If the labels are noisy — say, different labelers used different thresholds for labeling a scratch as significant (the left-hand graph in the image above)¸— an algorithm will need a large number of examples to determine the optimal threshold. But if the data were clean — if all the labelers agree on the length that causes the label to switch from 0 to 1 (the right-hand graph) — the optimal threshold is clear.Learning theory affirms that the number of examples needed is significantly lower when the data is consistently labeled. In the simple example above, the error decreases on the order of {1 / √ m} in the case on the left, and {1/m} in the case on the right, where m is the training set size. Thus, error decreases much faster when the labels are consistent, and the algorithm needs many fewer examples to do well.Clean labels are generally helpful. You might be better able to get away with noisy labels when you have 1 million examples, since the algorithm can average over them. And it’s certainly much harder to revise 1 million labels than 1,000. But clean labels are worthwhile for all machine learning problems and particularly important if you’re working with small data.Keep learning!Andrew",
        "summary": "* Clean, consistent labels are important for AI training, and Human-Level Performance (HLP) can help identify inconsistent labeling.\n* Inconsistent labels can lead to AI statistically beating HLP without practical improvement, as shown in a speech recognition example (\"um\" vs \"erm\").\n* Consistent labeling is particularly important for small data problems.\n* Learning theory indicates error decreases faster with consistent labels (order of 1/m) compared to inconsistent labels (order of 1/√m), requiring fewer examples.\n* Clean labels are generally helpful for all machine learning problems, especially with small data, though noisy labels might be more manageable with 1 million examples.",
        "publish_date": "Nov 25, 2020",
        "keywords": [
            "Human-Level Performance (HLP)",
            "Learning theory",
            "small data",
            "noisy labels",
            "Sample Complexity"
        ],
        "url": "https://www.deeplearning.ai/the-batch/data-centric-ai-development-small-data-problems/"
    },
    {
        "content": "Dear friends,Last week, I wrote about the limitation of using human-level performance (HLP) as a metric to beat in machine learning applications for manufacturing and other fields. In this letter, I would like to show why beating HLP isn’t always the best way to improve performance.In many machine learning problems, labels are determined by a person who evaluates the same sort of input as a learning algorithm would. For instance, a human labeler may look at a picture of a phone to determine if it’s scratched, and an algorithm would examine a similar picture to learn to detect scratches. (Note that this is not always the case. A human labeling a cancer diagnosis on an X-ray image may also rely on a tissue biopsy from the patient, while an algorithm would use the resulting dataset to learn to diagnose cancer based on images alone.)In cases where labels were determined by a human by looking at the same input that an algorithm would, what are we to make of situations in which HLP is well below 100 percent? This just means that different people labeled the data differently. For example, the ground-truth labeler who created a test set may have labeled a particular phone as scratched, while a different labeler thought the same phone was not scratched, and thus made a mistake in marking this example. If the second labeler disagreed with the ground-truth labeler on 1 out of 10 examples, then HLP in this task would be 90 percent.In this situation, rather than trying to build a learning algorithm that achieves 91 percent accuracy, it would be better to look into how the two labelers formed their judgements and try to help them make their labels more consistent.For example, all labelers may agree that scratches smaller than 1 mm are not significant (y=0), and scratches greater than 3 mm are significant (y=1), but they label scratches between 1 mm and 3 mm inconsistently. If we can spot this problem and get the labelers to agree on a consistent standard — say, that 1.5 mm is the point at which the labels should switch from y=0 to y=1 — then we’ll end up with less noisy labels.Setting standards that make labels more consistent will actually raise HLP, because humans now agree with one another more frequently. At the same time, having more consistently labeled data will result in better machine learning performance. This improvement is more important in many practical applications than the academic question of whether an algorithm beat HLP.HLP does have a role to play in establishing baseline performance for estimating irreducible, or Bayes, error, which in turn helps with error analysis. You can learn more about this in Deep Learning Specialization Course 3 and Machine Learning Yearning.But the message I hope you’ll take away from this letter is that, when a human labeler has created the class labels that constitute ground truth and HLP is significantly less than 100 percent, we shouldn’t just set out to beat HLP. We should take the deficit in human performance as a sign that we should explore how to redefine the labels to reduce variability.Keep learning!Andrew",
        "summary": "*   Human-Level Performance (HLP) is not always the optimal metric to surpass in ML applications, particularly when HLP is significantly below 100%.\n*   When HLP is below 100% and labels are human-determined from the same input as the algorithm, it often indicates inconsistency among human labelers.\n*   Improving label consistency by setting clearer standards is more beneficial than simply trying to surpass the current HLP, as it raises HLP and enhances ML performance.\n*   HLP serves a role in establishing a baseline for estimating irreducible (Bayes) error and for error analysis.",
        "publish_date": "Nov 18, 2020",
        "keywords": [
            "Human-Level Performance (HLP)",
            "Label consistency",
            "Human labelers",
            "Bayes error",
            "Error analysis"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-versus-human-level-performance-part-2/"
    },
    {
        "content": "Dear friends, The rise of AI creates opportunities for new startups that can move humanity forward. In the 1990s, the internet was embraced successfully by incumbent companies including Apple and Microsoft, but it also inspired hugely impactful startups like Amazon, Facebook, and Google. Similarly, AI now is empowering forward-looking incumbent companies — many of them former internet startups — and creating massive opportunities for new startups as well.I’ve been thinking about what I can do to help members of the DeepLearning.AI community who wish to create a company. At AI Fund (where I am managing general partner), I speak with many entrepreneurs who have either started or are thinking of starting a new company. I’ve noticed a few factors that increase the odds of success:Domain knowledge coupled with identification of a problem: Do you deeply understand an industry and a specific pain point? Have you experienced and struggled with solving the problem yourself?Initial hypothesis of a solution: Do you have a sense that AI-based automation can lead to a solution? Is it technically feasible and likely to solve the problem in a responsible and value-creating way?Large market opportunity: Is there a large number of potential customers who have a similar problem?Drive and grit: Startups move forward only because the people involved make it happen. Are you ready to struggle through the hard work, pain, and uncertainty that comes with starting a company?Many startup founders quietly obsess about startup ideas for years, since it can take a lot of thought and investigation to work out the nuances. (Before I cofounded Coursera, I had spent about five years obsessing over how to deliver effective online education. You can read more about my early experiences in “Origins of the Modern MOOC.”)Identifying a problem is one of the hardest steps. I didn’t understand this until I saw a lot of examples. So many things compete for attention in today’s world (in both business-to-business and business-to-consumer settings) that unless your offering creates compelling value, it’s hard to get people to pay attention. One test of a problem you’ve identified is: Have a number of people told you they would go to the trouble of exploring possible solutions?I’d love to hear from those of you who are, or aspire to become, entrepreneurs. My teams at DeepLearning.AI and AI Fund plan to hold a series of entrepreneur-oriented events next year. If the success factors I listed above describe you, and especially if you’re still in the early stages (say, from having identified a problem but not yet decided to start a company to having built a product and being ready to raise capital), please take this short survey and let us know how we can help you in your startup journey.Keep learning!Andrew",
        "summary": "*   AI is creating significant opportunities for new startups, akin to the internet era, alongside empowering incumbent companies.\n*   Key factors for increasing AI startup success include deep domain knowledge, identifying a specific pain point, hypothesizing a technically feasible and value-creating AI-based automation solution, a large market opportunity, and founder drive/grit.\n*   Identifying a compelling problem that creates significant value is a crucial and difficult step for startups.",
        "publish_date": "Dec 02, 2020",
        "keywords": [
            "AI",
            "AI-based automation solution",
            "Technically feasible",
            "Deep domain knowledge",
            "Value-creating"
        ],
        "url": "https://www.deeplearning.ai/the-batch/advice-for-start-up-founders/"
    },
    {
        "content": "Dear friends,Thinking about the future of machine learning programming frameworks, I recently reread computer scientist Fred Brooks’ classic essay, “No Silver Bullet: Essence and Accidents of Software Engineering.” Three decades after its initial publication, it still holds important lessons for software engineers building ML tools.Despite progress from typewriters to text editors, why is writing still hard to do? Because text editors don’t address the most difficult part: thinking through what you want to say.Programming tools have the same limitation. I’m glad to be coding in Python rather than Fortran. But as Brooks points out, most advances in programming tools have not reduced the essential complexity of software engineering. This complexity lies in designing a program and specifying how it should solve a given problem, rather than in expressing that design in a programming language.Deep learning is revolutionary because it reduces the essential complexity of building, say, a computer vision system. Instead of writing esoteric, multi-step software pipelines comprising feature extractors, geometric transformations, and so on, we get data and train a neural network. Deep learning hasn’t just made it easier to express a given design; it has completely changed what we design.As we work on ML programming frameworks, we should think about how to further reduce the essential complexity of building ML systems. This involves not just specifying an NN architecture (which is indeed waaay easier to do in TensorFlow or PyTorch than C++), but also deciding what is the problem to be solved and designing all the steps from data acquisition to model training to deployment.I don’t know what will be the key ideas for reducing this essential complexity, but I suspect they will include software reuse, ML model reuse (such as libraries of pretrained models) and tools not just for code versioning and reuse (like github) but also for data versioning and reuse. Breakthroughs in unsupervised and other forms of learning could also play a huge role.Even as I occasionally struggle to get an ML system to work (it’s not easy for me either), I am excited to see how our community is pioneering this discipline.Keep learning!AndrewP.S. My best learning creation so far, seven month-old Nova, just said her first words! 🙂",
        "summary": "*   Drawing on Fred Brooks' \"No Silver Bullet,\" the author argues that while programming tools improve accidental complexity, Deep Learning is revolutionary as it reduces the essential complexity of building systems like computer vision by replacing complex pipelines with training neural networks.\n*   Deep learning changes what is designed, moving from specifying multi-step software pipelines to training a neural network on data.\n*   Future ML programming frameworks should focus on reducing the essential complexity of building ML systems across the entire process, from data acquisition to deployment.\n*   Potential ways to reduce this essential complexity include software reuse, ML model reuse (pretrained models), tools for data versioning/reuse, and breakthroughs in unsupervised/other learning forms.",
        "publish_date": "Oct 02, 2019",
        "keywords": [
            "Computer Vision",
            "Pretrained models",
            "Data versioning",
            "Unsupervised learning",
            "Multi-step software pipelines"
        ],
        "url": "https://www.deeplearning.ai/the-batch/reducing-essential-complexity/"
    },
    {
        "content": "Dear friends, I read an interesting   comparing the results of traditional passive learning (sitting in a lecture) versus active methods like the flipped classroom, where students watch videos at home and work on exercises in class. The paper is nicely summarized by this figure: The leftmost pair of bars shows that students learn more from active learning. Ironically, they feel they are learning more from passive methods, shown by the remaining bars. I’ve been using a flipped classroom for much of my teaching, with great results. Students watch lectures on Coursera, then come to the classroom to ask questions and work in small groups. This paper explains why many instructors are reluctant to switch to active learning, even though it’s more effective. The world needs much better education everywhere. I hope more educators who teach in person will embrace active learning methods. Keep learning! Andrew",
        "summary": "Based on the provided content, there is no information directly related to Artificial Intelligence or Data Science. Therefore, a summary adhering strictly to the specified domain cannot be generated.",
        "publish_date": "Sep 25, 2019",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "Machine Learning",
            "Data",
            "Analysis"
        ],
        "url": "https://www.deeplearning.ai/the-batch/passive-versus-active-learning/"
    },
    {
        "content": "Dear friends, Last week, I saw a lot of social media discussion about a paper using deep learning to generate artificial comments on news articles. I’m not sure why anyone thinks this is a good idea. At best, it adds noise to the media environment. At worst, it’s a tool for con artists and propagandists.A few years ago, an acquaintance pulled me aside at a conference to tell me he was building a similar fake comment generator. His project worried me, and I privately discussed it with a few AI colleagues, but none of us knew what to do about it. It was only this year, with the staged release of OpenAI’s GPT-2 language model, that the question went mainstream.Do we avoid publicizing AI threats to try to slow their spread, as I did after hearing about my acquaintance’s project? Keeping secret the details of biological and nuclear weapon designs has been a major force slowing their proliferation. Alternatively, should we publicize them to encourage defenses, as I’m doing in this letter?Efforts like the OECD’s Principles on AI, which state that “AI should benefit people and the planet,” give useful high-level guidance. But we need to develop guidelines to ethical behavior in practical situations, along with concrete mechanisms to encourage and empower such behavior.We should look to other disciplines for inspiration, though these ideas will have to be adapted to AI. For example, in computer security, researchers are expected to report vulnerabilities to software vendors confidentially and give them time to issue a patch. But AI actors are global, so it’s less clear how to report specific AI threats.Or consider healthcare. Doctors have a duty to care for their patients, and also enjoy legal protections so long as they are working to discharge this duty. In AI, what is the duty of an engineer, and how can we make sure engineers are empowered to act in society’s best interest?To this day, I don’t know if I did the right thing years ago, when I did not publicize the threat of AI fake commentary. If ethical use of AI is important to you, I hope you will discuss worrisome uses of AI with trusted colleagues so we can help each other find the best path forward. Together, we can think through concrete mechanisms to increase the odds that this powerful technology will reach its highest potential.Keep learning!Andrew",
        "summary": "* AI generating fake comments on news articles is a threat, potentially adding noise or serving as a tool for con artists and propagandists.\n* A dilemma exists whether to publicize AI threats (like the GPT-2 release) to encourage defenses or keep them secret to slow proliferation.\n* Practical ethical guidelines and concrete mechanisms are needed for AI behavior, going beyond high-level principles.\n* Applying ethical models from fields like computer security (reporting vulnerabilities) or healthcare (professional duty) to AI is challenging due to global actors and undefined engineer duties.",
        "publish_date": "Oct 09, 2019",
        "keywords": [
            "GPT-2",
            "AI threats",
            "AI ethics",
            "Model release",
            "Generative AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-should-we-manage-ai-threats/"
    },
    {
        "content": "Dear friends,Recently I wrote about major reasons why AI projects fail, such as small data, robustness, and change management. Given that some AI systems don't work, users and customers sometimes rightly wonder whether they should trust an AI system.How can we persuade people to trust an algorithm? Some important techniques are:Explainability. If an AI can explain its decisions, this helps to build trust or identify problems before they can impinge on trust. For instance, the New York State Department of Financial Services is investigating whether the Apple/Goldman Sachs credit card exhibits gender bias in setting credit limits. If the algorithm could explain its decisions, we could determine whether such bias was driving them.Testing. Many of us are willing to take medicinal drugs whose biochemical effects no one fully understands. We trust these drugs because they have passed randomized clinical trials and received FDA approval. Similarly, black-box AI algorithms might gain our trust by undergoing rigorous testing.Boundary conditions. Clearly specifying boundary conditions (where the AI is expected to work) also helps. For instance, machine learning engineers developing systems to read medical images may specify the allowable range of inputs (for instance, X-rays must be this bright, and with a certain resolution) and so we can test against these conditions.Gradual rollout. Rather than having AI make fully automated decisions on Day One, we can start by allowing it merely to assist humans. For example, an AI trained to read X-rays might assist radiologists in making diagnoses rather than replacing doctors outright. Over time, having collected enough data and improved image readers sufficiently, we would come to trust higher and higher levels of automation, perhaps even full automation.Auditing. Third-party audits would build trust that our algorithms have minimal or no gender, race, or other bias, and that they meet certain performance standards.Monitors and alarms. Even after deploying a system, we can make sure we receive alerts if something goes wrong. By designing mechanisms that escalate serious issues, we can ensure that problems are fixed in a timely way.Trust isn’t just about convincing others that our solution works. I use techniques like these because I find it at least as important to convince myself that a solution works, before I ask a customer to rely on it.Keep learning!Andrew",
        "summary": "*   Techniques to build trust in AI systems include Explainability (AI explaining decisions) and rigorous Testing (like clinical trials).\n*   Trust is enhanced by defining Boundary Conditions (where AI operates) and implementing Gradual Rollout (starting with human assistance).\n*   Ensuring trust requires Auditing (third-party checks for bias/performance) and deploying Monitors/Alarms (alerts for post-deployment issues).",
        "publish_date": "Dec 04, 2019",
        "keywords": [
            "Explainability",
            "Boundary Conditions",
            "Gradual Rollout",
            "Auditing",
            "Monitors/Alarms"
        ],
        "url": "https://www.deeplearning.ai/the-batch/building-trustworthy-ai/"
    },
    {
        "content": "Dear friends, Last week, I asked readers to tell me what they’re doing to address the Covid-19 pandemic. Many of you wrote to say you’re taking actions such as shopping for neighbors, making masks, and creating posters that promote Covid-safe practices (see the campaign by Luter Filho, a creative director and designer in Berlin, below).Several members of the deeplearning.ai community are rising to meet the challenges of Covid-19 by building AI and other software projects:Arturo MP, a natural language engineer in Toronto, along with friends and associates organized a Spanish-language Covid-19 news archive and Twitter feed to address the shortage of information in languages other than English.Hermes Ribeiro Sant Anna, a machine learning engineer in São Paulo, Brazil, built a web app that highlights surfaces prone to coronavirus contamination by human touch.Fernanda Wanderley, a data scientist in São Paulo, Brazil, helped develop free X-ray interpretation software (in Portuguese) to triage Covid-19 patients.Oscar Alexander Kirschstein Schafer at the Universidad Autónoma de Madrid is organizing open hackathons to come up with ways to fight the pandemic.Josh Brown-Kramer, a data scientist in Lincoln, Nebraska, is testing people for Covid-19 in small groups and testing individuals only if the group that includes them tests positive. This pooling approach theoretically improves test throughput by 50 percent, he writes, although he has not received independent verification.Federico Lucca in Trento, Italy, is working with the University of Trento on ultrasound interpretation software to recognize lung problems related to Covid-19.It’s exciting to see the deeplearning.ai community helping to keep families, neighborhoods, and towns healthy. Your efforts are an inspiration as I develop my own projects to keep the virus at bay and help everyone heal and rebuild. In the future, we will look back on these days with sadness, but also with pride that our community’s creativity and ingenuity can have a positive impact on a global scale.Stay safe and keep learning!Andrew",
        "summary": "*   A data scientist helped develop free X-ray interpretation software (in Portuguese) to triage Covid-19 patients.\n*   Work is underway on ultrasound interpretation software to recognize lung problems related to Covid-19.\n*   A data scientist is using a pooling approach for Covid-19 testing, theoretically improving test throughput by 50 percent.",
        "publish_date": "Apr 08, 2020",
        "keywords": [
            "X-ray interpretation",
            "Ultrasound interpretation",
            "Pooling approach",
            "Medical Imaging Analysis",
            "Computer Vision"
        ],
        "url": "https://www.deeplearning.ai/the-batch/deep-learning-against-covid/"
    },
    {
        "content": "Dear friends,In the earlier weeks of Covid-19, I didn’t want to contribute noise, so that experts in infectious disease could be heard. But now the situation has worsened. I spoke yesterday with Eric Topol, a cardiologist at Scripps Institute and author of Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. He convinced me that it’s urgent for all of us to speak up.I’m deeply concerned about preventing Covid-19’s spread within healthcare systems. Apart from the widely reported shortages of personal protective equipment, healthcare systems in most countries, including the U.S., are not set up to adequately protect doctors and nurses from infection. We need to prioritize healthcare workers’ safety if we want them to keep taking care of us — and so that the death toll estimates, which are already staggering, don’t become even worse.Here are some projects that my teams and I have been up to:Sourcing best practices from different countries that have more experience with Covid-19 and SARS. We organized this webinar to inform U.S. doctors of South Korea’s best practices.Together with several colleagues, compiling such best practices into concrete, practical suggestions like segmenting hospitals according to risk, with appropriate protocols for high-, medium-, and low-risk zones.Shipping masks to the U.S. from abroad and donating them to local hospitals. Our first shipment just arrived, and more are on the way.It’s urgent for all of us to do what we can to flatten the curve. There are many things you can do to help. I hope that each of us will:Practice social distancing. Stay at home if you can, and encourage others to do the same.Support wearing masks by both healthcare workers and private citizens.Make local contributions, from offering to buy groceries for a neighbor to simply voicing your appreciation for the healthcare workers who are treating people nearby.It’s up to us to respect the quarantine and save lives. Let’s come together as one global community and make it happen. Let me know what you or your friends are doing to help your community by sending email to [email protected].Stay safe and keep learning!Andrew",
        "summary": "* The author spoke with Eric Topol.\n* Eric Topol is the author of the book \"Deep Medicine\".\n* The book \"Deep Medicine\" is titled \"How Artificial Intelligence Can Make Healthcare Human Again\", indicating its focus on Artificial Intelligence in Healthcare.",
        "publish_date": "Apr 01, 2020",
        "keywords": [
            "Deep Medicine",
            "Artificial Intelligence",
            "Healthcare",
            "AI in Healthcare",
            "Medical AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/covid-19-how-to-flatten-the-curve/"
    },
    {
        "content": "Dear friends,I’ll be spending Thanksgiving with Nova and watching her taste turkey for the first time. To those of you who celebrate Thanksgiving, I hope you spend time with loved ones, reflect on what you are thankful for, and discuss some very important topics around the dinner table:Should you get a real dog or a Boston Dynamics Spot?How can we keep the kids from using GPT-2 to write school essays?What do you say to Uncle Harold who thinks Siri is sentient?In AI, all of us should be thankful to stand on the shoulders of those who came before. I’ll leave you with one thought: What can you do now so that, in the future, dozens or more will feel thankful toward you? Let’s work together to help each other, and thereby move the world forward.Keep learning!Andrew",
        "summary": "* Potential discussion topics include comparing real dogs to Boston Dynamics Spot.\n* Potential discussion topics include preventing the use of GPT-2 for writing school essays.\n* Potential discussion topics include addressing the belief that Siri is sentient.\n* In AI, there is a sentiment of being thankful for those who came before.",
        "publish_date": "Nov 27, 2019",
        "keywords": [
            "Boston Dynamics Spot",
            "GPT-2",
            "Siri",
            "sentient",
            "AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-topics-for-dinner-table-discussionbuilding-trustworthy-ai/"
    },
    {
        "content": "Dear friends, When I was younger, I was not a fan of working from home. Too many distractions! So I worked a lot in coffee shops. They turned out to be convenient places to talk to strangers and ask for feedback about products I was working on, including early MOOC prototypes.Now much of the world is undergoing a remote work experiment. My teams and I are working from home.There have been positives and negatives. I love running into colleagues in our #virtualcoffeechat slack channel, especially people I don’t see so often around the office. I love reducing my carbon footprint and not having to commute, and I love getting to see Nova during my lunch break. (She’s learning to walk, and her unstable toddling is simultaneously cute and terrifying.)On the flip side, I miss seeing everyone in 3D. I miss the serendipitous discussions, and I miss being able to gather in the break room to chat and partake in the babka, gulab jamun, chicharron, and durian candy that teammates sometimes bring to share.Even though Covid-19 is a painful challenge, there is a silver lining in this shift in how we work. People in the tech industry are fortunate that a lot of work can be done remotely, and many companies are now learning how to do this well.Once this pandemic is over, I believe that many remote roles will open up. It will be easier for an aspiring AI engineer who lives in Dallas to get a job in Silicon Valley — without having to move. A recruiter who lives in Buenos Aires will have a better chance of being hired by a company in Montreal. A front-end engineer in Sydney might work for an employer in Tokyo. No matter where you live, more jobs will be coming to you in the future.Stay safe and keep learning!Andrew",
        "summary": "* The current remote work experience includes positives like virtual chats and reduced commute, contrasted with negatives such as missing in-person interaction and serendipitous discussions.\n* The tech industry is adapting to widespread remote work, learning effective practices during the pandemic.\n* A significant increase in remote roles is anticipated post-pandemic, potentially allowing aspiring AI engineers in locations like Dallas to secure jobs in hubs like Silicon Valley without needing to move.",
        "publish_date": "Mar 25, 2020",
        "keywords": [
            "AI engineers",
            "Artificial Intelligence",
            "remote work",
            "tech industry",
            "roles"
        ],
        "url": "https://www.deeplearning.ai/the-batch/covid-and-the-remote-work-experiment/"
    },
    {
        "content": "Dear friends,The unfolding Covid-19 crisis calls for individuals and organizations to step up and contribute to the common good. I believe that the tech community has an important role to play in slowing the progress of the virus and shortening the time it takes society to recover.Tech businesses can offer free or reduced-cost services, as well as extra support, to healthcare providers. I’m seeing a lot of unfulfilled needs in healthcare systems that communication and visualization tools might address. I’m providing IT support to doctor friends. Many of us can help with this.Individuals and organizations alike can combat fake news by calling out inaccurate and ill-informed perspectives and passing along accurate, timely information. Keeping digital channels free of misinformation and open for rapid dissemination of important news is critical.It’s especially important to encourage the free flow of information among researchers, healthcare systems, and epidemiologists, including data that can feed analytics or AI systems.Help others wherever you can, especially people in greater need.In my neighborhood, I’ve been gratified to see people volunteering on a local messaging app (Nextdoor) to shop for groceries or help out the elderly. We all need to pull together and lend a hand wherever we can.And of course, I hope you will take care of yourselves and your family.Stay safe,Andrew",
        "summary": "* It is important to encourage the free flow of information, including data, among researchers, healthcare systems, and epidemiologists to feed analytics or AI systems.",
        "publish_date": "Mar 18, 2020",
        "keywords": [
            "data",
            "analytics",
            "AI systems",
            "healthcare systems",
            "epidemiologists"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-tech-community-fights-covid/"
    },
    {
        "content": "Dear friends,In this series exploring why machine learning projects fail, let’s examine the challenge of “small data.”Given 1 million labeled images, many teams can build a good classifier using open source. But say you are building a visual inspection system for a factory to detect scratches on smartphones. No smartphone manufacturer has made 1 million scratched phones (that would have to be thrown away), so a dataset of 1 million images of scratched phones does not exist. Getting good performance with 100 or even 10 images is needed for this application.Deep learning has seen tremendous adoption in consumer internet companies with a huge number of users and thus big data, but for it to break into other industries where dataset sizes are smaller, we now need better techniques for small data.In the manufacturing system described above, the absolute number of examples was small. But the problem of small data also arises when the dataset in aggregate is large, but the frequency of specific important classes is low.Say you are building an X-ray diagnosis system trained on 100,000 total images. If there are few examples of hernia in the training set, then the algorithm can obtain high training- and test-set accuracy, but still do poorly on cases of hernia.Small data (also called low data) problems are hard because most learning algorithms optimize a cost function that is an average over the training examples. As a result, the algorithm gives low aggregate weight to rare classes and under-performs on them. Giving 1,000 times higher weight to examples from very rare classes does not work, as it introduces excessive variance.We see this in self-driving cars as well. We would like to detect pedestrians reliably even when their appearance (say, holding an umbrella while pushing a stroller) has low frequency in the training set. We have huge datasets for self-driving, but getting good performance on important but rare cases continues to be challenging.How do we address small data? We are still in the early days of building small data algorithms, but some approaches include:Transfer learning, in which we learn from a related task and transfer knowledge over. This includes variations on self-supervised learning, in which the related tasks can be “made up” from cheap unlabeled data.One- or few-shot learning, in which we (meta-)learn from many related tasks with small training sets in the hope of doing well on the problem of interest. You can find an example of one-shot learning in the Deep Learning Specialization.Relying on hand-coded knowledge, for example through designing more complex ML pipelines. An AI system has two major sources of knowledge: (i) data and (ii) prior knowledge encoded by the engineering team. If we have small data, then we may need to encode more prior knowledge.Data augmentation and data synthesis.Benchmarks help drive progress, so I urge the development of small data benchmarks in multiple domains. When the training set is small, ML performance is more variable, so such benchmarks must allow researchers to average over a large number of small datasets to obtain statistically meaningful measures of progress.My teams are working on novel small data techniques, so I hope to have details to share in the future.Keep learning!AndrewRead part 1 of this series now.Read part 2 of this series now.Read part 3 of this series now.Read part 5 of this series now.",
        "summary": "*   Small data is a challenge for ML, occurring with small absolute datasets (e.g., 10-100 images for factory inspection) or rare classes within large datasets (e.g., few hernia examples in 100,000 X-ray images).\n*   Small data problems are difficult because algorithms optimizing average cost under-perform on rare classes; simple re-weighting (e.g., 1,000 times higher weight) introduces excessive variance.\n*   Approaches to address small data include transfer learning, one-/few-shot learning, incorporating hand-coded prior knowledge, and data augmentation/synthesis.\n*   Developing small data benchmarks is crucial for progress, requiring averaging performance over many small datasets due to higher variability.",
        "publish_date": "Nov 13, 2019",
        "keywords": [
            "rare classes",
            "transfer learning",
            "one-/few-shot learning",
            "hand-coded prior knowledge",
            "data augmentation/synthesis"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-ai-projects-fail-part-4-small-data/"
    },
    {
        "content": "Dear friends,My last two letters explored robustness and small data as common reasons why AI projects fail. In the final letter of this three-part series, I’d like to discuss change management.Change management isn’t an issue specific to AI, but given the technology’s disruptive nature, we must pay attention to it if we want our projects to succeed. An AI system that, say, helps doctors triage patients in an emergency room affects many stakeholders, from doctors to the intake nurses to the insurance underwriters. To keep projects on track, people must be brought onboard and systems must be adjusted.I recently saw a union block even small-scale experiments because of fear that AI would automate jobs away. This was unfortunate, because the AI system being contemplated would have made employees more valuable without reducing employment. A change management process could have made the stakeholders comfortable with experimenting and helped them understand why it was worthwhile rather than threatening.Many engineers underestimate the human side of change management. Some tips:Budget enough time. Change management requires asking lots of questions, assessing how various roles will change, and explaining to many people what the AI will do.Identify all stakeholders. Either communicate with them directly or find ways to have colleagues talk to them. Many organizations make decisions by consensus, and it is important to minimize the odds of any stakeholder blocking or slowing down implementation. We also need to build trust among stakeholders that the AI will work.Provide reassurance. Where possible, explain to people how their work may change and how the new system will benefit them.Explain what’s happening and why. There is still significant fear, uncertainty and doubt (FUD) about AI. I have seen that providing a basic education — along the lines of the AI for Everyone curriculum — eases these conversations. Other tactics including explainability, visualization, rigorous testing, and auditing also help build trust in an AI system and convince our customers (and ourselves!) that it really works.Right-size the first project. If it is not possible to start with a complex deployment that affects a lot of people, consider starting with a smaller pilot (The AI Transformation Playbook includes helpful perspective on this) that affects a smaller number of stakeholders, and is thus easier to get buy in.As we have seen with self-driving cars, building an AI system often involves solving a systems problem. That requires reorienting not only stakeholder roles and organizational structures, but also many things around the AI, like setting expectations with other drivers, pedestrians, and first responders and updating procedures around road maintenance and construction. Addressing the systems problem will increase the odds of your project succeeding.If you understand the problems of robustness, small data, and change management, and if you can spot these problems in advance and pre-empt them, you’ll be well ahead of the curve in building a successful AI project.Building AI projects is hard. Let’s keep pushing and share what we learn with each other, so we can keep moving the field forward!Keep learning!AndrewRead part 1 of this series now.Read part 2 of this series now.Read part 3 of this series now.Read part 4 of this series now.",
        "summary": "*   Change management is a critical, often underestimated, factor for AI project success, alongside robustness and small data.\n*   Managing stakeholders is essential due to AI's disruptive nature; resistance (e.g., from unions fearing job automation) can block projects.\n*   Effective change management involves budgeting time, identifying stakeholders, providing reassurance, explaining the AI's purpose (addressing FUD), and using tactics like explainability and testing.\n*   Increasing AI project success odds requires right-sizing initial projects (e.g., pilots) and addressing the broader 'systems problem' around deployment.",
        "publish_date": "Nov 20, 2019",
        "keywords": [
            "robustness",
            "small data",
            "explainability",
            "testing",
            "deployment"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-ai-projects-fail-part-5-change-management/"
    },
    {
        "content": "Dear friends,The Covid-19 pandemic is a tragedy that demands urgent and humane response. It’s also pushing us toward new ways of gathering and sharing information — and that may be a faint silver lining that might grow brighter over time.Many important conferences are being canceled. Just as the rise of online video brought a new generation of online education, I believe the rise of livestreaming and videoconferencing will bring a new generation of online conferences.For many years, attendees at top conferences have asked themselves: Why do we travel to one location, when it means:Significant costIncreased carbon emissionsLimitations on attendance due to venue sizeLimitations imposed by the host country’s visa policiesJust as MOOCs today are a lot more than video, online conferences will be much richer than livestreamed video. Perhaps we’ll have regional chat rooms where attendees in the same country can share local resources even while they listen to a keynote. Or we will generate live transcripts through automatic speech recognition that attendees can tag with live commentary. Up- and downvoting one another’s questions will be routine, and some answers will be crowdsourced.I don’t expect online conferences to replace in-person events, which still have an important role. Rather, they’ll complement them. With more team members (including many in my organizations) working from home, the time is ripe to experiment with these ideas and move toward lower costs, smaller carbon footprints, democratized access, and stronger communities. If you have thoughts, let us know at [email protected].Wash your hands, stay safe, and keep learning!Andrew",
        "summary": "* Automatic speech recognition is proposed for use in online conferences.\n* This technology can generate live transcripts for attendees.\n* Attendees may tag these live transcripts with live commentary.",
        "publish_date": "Mar 11, 2020",
        "keywords": [
            "Automatic speech recognition",
            "live transcripts",
            "online conferences",
            "live commentary",
            "speech"
        ],
        "url": "https://www.deeplearning.ai/the-batch/conferences-in-the-time-of-covid/"
    },
    {
        "content": "Dear friends, Last week brought reports that the European Union is considering a three- to five-year moratorium on face recognition in public places. Face recognition is a problematic technology with significant potential for misuse, and I celebrate the EU’s effort to protect human rights and civil society. But the blunt instrument of a long moratorium is a terrible idea.Five years is an eternity in AI, and implementing this proposal would all but guarantee that EU teams fall behind their colleagues in the U.S., China, and other nations.Contrary to popular belief, face recognition is not a solved problem. Although many teams have achieved good performance on face recognition benchmarks such as LFW, the technology still has a long way to go. Open source software makes it easy to recognize faces from a front-facing still image, but a number of hard problems remain to besolved, including multi-camera tracking, re-identification (when someone exits the frame and then re-enters), robustness to occasional camera outages, and automatic multi-camera calibration. Such capabilities will advance significantly in the next few years.Countries that have the foundation to develop this technology will pull ahead of those that don’t. It would be ironic if the EU, having slowed its own work on face recognition, were to end up having to license it from American and Chinese companies.The Universal Declaration of Human Rights remains one of the most inspirational documents I have ever read. I won’t pretend that forming good regulations is easy; it is hard because it entails hard tradeoffs. We must make sure that privacy-respecting societies don’t fall behind in technology development precisely because of those laudable values. Instead of hobbling them, we must enable them to leap ahead in a way that propagates those values.Keep learning!Andrew",
        "summary": "*   The EU is considering a 3- to 5-year moratorium on face recognition in public places.\n*   A 5-year moratorium is seen as detrimental because 5 years is \"an eternity in AI,\" risking EU teams falling behind those in the U.S., China, and other nations.\n*   Face recognition is not a solved problem; while benchmarks like LFW show good performance, hard problems remain, including multi-camera tracking, re-identification, robustness to outages, and automatic multi-camera calibration.\n*   These remaining face recognition capabilities are expected to advance significantly in the next few years, and countries developing them will gain an advantage.",
        "publish_date": "Jan 22, 2020",
        "keywords": [
            "face recognition",
            "LFW",
            "multi-camera tracking",
            "re-identification",
            "automatic multi-camera calibration"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-problem-with-the-eus-moratorium-on-face-recognition/"
    },
    {
        "content": "Dear friends, In addition to creating tremendous value, AI is creating tremendous concentrations of power. Our community is wrestling with what constitutes fair use of that power.The Markup published an article criticizing car insurance giant Allstate for price discrimination — charging different fees to different customers — based not only on their risk but also on their predicted willingness to pay. Is this behavior okay?Digital technology enables online comparison shopping, which shifts pricing power toward consumers. But it also enables companies to create unique products for individual customers — say, a ride from point A to point B at a particular time, or a health insurance plan tailored to the customer’s personal history — and AI can help optimize prices to maximize profit for vendors. That can lead to both better products and worse price transparency.If an online store sells the same hammer to different people for different prices, customers eventually will notice. That helps keep this form of price discrimination in check. But the temptation for sellers is still there. In 2016, Uber revealed that customers pay higher prices when their phone battery is low. (The company said it didn’t take advantage of this phenomenon.)I wonder sometimes if I should comparison-shop more frequently than I do. Less because I’m anxious to save a few dollars on one purchase, but because I want to train vendors’ AI systems to think I’m sensitive to price and thus to offer me lower prices.In college, my Economics 101 professor taught about supply and demand, and how our economy creates surpluses for both producers and consumers. But AI is prompting us to revisit old economic theories — along with our sense of what’s fair.These are hard questions. I hope we can work on them together to give the world great products and services at even better prices.Keep learning!Andrew",
        "summary": "*   AI is creating concentrations of power and raising questions about its fair use.\n*   AI is potentially used for price discrimination based on predicted willingness to pay (e.g., Allstate).\n*   AI helps vendors optimize prices to maximize profit, potentially reducing price transparency.\n*   AI is prompting a re-evaluation of old economic theories and concepts of fairness.",
        "publish_date": "Mar 04, 2020",
        "keywords": [
            "Predicted willingness to pay",
            "Price optimization",
            "Price discrimination",
            "AI Fairness",
            "AI Power Concentration"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-price-optimization-leads-to-better-products-and-worse-transparency/"
    },
    {
        "content": "Dear friends, A student once asked me, “Can an AI ever love?” Since the early days of AI, people have wondered whether AI can ever be conscious or feel emotions. Even though an artificial general intelligence may be centuries away, these are important questions. But I consider them philosophical questions rather than scientific questions. That’s because love, consciousness, and feeling are not observable. Whether an AI can diagnose X-ray images at 95 percent accuracy is a scientific question; whether a chatbot can convince (or “fool”) an observer into thinking that it has feelings is a scientific question. But whether it can feel is a question best left to philosophers and their  . Or to the Tin Man, the robot character in The Wizard of Oz who longs for a heart only to learn that he had one all along. Even if we can’t be sure that an AI will ever love you, I hope you love AI, and also that you have a happy Valentine’s Day! Love, Andrew",
        "summary": "* The question of whether AI can love, be conscious, or feel emotions is considered a philosophical question, not scientific, due to these states being unobservable.\n* Scientific questions in AI involve observable outcomes, such as AI diagnosing X-ray images at 95 percent accuracy or a chatbot convincing an observer it has feelings.\n* Artificial General Intelligence (AGI) is mentioned as potentially centuries away.",
        "publish_date": "Feb 12, 2020",
        "keywords": [
            "X-ray images",
            "diagnosing",
            "chatbot",
            "Artificial General Intelligence (AGI)",
            "Accuracy"
        ],
        "url": "https://www.deeplearning.ai/the-batch/can-ai-fall-in-love/"
    },
    {
        "content": "Dear friends,  I chatted recently with MIT researcher Lex Fridman on his Artificial Intelligence podcast, where we discussed our experiences teaching deep learning. It was the most fun I’ve had in an interview lately, and you can watch the video  . Lex asked me what machine learning concepts students struggle with most. While I don’t think that any particular concept is especially difficult, studying deep learning is a lot like studying math. No particular math concept — addition, subtraction, and so on — is harder than others, but it’s hard to understand division if you don’t already understand multiplication. Similarly, deep learning involves many concepts, such as LSTMs with Attention, that build on other concepts, like LSTMs, which in turn build on RNNs. If you’re taking a course on deep learning and struggling with an advanced concept like how ResNets work, you might want to review earlier concepts like how a basic ConvNet works. As deep learning matures, our community builds new ideas on top of old ones. This is great for progress, but unfortunately it also creates longer “prerequisite chains” for learning the material. Putting in extra effort to master the basics will help you when you get to more advanced topics. Keep learning! Andrew",
        "summary": "* Learning deep learning is challenging due to prerequisite chains of concepts, similar to math.\n* Advanced concepts like LSTMs with Attention and ResNets build upon earlier concepts such as LSTMs, RNNs, and basic ConvNets.\n* Mastering basic concepts is essential for understanding more advanced topics in deep learning.",
        "publish_date": "Feb 26, 2020",
        "keywords": [
            "LSTMs with Attention",
            "ResNets",
            "LSTMs",
            "RNNs",
            "ConvNets"
        ],
        "url": "https://www.deeplearning.ai/the-batch/andrew-on-lex-fridmans-podcast/"
    },
    {
        "content": "Dear friends,Many of us apply labels to ourselves that shape our identity. Some say, “I’m a sports fan,” and this attitude motivates behaviors such as cheering for the home team. Others identify themselves as introverts, extroverts, vegetarians, gamers, athletes, scientists, and/or engineers. Each label implies its own set of habits and activities.I think it’s time for more of us to identify ourselves as life-long learners. To me, a life-long learner:Aspires to keep learning new thingsSeeks knowledge or skill beyond what would be immediately usefulInvests time, energy, and money to learn new thingsShares knowledge to help other lifelong learnersThis is the best way to keep growing over your entire lifetime. I’ve seen numerous people proactively learn about new technologies or gain skills in everything from product management to personal health, and develop as individuals as a result. They seem happier, and I’m sure they contribute more to their communities.Every weekend I spend several hours reading or taking online courses. This learning helps me do my work better, but I enjoy it so much that I’d do it even if it didn’t affect my work at all.The world is changing faster than ever, driven by technological change. So humanity needs a lot more lifelong learners to make sure we keep up. I hope you’ll join me in proudly telling others, “I’m a lifelong learner!”Keep learning,Andrew",
        "summary": "The provided content is not related to Artificial Intelligence or Data Science. Therefore, I cannot summarize it according to the specified instructions which require the summary to be strictly related to these fields.",
        "publish_date": "Feb 05, 2020",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "summarize",
            "fields",
            "content"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-value-of-life-long-learning/"
    },
    {
        "content": "Dear friends, I just finished reading BJ Fogg’s new book, Tiny Habits: The Small Changes That Change Everything. Fogg explains that the best way to build a new habit is to start small and succeed, rather than starting too big and giving up. For example, rather than trying to exercise for 30 minutes a day, he recommends aspiring to do just one push-up, and doing it consistently.This approach may be helpful to those of you who want to spend more time studying. If you hold yourself accountable for watching, say, 10 seconds of an educational video every day — and you do so consistently — the habit of studying daily will grow naturally. Even if you learn nothing in that 10 seconds, you’re establishing the habit of studying a little every day. On some days, maybe you’ll end up studying for an hour.Over the years, I have found a few resources for developing personal productivity that I love. My top picks include Getting Things Done by David Allen, the classic The 7 Habits of Highly Effective People by Stephen R. Covey, and Learning How to Learn Barbara Oakley (I recommend the Coursera course). I’m tempted to add Tiny Habits to this list.Keep learning!Andrew",
        "summary": "The provided content does not contain information related to Artificial Intelligence or Data Science. Therefore, I cannot generate a summary based on the specified criteria.",
        "publish_date": "Jan 29, 2020",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "Machine Learning",
            "Algorithm",
            "Model"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-best-eay-to-build-s-new-habit/"
    },
    {
        "content": "Dear friends,Building AI systems is hard. Despite all the hype, AI engineers struggle with difficult problems every day. For the next few weeks, I’ll explore some of the major challenges. Today’s topic: The challenge of building AI systems that are robust to real-world conditions.The accuracy of supervised learning models has grown by leaps and bounds thanks to deep learning. But there’s still a huge gap between building a model in a Jupyter notebook and shipping a valuable product.Multiple research groups, including mine and several others, have published articles reporting DL’s ability to diagnose from X-ray or other medical images at a level of accuracy comparable or superior to radiologists. Why aren’t these systems widely deployed?I believe robustness is a major impediment. For example, if we collect data from a top research hospital that has well trained X-ray technicians and high-quality X-ray machines, and we train and test a state-of-the-art model on data from this hospital, then we can show comparable or superior performance to a radiologist.But if we ship this algorithm to an older hospital with less well-trained technicians or older machines that produce different-looking images, then the neural network likely will miss some medical conditions it spotted before and see others that aren’t really there. In contrast, any human radiologist could walk over to this older hospital and still diagnose well.I have seen this sort of challenge in many applications:A speech recognition system was trained primarily on adult voices. After it shipped, the demographic of users started trending younger. The prevalence of youthful voices caused performance to degrade.A manufacturing visual inspection system was trained on images collected on-site over one month. Then the factory’s lighting changed. Performance degraded in turn.After engineers shipped a web page ranking system, language patterns evolved and new celebrities rose to fame. Search terms shifted, causing performance to degrade.As a community, we are getting better at addressing robustness. Approaches include technical solutions like data augmentation and post-deployment monitoring along with setting alarms to make sure we fix issues as they arise. There are also nascent attempts to specify operating conditions under which an algorithm is safe to use, and even more nascent attempts at formal verification. Robustness to adversarial attacks is another important consideration, but most practical robustness issues that I see involve non-adversarial changes in the data distribution.One of the challenges of robustness is that it is hard to study systematically. How do we benchmark how well an algorithm trained on one distribution performs on a different distribution? Performance on brand-new data seems to involve a huge component of luck. That’s why the amount of academic work on robustness is significantly smaller than its practical importance. Better benchmarks will help drive academic research.Many teams are still addressing robustness via intuition and experience. We, as a community, have to develop more systematic solutions.Keep learning!AndrewRead part 1 of this series now.Read part 2 of this series now.Read part 4 of this series now.Read part 5 of this series now.",
        "summary": "* Building AI systems robust to real-world conditions is a major challenge despite high accuracy in controlled settings.\n* Robustness issues arise from changes in data distribution post-deployment, causing performance degradation in applications like medical imaging, speech recognition, and visual inspection.\n* Addressing robustness is difficult due to challenges in systematic study and benchmarking performance across different data distributions.\n* Current approaches include technical solutions like data augmentation and post-deployment monitoring, but more systematic methods and better benchmarks are needed.",
        "publish_date": "Nov 06, 2019",
        "keywords": [
            "Data augmentation",
            "Post-deployment monitoring",
            "Data distribution",
            "Medical imaging",
            "Speech recognition"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-ai-projects-fail-part-3-robustness/"
    },
    {
        "content": "Dear friends, Welcome to the Halloween edition of The Batch!I promised last week to share some common reasons for AI project failures. But first, let’s start with some of the least common reasons.If your AI project fails, it is probably not because:Your neural network achieved sentience. Your implementation of ResNet not only refused to classify cat pictures accurately, but worse, it set out to enslave humanity.A poltergeist inhabits in your hardware. Now you know the real reason why GPUs run so hot. Track your system’s temperature and make sure you have an exorcist in your contacts.Daemon and zombie processes are in progress. Daemons and zombies are active in your computer. Wikipedia says so, so we know it to be true. Simple solution: Wipe all hard drives and find a different line of work.A hair-raising Halloween to all of you who celebrate it, with plenty of tricks and treats.Keep learning,AndrewRead part 1 of this series now.Read part 3 of this series now.Read part 4 of this series now.Read part 5 of this series now.",
        "summary": "* The text lists uncommon reasons for AI project failures.\n* One uncommon reason mentioned is a neural network achieving sentience.\n* Another uncommon reason is a ResNet implementation failing classification or attempting enslavement.",
        "publish_date": "Oct 30, 2019",
        "keywords": [
            "ResNet",
            "classification",
            "sentience",
            "neural network",
            "AI project failures"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-ai-projects-fail-part-2-common-pitfalls/"
    },
    {
        "content": "Dear friends, One of the best gifts a friend gave me last year was recommending a book that I subsequently read and loved. She didn’t even have to buy it for me!The right information at the right time can have a powerful impact. It can alter the course of a project or even a career. No online recommender system today knows you well enough to suggest the thing that’s best for you at this moment. But you may know a friend well enough to do that.Once, a team I was leading needed more product knowledge. Rather than spend eight hours explaining product management, I spent five minutes asking everyone to read Inspired: How To Create Products Customers Love. They came back with a much better direction for the product.Is there an educational resource you’d like to recommend to a friend? (Hint, hint: Recommending The Batch makes a wonderful gift. ????)Keep learning!Andrew",
        "summary": "* The text discusses \"online recommender system\".\n* It suggests that current \"online recommender system\" do not know users well enough to provide optimal, timely suggestions.\n* Human recommendations are presented as more effective than current \"online recommender system\" due to deeper personal knowledge.",
        "publish_date": "Jan 15, 2020",
        "keywords": [
            "online recommender system",
            "Recommender system",
            "User modeling",
            "Personalization",
            "User profiling"
        ],
        "url": "https://www.deeplearning.ai/the-batch/recommended-educational-resources/"
    },
    {
        "content": "Dear friends, I traveled to Taiwan last week, where I met many CEOs interested in AI transformation of traditional companies. I also visited Taiwan AI Labs which, similar to OpenAI, started as a nonprofit AI research institute.Funded by government and private financing, Taiwan AI Labs works on smart city, healthcare, and other projects; for example, using computer vision to estimate traffic flow. Ethan Tu, the lab’s leader, tells me it focuses on practical and socially important projects, including ones that are hard to fund commercially, and openly publishes all its work. I also several professors on sabbatical there. They told me that the lab gives them more engineering resources for AI than they can generally find in a university.I’m glad to see different nations experiment with new ways to organize AI research and development. I hope more countries will fund nonprofit AI research labs.Shout out also to National Taiwan University, Taiwan Ministry of Science and Technology, and Taiwania Capital for helping organize my trip!Keep learning,Andrew",
        "summary": "*   CEOs in Taiwan are interested in AI transformation of traditional companies.\n*   Taiwan AI Labs is a nonprofit AI research institute, government and privately funded, working on projects like smart city and healthcare, using computer vision for tasks such as estimating traffic flow.\n*   Taiwan AI Labs focuses on practical, socially important projects, openly publishes its work, and provides significant AI engineering resources.\n*   The text highlights different nations experimenting with organizing AI research and development, noting the model of nonprofit AI research labs.",
        "publish_date": "Sep 04, 2019",
        "keywords": [
            "computer vision",
            "Taiwan AI Labs",
            "smart city",
            "healthcare",
            "nonprofit AI research institute"
        ],
        "url": "https://www.deeplearning.ai/the-batch/andrew-visits-taiwan-ai-labs/"
    },
    {
        "content": "Dear friends, We ended a busy week in Colombia hosting a Pie & AI meetup in Medellín. I met hundreds of engineers and business professionals excited to take the next step in their AI careers. I was energized by the enthusiasm the Colombia community had for collaborating and for supporting each other to build up the local AI ecosystem. AI is still young enough that many cities can still become hubs of AI talent, but the city has to make smart investments, and the community has to work hard and keep learning, which Colombia is doing. I hope the future will bring many more global AI hubs. I also drank a lot of coffee on this trip. I don’t know whether it was because the coffee really was fresher or if it was a placebo effect, but Colombian coffee tasted better in Colombia than when I drink it at home! Keep learning,   Andrew",
        "summary": "* A Pie & AI meetup was hosted in Medellín, Colombia, attended by engineers and business professionals interested in AI careers.\n* The Colombian community demonstrated enthusiasm for collaborating and supporting the local AI ecosystem.\n* The text suggests cities can become AI talent hubs through smart investments, community effort, and continuous learning.",
        "publish_date": "Aug 28, 2019",
        "keywords": [
            "AI meetup",
            "AI careers",
            "AI ecosystem",
            "AI talent hubs",
            "AI community"
        ],
        "url": "https://www.deeplearning.ai/the-batch/pie-ai-in-medellin/"
    },
    {
        "content": "Dear friends,  I just replaced my two-year-old phone with a new one and figured out how to take long-exposure photos of Nova even while she’s asleep and the lights are very low. This piece of technology brought me a surprising amount of joy! I wrote about ethics last week, and the difficulty of distilling ethical AI engineering into a few actionable principles. Marie Kondo, the famous expert on de-cluttering homes, teaches that if an item doesn’t spark joy, then you should throw it out. When building AI systems, should we think about whether we’re bringing joy to others? This leaves plenty of room for interpretation. I find joy in hard work, helping others, increasing humanity’s efficiency, and learning. I don’t find joy in addictive digital products. I don’t expect everyone to have the same values, but perhaps you will find this a useful heuristic for navigating the complicated decision of what to work on: Is your ML project bringing others joy? This isn’t the whole answer, but I find it a useful initial filter. Keep learning! Andrew",
        "summary": "* Discusses the difficulty of distilling ethical AI engineering into actionable principles.\n* Proposes \"bringing others joy\" as a heuristic or initial filter for deciding what ML projects to work on.\n* Mentions personal values, such as increasing humanity's efficiency and avoiding addictive digital products, in the context of building AI systems.",
        "publish_date": "Oct 16, 2019",
        "keywords": [
            "ethical AI engineering",
            "ML projects",
            "AI systems",
            "principles",
            "heuristic"
        ],
        "url": "https://www.deeplearning.ai/the-batch/does-your-project-bring-joy/"
    },
    {
        "content": "Dear friends, I’ve heard this conversation in multiple companies:Machine learning engineer: Look how well I did on the test set!Business owner: But your ML system doesn’t work. This sucks!Machine learning engineer: But look how well I did on the test set!Why do AI projects fail? Last week, I addressed this question at our  Pie & AI meetup. We had a spirited discussion with a live audience in 10 cities from London to Berlin, Ghent (Belgium) to Logroño (Spain).I remain as optimistic as ever about the AI industry, but I also see many AI projects struggle. Unlike software engineering, the process of engineering AI systems is immature, and teams have not yet learned about the most common pitfalls and how to avoid them.Common pitfalls fall under the headings: robustness, small data, and workflow. You can increase your odds of success by analyzing your AI project in terms of these issues. I’ll flesh out my thoughts on this in coming weeks. Stay tuned.Keep learning!AndrewRead part 2 of this series now.Read part 3 of this series now.Read part 4 of this series now.Read part 5 of this series now.",
        "summary": "*   AI projects often struggle despite good test set performance, highlighting a gap between testing and real-world application.\n*   The engineering process for AI systems is immature compared to software engineering, contributing to project failures.\n*   Common pitfalls in AI projects are categorized under the headings: robustness, small data, and workflow.\n*   Analyzing AI projects based on robustness, small data, and workflow issues can increase the odds of success.",
        "publish_date": "Oct 23, 2019",
        "keywords": [
            "robustness",
            "small data",
            "workflow",
            "test set performance",
            "deployment"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-ai-projects-fail/"
    },
    {
        "content": "Dear friends, Many accomplished students and newly minted AI engineers ask me: How can I advance my career? Companies in many industries are building AI teams, but it may not be obvious how to join one of them.Different companies organize their teams differently and use different terms to describe the same job. Even more confusing, job titles don’t correspond directly with common AI tasks like modeling and data engineering.What positions are responsible for which tasks? What skills are recruiters looking for? Which opportunities are right for you?Workera, a deeplearning.ai affiliate, interviewed over 100 leaders in machine learning and data science to answer these questions. They summarized their findings in a report called “AI Career Pathways: Put Yourself on the  Right Track.”“AI Career Pathways” is designed to guide aspiring AI engineers in finding jobs and building a career. The table above shows Workera’s key findings about AI roles and the tasks they perform. You’ll find more insights like this in the free PDF.I invite you to read Workera’s report and compare its findings with your own experience, talents, and skills. This will help you understand how AI teams work, what role might fit you best, and which skills you can develop to position yourself for a particular role. You can download it here.Keep learning!Andrew",
        "summary": "*   Navigating AI/Data Science careers is challenging due to varied company structures and job titles not directly corresponding to tasks like modeling and data engineering.\n*   A report \"AI Career Pathways\" by Workera (deeplearning.ai affiliate) was developed based on interviews with over 100 leaders in machine learning and data science to address these challenges.\n*   The report provides findings on AI roles, the tasks they perform, and skills recruiters seek, aiming to guide aspiring AI engineers.",
        "publish_date": "Jan 08, 2020",
        "keywords": [
            "data engineering",
            "modeling",
            "AI engineers",
            "machine learning",
            "data science"
        ],
        "url": "https://www.deeplearning.ai/the-batch/building-your-ai-career-a-report-by-workera/"
    },
    {
        "content": "Dear friends,We here at deeplearning.ai wish you a wonderful holiday season.As you consider your New Year’s resolutions and set goals for 2020, consider not just what you want to do, but what you want to learn:What courses do you want to take this year?What books do you want to read?How many papers do you want to read?What meetups or conferences do you want to attend?I find that people who write down their learning goals are more likely to accomplish them. I do so regularly myself.Making a list will help set you up for a productive new year. But for now, I hope you are able to rest, reflect with gratitude on things that happened in 2019, and spend time with loved ones.Keep learning!Andrew",
        "summary": "* deeplearning.ai encourages setting learning goals for 2020.\n* Learning goals for 2020 include taking courses, reading books and papers, and attending meetups or conferences.\n* Writing down learning goals is suggested to increase the likelihood of accomplishing them.",
        "publish_date": "Dec 24, 2019",
        "keywords": [
            "deeplearning.ai",
            "Deep Learning",
            "Research papers",
            "AI/DS conferences",
            "Online courses"
        ],
        "url": "https://www.deeplearning.ai/the-batch/setting-learning-goals/"
    },
    {
        "content": "Dear friends, I’ve been thinking about AI and ethics. With the techlash and an erosion of trust in technology as a positive force, it’s more important than ever that we make sure the AI community acts ethically.There has been a proliferation of AI ethical principles. This paper surveys 84 such statements. These statements are a great start, but we still need to do better.Take, for example, the OECD’s statement: AI should benefit people and the planet by driving inclusive growth, sustainable development, and well-being. When I ask engineers what effect such statements have on their day-to-day actions, they say, “pretty much none.” It is wonderful that the OECD is thinking about this. But we need more actionable codes of ethics that give more concrete and actionable suggestions.I described earlier struggling with an ethical decision of whether to publicize an AI threat. It’s in situations like that we need better guidelines and processes for decision making.Many existing AI ethics codes come from large corporations and governments. But if we hope that the global AI community will follow a set of guidelines, then this community — including you — needs to have a bigger voice in its development. We need an ethical code written by the AI community, for the AI community. That will also be the bestway to make sure it truly reflects our community’s values, and that all of us buy into it and will follow it.Last Friday, deeplearning.ai hosted our first Pie & AI on AI and ethics. Four cities joined us: Hong Kong, Manila, Singapore, and Tokyo. We started with an interactive discussion, and each city came up with three actionable ethics statements, preferably starting with, “An AI engineer should …” The ideas they presented ranged from seeking diverse perspectives when creating data to staying vigilant about malicious coding. I was heartened to see so many people motivated to debate ethical AI in a thoughtful way.I hope to do more events like this to encourage people to start the conversation within their own communities. This is important, and we need to figure this out.I would love to hear your suggestions. You can email us at [email protected].Keep learning!Andrew",
        "summary": "*   The text highlights a proliferation of 84 surveyed AI ethical principles, emphasizing the need for more actionable guidelines due to techlash and trust erosion.\n*   Existing AI ethics statements, such as the OECD's, are often not seen as actionable by engineers in their day-to-day work.\n*   There is a call for the AI community to develop its own ethical code to ensure relevance, buy-in, and reflection of community values.\n*   A deeplearning.ai event in 4 cities (Hong Kong, Manila, Singapore, Tokyo) generated actionable ethics statements from participants, including seeking diverse data perspectives and vigilance against malicious coding.",
        "publish_date": "Dec 11, 2019",
        "keywords": [
            "deeplearning.ai",
            "diverse data perspectives",
            "malicious coding",
            "AI ethical principles",
            "actionable ethics statements"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-ethics-must-be-actionable/"
    },
    {
        "content": "Dear friends,  I’ve been reflecting on the NeurIPS 2019 conference, which ended on Saturday. It’s always a wonderful event, but this year I found it a bittersweet experience. Bitter because the conference has grown so much that we no longer focus on a handful of ideas. I missed the feeling of a community coming together. I was excited about the progress in self-supervised learning. Others were buzzing about Bayesian networks and causality, federated learning in healthcare applications, or using DL to predict biological sequences such as proteins. These are fascinating areas, but it’s clear the AI community no longer marches to only one beat. The sweet part is that NeurIPS is growing up. As Karen Hao   in MIT Technology Review, NeurIPS has matured from a venue with great science, hard partying, and wild dancing into a forum with great science and a focus on using AI for good. The AI community is getting better at diversity, inclusion, and taking responsibility for our actions, though there’s still room to grow. As part of the panel during the climate change workshop, I spoke about the importance of building an actionable ethical code for AI. Ideally written by the AI community, for the AI community. You can hear my remarks on that subject   at 1:15. It was great fun speaking on the panel with Yoshua Bengio, Jeff Dean, Carla Gomes, and Lester Mackey. Thanks to David Rolnick, Priya Donti, Lynn Kaack, and others for organizing the great workshop. Keep learning! Andrew",
        "summary": "*   NeurIPS 2019 reflected the AI community's diversification across multiple areas rather than focusing on a few ideas.\n*   Key AI/DS topics discussed included self-supervised learning, Bayesian networks and causality, federated learning in healthcare applications, and using DL to predict biological sequences.\n*   The conference highlighted the AI community's growing maturity, focusing on \"using AI for good,\" diversity, inclusion, and taking responsibility.\n*   Discussion included the importance of building an actionable ethical code for AI, ideally written by and for the AI community.",
        "publish_date": "Dec 18, 2019",
        "keywords": [
            "self-supervised learning",
            "Bayesian networks",
            "causality",
            "federated learning",
            "predict biological sequences"
        ],
        "url": "https://www.deeplearning.ai/the-batch/neurips-grows-up/"
    },
    {
        "content": "Dear friends,I am writing to you from Colombia today, and am excited to announce the opening of our office in Medellín. The office will serve as the Latin American headquarters for three of the companies in our AI ecosystem: Landing AI, deeplearning.ai, and AI Fund.AI is still in its infancy. Although Silicon Valley and Beijing are currently leading the way in AI, with the UK and Canada also emerging as innovation hubs, there are still opportunities for every major country. Colombia is on a trajectory to become a hub of AI in Latin America.I am proud to bet on Colombia and support the growth of the Colombian AI community and the broader Latin American AI community. You can find additional details here or in Frederic Lardinois’ TechCrunch article.Keep learning!  Andrew",
        "summary": "*   An office is opening in Medellín, Colombia, serving as the Latin American headquarters for three AI companies: Landing AI, deeplearning.ai, and AI Fund.\n*   AI is described as being in its infancy.\n*   Silicon Valley and Beijing are leading in AI, with the UK and Canada emerging as innovation hubs.\n*   Colombia is on a trajectory to become an AI hub in Latin America.\n*   Support is being provided for the growth of the Colombian and broader Latin American AI community.",
        "publish_date": "Aug 21, 2019",
        "keywords": [
            "Landing AI",
            "deeplearning.ai",
            "AI Fund",
            "AI hub",
            "Deep learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/deeplearning-ai-opens-medellin-office/"
    },
    {
        "content": "Dear friends, Happy New Year! As we enter 2021, I want to share with you three wishes I have for AI in the upcoming year. I hope we can:Narrow the gap between proofs-of-concept and production. While building good models is important, many organizations now realize that much more needs to be done to put them into practical use, from data management to deployment and monitoring. In 2021, I hope we will get much better at understanding the full cycle of machine learning projects, at building MLOps tools to support this work, and at systematically building, productionizing, and maintaining AI models.Strengthen the AI community with shared values. As a community, part of our success has come from welcoming with open arms anyone who wants to join us. But over the past decade, we’ve grown from thousands to millions across the globe, and this has led to more opportunities for misunderstanding and misalignment. It is more important than ever to establish a shared set of values, so we can support each other in doing good. Let’s make sure the AI community doesn’t splinter into different factions like the political sphere in some countries. We need to put more energy into understanding each other, have vigorous — yet civil — debates, and hopefully still come together as one community.Ensure that the outcomes of our work are fair and just. The issues of bias and fairness in AI have been widely discussed. Much difficult and important work remains to be done in those areas, and we must not relent. Meanwhile, AI’s contribution to wealth inequality has received less attention. Many tech businesses are winner-take-most businesses. What is the fifth most valuable web search engine? Or the fifth most valuable social media company? As tech infiltrates every industry from agriculture to zymurgy, it’s spreading these winner-take-most dynamics. Are we creating a world where the wealth is concentrated in a small handful of companies in every industry? How can we ensure that the massive wealth we help to generate is shared fairly?I have great optimism for AI in 2021, and for the role you will play in it. I look forward to wrestling with these and other challenging problems with you.Keep learning!Andrew",
        "summary": "*   Narrow the gap between AI proofs-of-concept and production in 2021, focusing on the full machine learning project cycle, MLOps tools, and systematic production/maintenance.\n*   Strengthen the AI community with shared values to support each other, promoting understanding and civil debate despite growth.\n*   Ensure fair and just outcomes from AI work, addressing bias/fairness and the contribution to wealth inequality and winner-take-most dynamics.",
        "publish_date": "Dec 30, 2020",
        "keywords": [
            "MLOps",
            "bias/fairness",
            "production",
            "machine learning project cycle",
            "proofs-of-concept"
        ],
        "url": "https://www.deeplearning.ai/the-batch/happy-new-year-three-aspirations-for-ai-in-2021/"
    },
    {
        "content": "Dear friends, Every year for the past decade, I flew to Singapore or Hong Kong to celebrate my mother’s birthday with her on December 22. This year, for the first time, we did it via Zoom. Despite the distance, I was warmed that my family could gather from the U.S., Singapore, Honk Kong, and New Zealand and sing a poorly synchronized “Happy Birthday To You.”I wish I could also be on a Zoom call with each of you to personally wish you happy holidays and an even happier new year!Over the holidays, I often think through the list of important people in my life, recall what they’ve done for me or others, and quietly acknowledge my gratitude to them. This makes me feel more connected to them. Perhaps you’ll find it valuable to think about this, too, during the socially distanced holiday that many of us will have: Who are the important people in your life, and what reasons might you have to be grateful to them?Whether in-person or online, I hope you’ll find ways to nurture your most important relationships over this holiday season.Keep learning!Andrew",
        "summary": "The provided content does not contain information related to Artificial Intelligence or Data Science. Therefore, I cannot generate a summary based on these topics.",
        "publish_date": "Dec 23, 2020",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "summary",
            "generate",
            "information"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-concentrates-power-and-wealth-2/"
    },
    {
        "content": "Dear friends, Happy New Year! Every winter holiday, I pursue a learning goal around a new topic. In between visits with family, I end up reading a lot. About a decade ago, my holiday topic was pedagogy — I still remember lugging a heavy suitcase of books through the airport — and this helped the early days of Coursera. Last year, before Nova’s birth, I read a pile of books on child care. This holiday, I’ve been catching up on epigenetics and the emerging science (and sometimes quackery) of anti-aging. I also visited my 101-year-old grandfather. I told him what I was reading, and he said that remaining curious is the key to longevity. If he’s right, then I think many of you will thrive well past 101! Wishing you a wonderful 2020, with lots of curiosity, learning, and love. Keep learning! Andrew",
        "summary": "The provided content does not contain information related to Artificial Intelligence or Data Science. Therefore, a summary based on the specified criteria cannot be generated.",
        "publish_date": "Jan 01, 2020",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "Machine Learning",
            "Deep Learning",
            "Data"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-key-to-longevity/"
    },
    {
        "content": "Dear friends,  Like many people in the AI community, I am saddened by the sudden departure from Google of ethical AI researcher Timnit Gebru. Timnit is a tireless champion of diversity and fairness in AI. Her work, for example highlighting bias in face recognition systems, has been a productive influence on many researchers and companies. At the same time, my friend Jeff Dean built Google AI into a world-class engineering organization. I’ve seen him speak up for diversity when no one else in the room was doing so. Having not yet spoken to either of them, I hesitate to offer my opinion on the matter at this time. But the situation highlights a larger problem in the AI community: lack of a shared set of values (such as fairness, diversity, and transparency) and norms (such as what to do when there’s a problem). In academia, all scholars place high value on the pursuit and dissemination of knowledge. In medicine, all doctors recognize that the wellbeing of patients is their primary duty. We need that kind universal commitment in AI. We’re building technology that affects billions of people without a coherent set of guiding principles. Many companies and think tanks have published their own codes of ethics, and these statements are important — but they are far from sufficient. We need a set of values and norms that are shared across our entire community and transcend any one company. That way, we can collectively hold individuals, companies, and perhaps even governments accountable to them and operate for the common good even when we disagree. How can we bring the AI community together around shared values and norms? I encourage you to spend time with your teams, collaborators, and peers to discuss this difficult question. It’s past time to lay the foundation for a set of values and norms that all AI practitioners will proudly stand up for. Keep learning! Andrew",
        "summary": "* The situation highlights a lack of shared values (fairness, diversity, transparency) and norms within the AI community.\n* Ethical AI researcher Timnit Gebru's work highlighted bias in face recognition systems, exemplifying ethical challenges in AI.\n* AI technology affects billions without a coherent set of guiding principles or universal commitment across the community, unlike fields like academia or medicine.\n* Existing company/think tank ethics codes are insufficient; a set of values and norms shared across the entire AI community is needed for collective accountability and the common good.",
        "publish_date": "Dec 09, 2020",
        "keywords": [
            "face recognition systems",
            "bias",
            "fairness",
            "diversity",
            "transparency"
        ],
        "url": "https://www.deeplearning.ai/the-batch/values-for-the-ai-community-part-3-ethical-ai/"
    },
    {
        "content": "Dear friends, When it comes to artificial intelligence, one of the biggest mistakes large companies make is thinking tactically rather than strategically.What’s the difference? Some taxi companies thought they had the internet revolution “covered” because they built a website. Then ride-sharing startups disrupted the industry with internet-connected mobile apps that transformed the ride-hailing experience.Similarly, some companies’ response to AI starts and ends with tactically building a few small projects. But the strategic question is: How will AI transform your industry’s core business, and how will that change what it takes for your company to thrive?I spoke about this at TechCrunch’s conference on Thursday, and Fortune published a nice summary of my remarks. It’s not too late for traditional companies to develop a strategic plan to take advantage of AI. The technology is only beginning to find its way into applications outside of software development. But for many companies, it will be critical to act quickly.AI transformation should start with concrete projects, but it cannot end there. I hope more CEOs learn about AI and think strategically about it.Keep learning!Andrew",
        "summary": "*   Companies often approach AI tactically (small projects) instead of strategically (considering its transformation of core business and industry).\n*   A strategic plan for AI is crucial for traditional companies, as the technology is beginning to find applications outside software development.\n*   AI transformation should start with concrete projects but must extend to a broader strategic plan.",
        "publish_date": "Sep 11, 2019",
        "keywords": [
            "AI transformation",
            "AI strategic plan",
            "Tactical AI",
            "Strategic AI",
            "applications outside software development"
        ],
        "url": "https://www.deeplearning.ai/the-batch/think-strategically-not-tactically/"
    },
    {
        "content": "Dear friends, When a researcher works for a company, what rights should they have to publish their work, and what rights should the company that sponsored the work have? This issue has come up many times in the AI community across many companies, most recently around Timnit Gebru’s very public departure from Google, which involved a disagreement over research she was preparing to publish.Researchers and companies often share a desire to contribute ideas that move AI forward. At the same time, they can also have completely legitimate interests that may differ. Researchers may want to make their work available to the community, while the organizations that fund that work may want to keep certain inventions secret or patent them. Researchers and companies may be willing or unwilling, to varying degrees, to point out inconvenient truths that need to be addressed.It’s not always obvious how to balance these interests. For example:Should researchers be allowed to release any technology they wish, as long as they don’t publish confidential information?Alternatively, should companies (and universities) have the final say, including the right to stop publication of papers when it’s in their interest to do so? (This is the de facto policy in many companies today.)Should a company be responsible for ensuring the quality of research published under its name, or should this be left only to peer review? Conversely, If a researcher publishes a scientifically flawed paper, does the fault lie with the researcher, or with both the researcher and the company?What would be a reasonable prepublication review process within companies, and how can we ensure that it is applied fairly and consistently?What rights and responsibilities do researchers and companies have with respect to patent filings of inventions in which they both played a part?I’ve submitted publications for review, and I’ve set policies that govern how others’ work should be reviewed. As a co-author, I’ve also pulled publications when I felt they were not up to standard. These experiences have shown me that the answers to these questions may differ, depending on the parties involved.What is clear, though, is that researchers and companies need to set clear expectations ahead of time, and then abide by them consistently. Both parties have an interest in avoiding situations where a researcher spends substantial time and energy working on ideas with the intent to publish them, only to be surprised that they’re unable to do so.I would like to see the AI community get together and establish a fair set of rules that balance everyone’s interests. Every researcher, company, and university is different, and possibly no one-size-fits-all answer will work for everyone. But if we set expectations collectively, we might be able to nudge companies toward a balanced set of policies around publications.What rules do you think would be fair? Let me know via social media or by sharing your ideas here.Keep learning!Andrew",
        "summary": "*   A key issue in the AI community is balancing researcher publication rights with company interests (secrecy, patents, reputation), highlighted by the Timnit Gebru/Google case.\n*   Questions arise regarding who has final publication authority, company responsibility for research quality, and establishing fair internal review processes.\n*   Clear, upfront expectations between researchers and companies are essential to prevent conflicts over publication.\n*   The author proposes the AI community collectively establish fair rules to help companies develop balanced publication policies.",
        "publish_date": "Dec 16, 2020",
        "keywords": [
            "AI Research",
            "Research Publication",
            "Researcher Rights",
            "Company Policy",
            "Internal Review"
        ],
        "url": "https://www.deeplearning.ai/the-batch/rules-for-corporate-ai-researchers/"
    },
    {
        "content": "Dear friends, Over the weekend, we hosted our first Pie & AI meetup in Kuala Lumpur, Malaysia, in collaboration with the AI Malaysia group, MDEC, and ADAX. The event was part of Malaysia’s AI & Data Week 2019. Several people traveled from neighboring southeast Asian countries to attend!I’m glad to see so many AI communities growing around the world, and I’m excited to bring more exposure to them. If you’d like to partner with us for a Pie & AI event, I hope you’ll drop us a note at [email protected].Keep learning!Andrew",
        "summary": "*   A Pie & AI meetup related to AI and Data Science was held in Kuala Lumpur, Malaysia.\n*   The event was a collaboration with the AI Malaysia group, MDEC, and ADAX.\n*   It was part of Malaysia’s AI & Data Week 2019.\n*   The event highlights the growth of AI communities around the world.",
        "publish_date": "Sep 18, 2019",
        "keywords": [
            "Pie & AI meetup",
            "AI Malaysia group",
            "MDEC",
            "ADAX",
            "Malaysia’s AI & Data Week 2019"
        ],
        "url": "https://www.deeplearning.ai/the-batch/pie-ai-in-kuala-lumpur/"
    },
    {
        "content": "Dear friends,This week’s issue of The Batch is all about medical applications of AI.Amid the current pandemic, the marriage of AI and medicine is more urgent than ever. My father is a practicing doctor, and I grew up seeing firsthand how the right care can save lives and reunite families. I’ve been privileged to participate in projects that applied deep learning to diagnosing chest X-rays, assisting with mental health, and interpreting electrocardiograms.Despite significant research progress, there’s still a long way to go. Jumping into AI for medicine now is like jumping into AI for computer vision back in 2012.For those who are ready to make the leap, deeplearning.ai is proud to introduce the AI for Medicine Specialization. This new series of courses will teach you the machine learning techniques you need to build a wide range of medical applications.If you’re new to deep learning, start with the Deep Learning Specialization. But if you’ve completed the DLS, or if you have a working knowledge of deep learning and convolutional networks as well as intermediate Python skills, the AI For Medicine Specialization will unlock many opportunities to help solve important problems.The world needs more AI people working on medicine. I hope you’ll consider being one of them.Keep learning!Andrew",
        "summary": "*   AI in medical applications is urgent, especially amid the current pandemic.\n*   Deep learning has been applied to diagnosing chest X-rays, assisting with mental health, and interpreting electrocardiograms.\n*   Despite significant research progress, AI for medicine is still early stage, comparable to computer vision in 2012.\n*   deeplearning.ai offers an AI for Medicine Specialization course series to teach machine learning techniques for medical applications.\n*   Prerequisites for the AI for Medicine Specialization include completing the Deep Learning Specialization or having deep learning/convolutional network knowledge and intermediate Python skills.",
        "publish_date": "Apr 15, 2020",
        "keywords": [
            "Deep learning",
            "convolutional network",
            "Python",
            "computer vision",
            "machine learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/medical-applications-of-ai/"
    },
    {
        "content": "Dear friends, Last Wednesday, the U.S. Capitol building was overrun by insurrectionists at the moment when members of Congress were certifying the results of a national election. Reading accounts of how close the mob came to where those representatives had sheltered, I believe the legislative branch came closer to falling than many people realize. This event was unprecedented, and its consequences will be playing out for a long time.U.S. democracy has taken a lot of damage in recent years. Citizens have become polarized. Some politicians have become brazen in their disregard for facts. Voters have been suppressed. The press has been vilified and attacked. Similar things have happened in other countries, and formerly healthy democracies have fallen into populism, authoritarianism, or totalitarianism.I hope this latest challenge will inspire a renewal of democracy. Organizations that are tested — and that survive the test — end up stronger.Democracy stands on several pillars, among them:Citizens who are informed by truthful perspectives supported by a free press and scientific enquiryInstitutions that create and enforce laws to make sure that society operates according to rulesFree and fair elections in which each individual has a vote that countsThe AI community can help strengthen all three.As ambiguous information surfaces and is tossed into the grinder of social media, recommendation engines can drive polarization. How can we build recommenders that bring people together rather than driving them apart?Decisions to ban polarizing entities — including President Trump — from tech platforms have appeared to be made ad hoc. Instead, they need to be based on rules that are fair and consistently applied. If companies and regulators can develop such rules — which will not be easy — AI can play a significant role in implementing them at scale.Digital tools have been used to selectively discourage voting and to gerrymander. On the positive side, they’ve also been used to inform voters and drive turnout. We need to develop new categories of tools and muster the political will to use them o empower all voters.January 6, 2021, was a nadir for the U.S., and the path ahead will be long and hard. But I believe the country has reached a turning point. I hope the dire events of the past week will renew our appreciation of just how precious sound government is.Keep learning!Andrew",
        "summary": "* The AI community can help strengthen the pillars of democracy.\n* Recommendation engines can drive polarization, highlighting the need to build recommenders that bring people together.\n* AI can play a significant role in implementing fair and consistently applied rules for banning entities on tech platforms at scale, provided such rules are developed.\n* Digital tools have been used to selectively discourage voting and gerrymander, as well as to inform voters and drive turnout, suggesting a need for new categories of tools to empower all voters.",
        "publish_date": "Jan 13, 2021",
        "keywords": [
            "Recommendation engines",
            "Content moderation",
            "Election analytics",
            "Voter targeting",
            "Algorithmic bias"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-ai-can-strengthen-democracy/"
    },
    {
        "content": "Dear friends, In my letter last week, I alluded to the way AI tends to concentrate power and wealth. This tendency worries me, and I believe it deserves more attention.The U.S. government has been looking into these winner-take-most dynamics at a few leading technology companies from an antitrust perspective. But the issue is much bigger than that. AI will concentrate power in many industries, including ones that haven’t traditionally relied on high tech, in the hands of a few winners.For instance, Amazon has come to dominate retailing at the expense of innumerable chains and mom-and-pop stores. Uber, Lyft, and Didi are concentrating power over the taxi industry, which used to support hundreds of thriving local companies. Retailing and taxi service are not traditionally viewed as tech industries.Driven by digitization and AI, this pattern will play out in many more industries in this decade.Covid-19 has added further fuel to these dynamics. Some retailers managed the shift to e-commerce. They are collecting data and implementing AI to optimize sales, and they’re becoming more powerful. But others were nearly destroyed as the pandemic choked off foot traffic in brick-and-mortar stores. They don’t have spare dollars to invest in AI, and they’re falling farther and farther behind.Even as AI creates tremendous wealth, I worry about the growing concentrations of power and wealth, and those who will be left behind. Government will have to step up to address this situation, but significant responsibility also lies with the all of us who conceive, build, and manage this technology. I ask each of you to use your knowledge wisely, in ways that benefit society at large rather than a select few — even if that “select few” is yourself.Keep learning!Andrew",
        "summary": "*   AI tends to concentrate power and wealth, creating \"winner-take-most dynamics\" across various industries, including those not traditionally reliant on high tech.\n*   This concentration pattern, driven by digitization and AI, is illustrated by examples like retailing and taxi services.\n*   The Covid-19 pandemic exacerbated this trend, favoring companies investing in AI for optimization over those unable to do so.\n*   The text raises concern about the growing concentration of power and wealth driven by AI and the risk of individuals/companies being left behind.\n*   Responsibility for addressing this lies with government and those who develop/manage AI, urging its use for broad societal benefit.",
        "publish_date": "Jan 06, 2021",
        "keywords": [
            "AI",
            "Optimization",
            "Concentration",
            "Winner-take-most dynamics",
            "Societal impact"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-concentrates-power-and-wealth/"
    },
    {
        "content": "Dear friends,  Experience gained in building a model to solve one problem doesn’t always transfer to building models for other problems. How can you tell whether or not intuitions honed in one project are likely to generalize to another? I’ve found that two factors can make the difference: the size of the training set and whether the data is unstructured or structured.For instance, I’ve heard blanket statements like, “you should always have at least 1,000 examples before tackling a problem.” This is good advice if you’re working on a pedestrian detector, where data is readily available and prior art shows that large datasets are important. But it’s bad advice if you’re building a model to diagnose rare medical conditions, where waiting for 1,000 examples might mean you’ll never get started.Unstructured data includes text, images, and audio clips, which lend themselves to interpretation by humans. Structured data, on the other hand, includes things like transaction records or clickstream logs, which humans don’t process easily.This difference leads to very different strategies for training and deploying models:Unstructured data: Because the examples are easy for humans to understand, you can recruit people to label them and benchmark trained models against human-level performance (HLP). If you need more examples, you might be able to collect them by capturing more text/images/audio or by using data augmentation to distort existing examples. Error analysis can take advantage of human intuition.Structured data: This class of data is harder for humans to interpret, and thus harder for humans to label. Algorithms that learn from structured data often surpass HLP, making that measure a poor benchmark. It can also be hard to find additional examples. For instance, if the training dataset comprises records of your customers’ purchases, it’s hard to get data from additional customers beyond your current user base.Dataset size has implications as well:Small dataset: If the dataset includes <1,000 examples, you can examine every example manually, check if the labels are correct, and even add labels yourself. You’re likely to have only a handful of labelers, so it’s easy to hash out any disagreements together on a call. Every single example is a significant fraction of the dataset, so it’s worthwhile to fix every incorrect label.Large dataset: If the dataset is >100,000 examples, it’s impractical for a single engineer to examine every one manually. The number of labelers involved is likely to be large, so it’s critical to define standards clearly, and it may be worthwhile to automate labeling. If a significant number of examples are mislabeled, it may be hard to fix them, and you may have to feed the noisy data to your algorithm and hope it can learn a robust model despite the noise.If you find yourself in need of advice while working on, say, a manufacturing visual inspection problem with 100 examples, the best person to ask would be someone who has worked on a manufacturing visual inspection problem with 100 examples. But if you can’t find such a person, consider looking for someone with expertise in the same dataset size/type quadrant as the problem you’re working on.As you develop your career, you might also consider whether you want to stay in one quadrant and develop deep expertise there, or move across quadrants and develop more general skills.Keep learning!Andrew",
        "summary": "*   Generalization of AI/ML intuition across problems depends on dataset size and whether data is unstructured (text, images, audio) or structured (transaction records, logs).\n*   Unstructured data is human-interpretable, enabling HLP benchmarking and data augmentation; structured data is harder for humans, with algorithms often surpassing HLP.\n*   Small datasets (<1,000 examples) allow manual examination and label correction; large datasets (>100,000 examples) necessitate automation and may involve noisy data.\n*   Seeking expertise is most effective from those experienced in the same dataset size/type quadrant (e.g., manufacturing visual inspection with 100 examples).",
        "publish_date": "Jan 20, 2021",
        "keywords": [
            "unstructured data",
            "structured data",
            "small datasets",
            "large datasets",
            "HLP (Human-Level Performance)"
        ],
        "url": "https://www.deeplearning.ai/the-batch/structured-and-unstructured-data-implications-for-ai-development/"
    },
    {
        "content": "Dear friends,Inflection points in society create opportunities. The rise of online video was an inflection point that enabled scalable online education. The rise of the GPS-enabled smartphones similarly enabled Uber, Lyft, Airbnb, and many other services. Today, the rise of deep learning is transforming many industries.Covid-19 is both a tragedy and an inflection point.Working from home seems to be here to stay. Several of my California-based teams no longer hire exclusively in the state, but anywhere within three hours of our time zone. As more companies do this, it will open up job opportunities while intensifying the need for remote collaboration tools.Many parts of society seem to be accepting some Covid tracking tools to improve safety, even if they modestly sacrifice privacy.Industries such as air travel, tourism, and commercial real estate are being decimated and will have to adapt as demand remains suppressed for the foreseeable future.Many schools have scrambled to go online. As learners worldwide get used to studying remotely, many won’t want to go back to the old way.Untold numbers of workers are unemployed. When we eventually bring unemployment down again, the distribution of jobs will be very different than it is today.We have powerful AI tools at our disposal, and we can use them to meet this inflection point. Our community can build better collaboration tools, find ways to retrain displaced workers, implement Covid tracking systems that protect civil liberties even as they promote public health, bring decimated brick-and-mortar businesses online, and invent new jobs that can be done from home. The work we do today will lay the foundation for the tomorrow we live in.How can we navigate these tumultuous changes and help the most vulnerable? My teams will be trying to do our part, and I hope you will too.Keep learning!Andrew",
        "summary": "*   The rise of deep learning is transforming many industries.\n*   Powerful AI tools are available and can be used to meet the current societal inflection point.\n*   AI tools can potentially be applied to build better collaboration tools, retrain displaced workers, implement Covid tracking systems, bring businesses online, and invent new jobs.",
        "publish_date": "May 13, 2020",
        "keywords": [
            "deep learning",
            "Artificial Intelligence",
            "Collaboration tools",
            "Covid tracking systems",
            "Workforce retraining"
        ],
        "url": "https://www.deeplearning.ai/the-batch/covid-19-transforms-industries/"
    },
    {
        "content": "Dear friends, There has been a lot of excitement about the idea of using deep learning to diagnose diabetic retinopathy: That is by taking a photo of the retina and using AI to detect signs of disease. I was fascinated by a new paper by Emma Beede and others that studied the use of Google’s diabetic retinopathy detecting system in 11 clinics in Thailand and found that, despite all the research progress, this technology isn’t working well in production yet.We as a community need to get much better at bridging proofs of concept (PoCs) and production deployments. Even Google’s excellent AI team ran into many practical issues:Images collected in the sometimes poorly-equipped clinics were not of the same uniform high quality as those in the training and test sets used in the original research. For example, in some clinics, pictures were taken with the ceiling lights on, resulting in lower image quality.There were unexpected corner cases. For example, the paper describes how a nurse who was unable to take a single, clean image of the retina instead took two partial images and wanted to diagnose from a composite of them.Many details relating to the patients’ and clinicians’ workflow still needed to be sorted out. Some patients were dissuaded from using the system because of concerns that its results might require them to immediately go to a hospital — a time consuming and expensive proposition for many. More generally, decisions about when/whether/who/how to refer a patient based on the AI output are consequential and need to be sorted out.The paper was published by SIGCHI, a leading conference in human-computer interaction (HCI). I’m encouraged to see the HCI community embrace AI and help us with these problems, and I applaud the research team for publishing these insights. I exchanged some emails with the authors, and believe there’s a promising path for AI diagnosis of diabetic retinopathy. To get there, we’ll need to address challenges in robustness, small data, and change management.Many teams are working to meet these challenges, but no one has perfect answers right now. As AI matures, I hope we can turn building production systems into a systematic engineering discipline, so we can deploy working AI systems as reliably as we can deploy a website today.Keep learning!Andrew",
        "summary": "*   A study of Google’s AI system for diabetic retinopathy diagnosis in 11 clinics in Thailand found it is not working well in production yet, despite research progress.\n*   Production challenges included non-uniform image quality in clinics compared to training data, unexpected corner cases (e.g., composite images), and issues integrating into patient/clinician workflows.\n*   Bridging AI proofs of concept and production deployment requires addressing challenges in robustness, small data, and change management.",
        "publish_date": "May 06, 2020",
        "keywords": [
            "Diabetic retinopathy diagnosis",
            "Production deployment",
            "robustness",
            "non-uniform image quality",
            "corner cases"
        ],
        "url": "https://www.deeplearning.ai/the-batch/from-proof-of-concept-to-production/"
    },
    {
        "content": "Dear friends,In an earlier letter, I wrote about the challenge of robustness: A learning algorithm that performs well on test data often doesn’t work well in a practical production environment because the real world turns out to be different than the test set.Amid the Covid-19 pandemic, many machine learning teams have seen this firsthand:Financial anti-fraud systems broke because consumers changed their behavior. For example, credit card companies often flag a card as possibly stolen if the purchase pattern associated with it suddenly changes. But this rule of thumb doesn’t work well when huge swaths of society start working from home and stop going to restaurants and malls.Logistics models used to predict supply and demand broke when manufacturers, shippers, and consumers changed their behavior. Trained on last year’s data, a model that predicts 1,000 widgets arriving on time next month can’t be trusted anymore.Online services receiving a new surge or plunge in users are rethinking their demand estimation models, since earlier models no longer are accurate.Although the tsunami of Covid-19 — with its devastating impact on lives and livelihoods — is a dramatic example of change in the world, small parts of the world experience waves of change all the time. A new online competitor may mean that a retail store’s demand estimation model no longer works. A new tariff by a small country subtly shifts supply chain behavior among larger ones.Building practical machine learning systems almost always requires going beyond achieving high performance on a static test set (which, unfortunately, is what we are very good at). You may need to build an alert system to flag changes, use human-in-the-loop deployments to acquire new labels, assemble a robust MLOps team, and so on.Technological improvements will make our algorithms more robust to the world’s ongoing changes. For the foreseeable future, though, I expect deploying ML systems — and bridging proof of concept and production deployments — to be rewarding but also hard.I hope all of you continue to stay safe.Keep learning!Andrew",
        "summary": "*   A key challenge in ML is robustness: models performing well on test data often fail in practical production environments due to real-world changes.\n*   The Covid-19 pandemic highlighted this issue, causing financial anti-fraud, logistics, and demand estimation models to break as behaviors changed.\n*   Building practical ML systems requires going beyond high performance on static test sets, necessitating strategies like alert systems, human-in-the-loop deployments, and robust MLOps teams.\n*   Deploying ML systems from proof of concept to production is expected to remain challenging due to the world's ongoing changes.",
        "publish_date": "Apr 29, 2020",
        "keywords": [
            "robustness",
            "MLOps",
            "human-in-the-loop deployments",
            "production environments",
            "ML deployment"
        ],
        "url": "https://www.deeplearning.ai/the-batch/tips-for-building-practical-machine-learning-systems/"
    },
    {
        "content": "Dear friends,Over the last several decades, driven by a multitude of benchmarks, supervised learning algorithms have become really good at achieving high accuracy on test datasets. As valuable as this is, unfortunately maximizing average test set accuracy isn’t always enough. I’ve heard too many conversations like this:Machine learning engineer: It did well on the test set!Product manager: But it doesn’t work for my application.Machine learning engineer: But . . . It did well on the test set!What else is there?Robustness and generalization: In a production deployment, performance can degrade due to concept drift (where the function mapping from x->y changes; say, the model predicts housing prices y and inflation causes prices to rise) and data drift (where the input distribution changes). One important subset of data drift relates to performance on classes that are rare in or absent from the training set. For example, a speech recognition system may achieve high average accuracy despite poor performance on speakers with a British accent, because the training and test sets included few examples of British speakers. If the product takes off in the U.K. and a lot more British speakers jump in, its accuracy will plummet. A more robust system would fare better.Performance on relatively important examples: Some examples are more important than others, and even if average test set accuracy is high, a system that performs poorly on important examples may be unacceptable. For example, users might forgive a search engine that doesn’t always return the best results to informational and transactional queries like “apple pie recipe” or “wireless data plan.” But when they enter a navigational query such as “stanford,” “youtube,” or “reddit,” they have a specific website in mind, and the search engine had better return the right URL or risk losing the user’s trust. In theory, weighting test examples according to their importance can address this issue, but it doesn’t always work in practice. Performance on key slices of data: Say a machine learning system predicts whether a prospective borrower will repay a loan, so as to decide whether to approve applications. Even if average accuracy is high, if the system is disproportionately inaccurate on applications by a specific minority group, we would be foolhardy to blindly deploy it. While the need to avoid bias toward particular groups of people is widely discussed, this issue applies in contexts beyond fairness to individuals. For example, if an ecommerce site recommends products, we wouldn’t want it to recommend products from large sellers exclusively and never products from small sellers. In this example, poor performance on important slices of the data — such as one ethnicity or one class of seller — can make a system unacceptable despite high average accuracy. My advice: If a product manager tells us that our AI system doesn’t work in their application, let’s recognize that our job isn’t only to achieve high average test accuracy — our job is to solve the problem at hand. To achieve this, we may need visualizations, larger datasets, more robust algorithms, performance audits, deployment processes like human-in-the-loop, and other tools.Keep learning!Andrew",
        "summary": "*   While supervised learning algorithms achieve high average test set accuracy, this is often insufficient for production applications.\n*   Performance can degrade due to concept drift (where the function mapping from x->y changes) and data drift (where the input distribution changes), including poor performance on classes rare in or absent from the training set.\n*   Poor performance on relatively important examples (e.g., navigational search queries) or key slices of data (e.g., specific minority groups, small sellers) can make a system unacceptable despite high average accuracy.\n*   Solving the problem requires more than just high average test accuracy, potentially involving robust algorithms, performance audits, or deployment processes like human-in-the-loop.",
        "publish_date": "Feb 10, 2021",
        "keywords": [
            "supervised learning algorithms",
            "concept drift",
            "data drift",
            "performance audits",
            "human-in-the-loop"
        ],
        "url": "https://www.deeplearning.ai/the-batch/high-test-set-accuracy-is-not-enough/"
    },
    {
        "content": "Dear friends,When a lot of data is available, machine learning is great at automating decisions. But when data is scarce, consider using the data to augment human insight, so people can make better decisions.Let me illustrate this point with A/B testing. The common understanding of the process is:Build two versions of your product. For example, on the DeepLearning.AI website, one version might say, “Build your career with DeepLearning.AI,” and another, “Grow your skills with DeepLearning.AI.”Show both versions to groups of users chosen at random and collect data on their behavior.Launch the version that results in better engagement (or another relevant metric).But this is not how I typically use A/B testing. Often I run such tests to gain insight, not to choose which product to launch. Here‘s how it works:Build two versions of your product.Have the product team make predictions about which version will perform better.Test both versions and collect data on user behavior.Show the results to the team, and let them influence their beliefs about users and their reactions. If someone says, “Oh, that’s weird. I didn’t realize our users wanted that!” then we’ve learned something valuable.Based on the team’s revised intuitions, have them decide what to launch. It could be version A, version B, or something else.Repeat until you reach diminishing returns in terms of learning.On major websites, where the developers may run thousands of automated experiments a day — for example, trying out different ad placements to see who clicks on what — it’s not possible for people to look at every experimental result to hone their intuition. In this case, fully or mostly automated decision making works well. An algorithm can try multiple versions and pick the one that achieves the best metrics (or use the data to learn what to show a given user). But when the number of daily experiments is small, using such experiments to hone your intuition allows you to combine limited trials with human insight to arrive at a better decision.Beyond A/B testing, the same concept applies to building machine learning systems. If your dataset size is modest, combining data-derived insights with human insights is critical. For example, you might do careful error analysis to derive insights and then design a system architecture that captures how you would carry out the task. If you have a massive amount of data, more automation — perhaps a large end-to-end learning algorithm — can work. But even then, error analysis and human insight still play important roles.Keep learning!Andrew",
        "summary": "*   Machine learning excels at automating decisions with abundant data, but with scarce data, it should augment human insight to improve decision-making.\n*   A/B testing can be used for automated decision-making (launching the best version) or, particularly with a small number of trials, for gaining human insight to inform decisions.\n*   For building ML systems, combining data insights (like error analysis) with human insight is critical for modest datasets, while massive datasets allow more automation (e.g., end-to-end learning), though human insight remains important.",
        "publish_date": "Feb 17, 2021",
        "keywords": [
            "A/B testing",
            "Error analysis",
            "End-to-end learning",
            "Automated decision-making",
            "Machine learning"
        ],
        "url": "https://www.deeplearning.ai/the-batch/a-different-approach-to-a-b-testing-2/"
    },
    {
        "content": "Dear friends, I was dismayed on Monday to read that the U.S. is suspending the H1-B visa program at least through the end of the year. This effort to discourage immigration can only bring distress to workers from other countries and harm to the U.S.H1-B visas allow U.S. companies to bring in talent from around the world, enriching both their business and the economy. People from many different countries have been central to U.S. innovation in AI (see “Mapping AI’s Talent Pipeline” below).To me, H1-B holders aren’t just “workers.” They are my friends, students, and collaborators, and it pains me to see them facing the stress and uncertainty that comes with sudden, arbitrary shifts in immigration policy.Stanford University sponsored my H1-B visa many years ago, which enabled me to teach and do research there. It feels deeply unfair to deny the same opportunities to the next generation. We should do whatever we can to attract top talent, not turn it away. As a planet, we should be working to empower individuals to do their best work, wherever they may end up doing it.Through education, I remain committed to creating opportunities to learn and grow for as many people as I can. I hope the AI community will continue to transcend national borders and come together to build AI for the betterment of all.Keep learning!Andrew",
        "summary": "* People from many different countries have been central to U.S. innovation in AI.\n* The author's H1-B visa enabled teaching and research at Stanford University.\n* Hope expressed for the AI community to transcend national borders and come together.",
        "publish_date": "Jun 24, 2020",
        "keywords": [
            "AI",
            "Stanford University",
            "H1-B visa",
            "Research",
            "AI community"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-we-need-h1-b-visas/"
    },
    {
        "content": "Dear friends, Last week, I talked about how best practices for machine learning projects are not one-size-fits-all, and how they vary depending on whether a project uses structured or unstructured data, and whether the dataset is small or big. Another dimension that affects best practices is which phase of development a project is in: proof of concept or production.During the proof of concept (POC) phase, the primary goal is to determine if a system is worth building and deploying. During this phase, you might ask:For a visual inspection system, can we build a model that matches the performance of human inspectors?For face detection, can we build an edge (on-device) implementation that’s nearly as accurate as the cloud version while avoiding an unacceptable level of bias?For a sales-lead scoring application, how much will estimated revenue increase by using machine learning to prioritize leads?When building a POC, my goal is to move fast. We’ve all been told we should build replicable, robust, and scalable systems — but when I haven’t even determined if a project is technically feasible, I often trade replicability for speed. I hope I don’t get too much hate mail for this, but if it buys you speed, it is okay to hard-code parameters, compute key variables in a Jupyter notebook, use local copies of data, and operate with lightweight code review or versioning processes.If you already have a platform for experimentation, you may be able to build POCs in a systematic and robust way without sacrificing speed. But if you don’t, avoid over-investing in infrastructure at this stage. Instead, focus on getting the key information you need: whether this project is worth taking to production.(Those of you who are familiar with the lean startup philosophy will see the parallel to building a minimum viable product, which is often a clunky piece of software that helps validate or falsify a hypothesis.)In contrast, during the production phase, the goal is to build and deploy a system that generates practical value. I might go back to the messy POC and make sure that every step is replicable and documented. I put a lot of thought into scalable data pipelines, monitoring systems, and reliability.For example, if a researcher wrote preprocessing routines (say, a sequence of scripts and regexps to remove data associated with spam accounts), these now need to be documented, tested, and incorporated into the system. You’ll likely want to document everything to make sure models can be replicated and maintained: hyperparameters, model choices, data provenance (where the data came from), data lineage (how it was processed). During this phase, tools like TensorFlow Transform and Apache Beam can be lifesavers.If you’re building a project, don’t confuse the POC and production phases! Both are important, but the best practices depend on whether you’re deciding as quickly as possible if a project is worth putting into production or building a system that delivers real results to real users.Keep learning!Andrew",
        "summary": "*   ML project best practices vary significantly depending on whether the project is in the Proof of Concept (POC) or Production phase.\n*   The POC phase prioritizes speed to determine technical feasibility, allowing trade-offs like hard-coding parameters, using Jupyter notebooks, and lightweight processes over replicability.\n*   The Production phase focuses on building a valuable, deployable system, requiring emphasis on replicability, documentation (hyperparameters, data provenance/lineage), scalability, monitoring, and reliability, potentially using tools like TensorFlow Transform and Apache Beam.",
        "publish_date": "Jan 27, 2021",
        "keywords": [
            "TensorFlow Transform",
            "Apache Beam",
            "data provenance/lineage",
            "hyperparameters",
            "Jupyter notebooks"
        ],
        "url": "https://www.deeplearning.ai/the-batch/dont-confuse-proof-of-concept-with-production-deployment/"
    },
    {
        "content": "Dear friends, The price of shares in video game retailer GameStop (NYSE: GME) gyrated wildly last week. Many people viewed the stock’s rapid ascent as a David-versus-Goliath story: Tech-savvy individual retail investors coordinated their trades online to push up the price and “stick it to” large hedge funds that had shorted the stock. Sadly, the reality is very different.Some retail investors in GameStop made money last week. But automated trading driven by AI now surpasses the speed and knowledge of most retail investors. I believe that wild swings in share price like the one driven by the GameStop crowd actually tend to result in a net transfer of wealth from retail investors to the hedge funds with the best AI teams.Hedge funds that use AI to trade stocks make decisions based on a multitude of features including financial indices, social media chatter, and other forms of public or licensed data. Compared to a retail investor who reads r/wallstreetbets, they have access to far more information. They also have natural language processing and financial prediction tools to process all that information. Because of this, a typical human trader today can no more outperform an AI trader than beat a good reinforcement learning algorithm at an Atari game.I differentiate between trading and investing. Human investors who choose stocks because they believe the underlying company is fundamentally valuable, and hold those stocks to realize that value, can do very well. Allocating capital to deserving companies can also help them grow, and thus make everyone involved better off. That’s different from trading, in which the aim is to buy shares solely to sell them to someone else at a higher price. Ultimately, trading creates little, if any, net wealth. When there are so many opportunities to grow the pie, why would we work so hard on activities that keep the pie the same size but squeeze out a bigger piece for ourselves at others’ expense?In The Washington Post, Helaine Olen wrote about how the volatility in GameStop’s stock price wasn’t just the story of a get-rich-quick scheme. It was also a tale of inequality, as young people who can’t find a good job dream of gaming the system. I’m glad that some traders will use their GameStop winnings to improve their lives. But I am fearful for those who will lose their savings playing a game they’re unlikely to win. For example, those who bought at GameStop’s January 27 peak and might end up incurring substantial losses they can ill afford.When you decide what AI projects to work on, I hope you will pick something that enriches not only yourself but society as a whole. Let’s also do what we can to make sure that whatever wealth we create is fairly and widely shared.Keep learning!Andrew",
        "summary": "*   Automated trading driven by AI surpasses the speed and knowledge of most retail investors in stock trading.\n*   Hedge funds use AI for trading, processing extensive data (financial indices, social media, etc.) with natural language processing and financial prediction tools.\n*   AI traders are presented as superior to human traders, comparable to reinforcement learning algorithms outperforming humans in games.\n*   The author advocates for choosing AI projects that enrich society broadly.",
        "publish_date": "Feb 03, 2021",
        "keywords": [
            "Automated trading",
            "Natural Language Processing",
            "Reinforcement Learning",
            "Financial prediction",
            "Stock trading"
        ],
        "url": "https://www.deeplearning.ai/the-batch/trading-on-ai-the-gamestop-phenomenon/"
    },
    {
        "content": "Dear friends,  Last week, I wrote about the diversity problem in AI and why we need to fix it. I asked you to tell us about your experiences as a Black person in AI or share the names of Black colleagues you admire. Thank you to everyone who responded. It was heart-warming to hear from so many of you. Many of you shared your frustration with the lack of mentors who understand your challenges, the alienation of being the only Black face at professional meetings, and the struggle to overcome economic and social inequalities. Black women, especially, wrote about the difficulties of building a career in AI. Some of you described your efforts to support Black people in science and technology and provide tech resources to underserved communities. Thank you for sharing with us your dreams and also your disappointments. We will feature some of your stories in our Working AI blog series. Please stay tuned. One thing I love about the AI community is that many of us set the highest ideals for ourselves and our community — things like fairness, equity, and justice. Sometimes these ideals are so high, we may never fully live up to them, but we keep aspiring and keep trying. These days, I know it feels like society is falling far shorter of these ideals than we would like, but that’s why it’s more important than ever that we keep aspiring and keep trying. It will be a long road to vanquish racism, but working together, I believe we will get there. Keep learning! Andrew",
        "summary": "* The text addresses the diversity problem in AI, focusing on the experiences of Black individuals in the field.\n* Challenges highlighted include lack of mentors, alienation, economic/social inequalities, and specific difficulties for Black women in building AI careers.\n* Efforts are noted to support Black people in science/technology and provide tech resources to underserved communities.\n* The AI community is described as aspiring to ideals of fairness, equity, and justice.",
        "publish_date": "Jun 10, 2020",
        "keywords": [
            "Black individuals in AI",
            "Diversity in AI",
            "Challenges in AI careers",
            "Support for Black technologists",
            "Fairness in AI"
        ],
        "url": "https://www.deeplearning.ai/the-batch/toward-diversity-in-ai/"
    },
    {
        "content": "Dear friends,I’m thrilled to announce our new Natural Language Processing Specialization! Courses 1 and 2 are available on Coursera. We expect to release Courses 3 and 4 soon.NLP is reshaping daily life. No doubt you’ve found valuable information using web search and the search functions found on countless websites and apps. Anti-spam systems are a critical part of the global email system. How does a smart speaker understand your commands? How does a chatbot generate relevant responses? This specialization will give you the foundation you need to understand such systems and the knowledge to build them yourself.You will implement a sentiment analysis system, build models that translate human languages, and even construct a chatbot. You will master the most important NLP architectures including transformer networks, and you will receive practical, hands-on training to implement techniques like tokenizing text (turning words into features suitable for training neural networks or other machine learning algorithms).The courses are taught by two wonderful instructors: Younes Bensouda Mourri, with whom I’ve had the pleasure of working for many years at Stanford, and Łukasz Kaiser, a member of the Google Brain team whom you might recognize as a co-author of TensorFlow.I invite you to dive into the NLP Specialization and use the skills you gain to do amazing things.Keep learning!Andrew",
        "summary": "*   A new Natural Language Processing Specialization is announced, with Courses 1 and 2 available and Courses 3 and 4 expected soon.\n*   The specialization provides the foundation to understand and build NLP systems, covering implementation of sentiment analysis, building language translation models, and constructing a chatbot.\n*   Participants will master important NLP architectures, including transformer networks, and implement techniques like tokenizing text for neural networks or other machine learning algorithms.\n*   Courses are taught by instructors from Stanford and Google Brain (TensorFlow co-author).",
        "publish_date": "Jun 17, 2020",
        "keywords": [
            "Natural Language Processing (NLP)",
            "transformer networks",
            "TensorFlow",
            "sentiment analysis",
            "language translation models"
        ],
        "url": "https://www.deeplearning.ai/the-batch/introducing-the-natural-language-processing-specialization/"
    },
    {
        "content": "Dear friends, Like many of you, I’m deeply saddened by the events of the past week. I’m horrified by the senseless violence perpetrated against Black communities and appalled by the persistent racial injustice of our society. It’s long past time to right these terrible wrongs.The tragic deaths of George Floyd, Ahmaud Arbery, Breonna Taylor, Sean Reed, and innumerable others remind us that life is precious, and that we have much more work to do to build an inclusive society. Minority voices are often marginalized, and that creates a responsibility for the rest of us to keep our ears and minds open, and add our voices to theirs when the occasion calls.The AI community itself has a diversity problem. The number of Black people in the field is vanishingly small. A narrow perspective can lead to severely flawed work if we overlook factors like skin color when we collect and annotate datasets or validate results. Without diverse teams, instead of building AI systems that help a cross section of people, we open doors for some while locking out others.Lack of diversity in the AI community has another effect: It reinforces the belief, often unconscious, that certain people can’t make important contributions to the field. We need to fight this sort of bias as well.If you are Black and working in AI, we would like to know about your experiences in the field. If you have Black colleagues whom you admire, please let us know about them as well. We hope to share some of your stories. Please write to us at [email protected].Maybe I’m naive, but the protests this time do feel different, and I’m cautiously optimistic that this may be the time when we finally make a huge dent in racism. As members of the AI community, let us join this movement, condemn racism everywhere we see it, and settle for nothing less than a fair and inclusive world.Keep learning!Andrew",
        "summary": "* The AI community has a diversity problem, with a vanishingly small number of Black people in the field.\n* A narrow perspective from lack of diversity can lead to severely flawed AI work by overlooking factors like skin color in data collection, annotation, or validation.\n* Without diverse teams, AI systems may open doors for some while locking out others, instead of helping a cross section of people.\n* Lack of diversity in the AI community reinforces the belief (often unconscious) that certain people can’t make important contributions to the field.",
        "publish_date": "Jun 03, 2020",
        "keywords": [
            "data collection",
            "annotation",
            "validation",
            "AI systems",
            "data"
        ],
        "url": "https://www.deeplearning.ai/the-batch/why-diversity-is-vital-to-ai/"
    },
    {
        "content": "Dear friends,  I’m proud to announce that we held the 100th Pie & AI last Friday. Pie & AI is our meetup series that brings together members of the AI community worldwide for education, conversation, and a slice of pie. Pie & AI kicked off in Seattle last year shortly after Pi Day (March 14, or 3.14). Since then, we’ve hosted events in over 68 cities in 38 countries. Friday’s event was streamed from Azerbaijan. With social distancing keeping us apart physically, it’s more important than ever for AI to have a strong online community. So we’ve doubled down on making Pie & AI a virtual meetup. No matter where you are, you can attend any of our events, learn from experts, and chat with peers even if they’re thousands of miles away. I would like to say a special thank you to Pie & AI’s 60 event ambassadors. These extraordinary people organize events locally, share resources and tips, and sometimes speak about how AI applies to local businesses and problems. I am grateful and inspired by your dedication to sharing your knowledge and enthusiasm. If Pie & AI has answered your questions, helped you grow, or inspired you, please let us know on Twitter using #PieandAI. You can check out upcoming events  . Keep learning! Andrew",
        "summary": "*   Pie & AI is a meetup series for the AI community focused on education and conversation.\n*   The 100th Pie & AI event was held, with events hosted in over 68 cities in 38 countries.\n*   Pie & AI is now a virtual meetup to support the AI online community, allowing remote attendance, learning from experts, and peer chat.\n*   60 event ambassadors organize local events, share resources, and speak on AI application to local businesses/problems.",
        "publish_date": "May 27, 2020",
        "keywords": [
            "AI",
            "AI application",
            "AI community",
            "education",
            "virtual meetup"
        ],
        "url": "https://www.deeplearning.ai/the-batch/pie-ai-turns-100/"
    },
    {
        "content": "Dear friends, I received a copy of Why We Sleep: Unlocking the Power of Sleep and Dreams as a Christmas gift — back in the pre-Covid era — and finished it last weekend. This book by Matthew Walker, director of University of California Berkeley’s sleep and neuroimaging lab, is a useful reminder of the importance of sleep for learning and also for physical and mental health.Say you spend a few hours learning something new on Wednesday. Getting a solid night of sleep the same day will help consolidate the new memories and strengthen your long-term retention. If your sleep on Wednesday night is disrupted, your long-term retention will be affected even if you catch up on sleep later in the week.But the story doesn’t end there. Over the next few days, your brain may still be busy consolidating the new learnings. A surprising study showed that even if your sleep is disrupted on Friday — two days later — long-term retention can still be significantly affected.Bottom line: After you spend time studying during the day, I encourage you to get a good night’s sleep. Even better, try to get a good night’s sleep every night.The world is going through turbulent times. With society buffeted by biological, social, and political forces, who has time for sleep?! I try to sleep from midnight to 8 a.m. every day, including weekends. With an 18-month-old daughter who wakes up whenever she wants, and occasional meetings with business partners in Asia or in Europe at odd hours, my sleep schedule is far from perfect.You’re probably incredibly busy as well. Despite everything going on, I make sleep a priority, and I hope you will, too.Keep learning,Andrew",
        "summary": "The provided content does not contain information related to Artificial Intelligence or Data Science. Therefore, I cannot generate a summary based on these topics as per the instructions.",
        "publish_date": "Jul 22, 2020",
        "keywords": [
            "Content Analysis",
            "Topic Identification",
            "Information Retrieval",
            "Text Classification",
            "Summarization"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-power-of-sleep/"
    },
    {
        "content": "Dear friends,  I am appalled by the policy, announced on Monday by U.S. Immigrations and Customs Enforcement (ICE), that international students in the country on an F-1 visa must leave if their school goes fully online to cope with Covid-19. Two weeks ago, I   about the suspension of H1-B visas for foreign workers. The policy unveiled this week will deepen the pain of young people who are aiming to contribute to society and further deprive the U.S. of much-needed talent. The new policy, which is being called the #StudentBan on social media, is cruel and capricious. Sometimes an entire family may pool their savings to send someone to study and give them a brighter future. Imagine being halfway to earning a degree and suddenly forced to leave the country amid the pandemic, when your home country may have closed its borders, even to citizens. Students have confided to me their worries about letting down their family or being unable to afford a plane ticket home. University faculty and administrators are scrambling to offer in-person classes, even if it may not be safe or may have little pedagogical benefit, just for the purpose of protecting their international students from deportation. They were already struggling to manage campus shutdowns. This policy delivers another blow at a time when they least can afford it. The U.S. is known worldwide as a great place to receive an education. That’s why I came here many years ago — on an F-1 visa — to attend college. If the U.S. loses this reputation, the whole world will be poorer for it. If my daughter ever studies overseas, I hope that whatever country hosts her will treat her with greater kindness and respect than the U.S. is extending to our international students. Keep learning, Andrew",
        "summary": "The provided content is not related to Artificial Intelligence or Data Science. Therefore, I cannot summarize it according to the specified instructions and role.",
        "publish_date": "Jul 08, 2020",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "summarize",
            "provided content",
            "specified instructions"
        ],
        "url": "https://www.deeplearning.ai/the-batch/let-international-students-study/"
    },
    {
        "content": "Dear friends,  Last week, I wrote about the U.S. Immigration and Customs Enforcement (ICE) policy that would have forced international students to leave the country if their university went fully online to manage the risk of Covid-19. This sudden change in the rules for student visas had students and institutions alike scrambling to figure out ways to comply. Social media erupted in protest as students, parents, teachers, and administrators expressed their concerns. Harvard and MIT sued to block the policy. Attorneys general in at least 18 states brought lawsuits as well. Yesterday, the government rescinded the policy, allowing international students to remain in the U.S. even if they take all their courses online. I am thrilled! ICE’s retreat is an important reminder that our voices can make a difference. I have little doubt that the public outcry helped motivate the universities to sue and the government to backtrack. I believe we all have a responsibility to speak out against injustice — respectfully and with cogent arguments, not “flame wars.” Even if each individual voice is just one among many, collectively we can make a huge impact. Speaking out is especially important for the AI community as we grapple with difficult issues of bias, privacy, surveillance, and disinformation. We need every voice — including yours — to fulfill AI’s promise for the benefit of all people. Keep learning! Andrew",
        "summary": "* The AI community should speak out on difficult issues such as bias, privacy, surveillance, and disinformation.\n* Every voice is needed in the AI community to help fulfill AI’s promise for the benefit of all people.",
        "publish_date": "Jul 15, 2020",
        "keywords": [
            "bias",
            "privacy",
            "surveillance",
            "disinformation",
            "artificial intelligence"
        ],
        "url": "https://www.deeplearning.ai/the-batch/a-victory-for-international-education/"
    },
    {
        "content": "Dear friends, We know that biased data leads to biased machine learning. But does the problem go beyond that? A few colleagues asked about this after a heated exchange on Twitter between Yann LeCun and Timnit Gebru (see “Image Resolution in Black and White” below).There are plenty of documented examples of biased data contributing to bad outcomes. But suppose we find purely unbiased data and build an AI system that helps lenders optimize interest rates for payday loans. We’re careful to make sure the data, algorithms, and learned models don’t discriminate unfairly against any disadvantaged or minority group. Our results are unbiased and in the clear, right?Unfortunately, no. Payday loans are quick-turnaround loans often with very high interest rates — in California, a lender can charge 459 percent interest on a $100, 14-day loan. They target low income individuals. In the U.S., they’re used disproportionately by the Black community. Thus even a fair algorithm will hurt this community especially.Beyond biased data, the way we frame problems, choose what to build, and choose where to deploy can add to or subtract from problems of bias and privilege. An “unbiased” AI technology operating in an unfair social system can contribute to biased outcomes.We still have a lot of work ahead to address harmful biases throughout society. Twenty years ago, the AI community was a small group working on an exciting but obscure technology. Today our community is large, worldwide, and rapidly growing, and we contribute to applications at the center of daily life. We have a greater responsibility than ever to educate ourselves not only in the technology but also in its social context.It’s not always easy to foresee the indirect impact of our work. Who would have guessed that a poorly designed software implementation to enable freedom of speech would lead to toxic communications on social media? But with a broader perspective, I hope our community can better understand the impact of our work and make better decisions about how to help society move forward with greater fairness and less bias.Keep learning!Andrew",
        "summary": "*   Biased data leads to biased machine learning.\n*   Bias in AI goes beyond biased data; even with unbiased data and algorithms, deployment in an unfair social system can contribute to biased outcomes.\n*   Factors like problem framing, choice of what to build, and where to deploy can add to problems of bias and privilege.\n*   The AI community has a responsibility to educate itself on the social context of its work to address harmful biases.",
        "publish_date": "Jul 01, 2020",
        "keywords": [
            "Bias",
            "Biased data",
            "Machine learning",
            "Algorithms",
            "Deployment"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-unbiased-ai-can-lead-to-biased-outcomes/"
    },
    {
        "content": "Dear friends, I spoke last week at the National Intergovernmental Audit Forum, a meeting attended by U.S. federal, state, and local government auditors. (Apparently some of the organizers had taken AI for Everyone.) Many attendees wanted to know how AI systems can be rolled out in a responsible and accountable way.Consider the banking industry. Many regional banks are under tremendous competitive pressure. How well they assess risk directly affects their bottom line, so they turn to credit scoring systems from AI vendors. But if they don’t have the technical expertise to evaluate such models, a hasty rollout can lead to unintended consequences like unfairly charging higher interest rates on loans to minority groups.For AI systems to enjoy smooth rollouts, we need to (a) make sure our systems perform well and pose minimal risk of unintended consequences and (b) build trust with customers, users, regulators, and the general public that these systems work as intended. These are hard problems. They require not just solving technical issues but also aligning technology with society’s values, and expectations.An important part of the solution is transparency. The open source software movement has taught us that transparency makes software better. And if making source code publicly available means that someone finds an embarrassing security bug, so be it! At least it gets fixed.With the rise of AI, we should similarly welcome third-party assistance, such as allowing independent parties to perform audits according to a well established procedure. That way, we can identify problems and fix them quickly and efficiently.After my presentation, the moderator asked me how auditors can avoid getting into adversarial relationships with AI vendors. Instead, we need to build collaborative relationships. By collaborating, we can help make sure the criteria used to judge our systems is reasonable and well specified. For instance, what are the protected groups we need to make sure our systems aren’t biased against? We can also better avoid “gotcha” situations in which our systems are assessed according to arbitrary, after-the-fact criteria.The AI community has a lot of work to do to ensure that our systems are fair, accountable, and reliable. For example, Credo AI (disclosure: a portfolio company of AI Fund, a sister organization to deeplearning.ai) is building tools that help audit and govern AI systems. Efforts like this can make a difference in designing and deploying AI systems that benefit all people.Keep learning!Andrew",
        "summary": "*   Hasty rollouts of AI systems, such as credit scoring in banking without technical expertise, can lead to unintended consequences like unfairly charging higher interest rates to minority groups.\n*   Smooth AI system rollouts require ensuring performance, minimizing unintended consequences, building trust, and aligning technology with society's values and expectations.\n*   Transparency, welcoming third-party audits according to established procedures, and fostering collaborative relationships between auditors and AI vendors are important for identifying problems and defining assessment criteria like protected groups to avoid bias.\n*   The AI community needs to ensure systems are fair, accountable, and reliable, with tools being developed to help audit and govern AI systems.",
        "publish_date": "Aug 05, 2020",
        "keywords": [
            "Bias",
            "Fairness",
            "Accountability",
            "AI governance",
            "AI auditing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/making-ai-fair-accountable-and-reliable/"
    },
    {
        "content": "Dear friends, Earlier this week, I asked a question on social media: What is the most important problem that the AI community should work on?Thousands of you responded. The most frequently mentioned themes included:Climate change and environmental issuesCombating misinformationHealthcare including Covid-19Explainable and ethical AIThank you to each person who responded. I have been reading and thinking a lot about your answers. Many of the most pressing problems, such as climate change, aren’t intrinsically AI problems. But AI can play an important role, and I’m encouraged that so many of you want to do good in the world.Each of us has a role to play. But we rarely succeed alone. That’s why community matters.To my mind, the defining feature of a community is a shared set of values. The medical community prioritizes patients’ wellbeing. When one doctor meets another, their shared priorities immediately create trust and allow them to work together more effectively, say, consulting on complex cases or building initiatives to help underserved people. The academic community also has a history of collaboration stemming from its shared belief in the value of searching for and disseminating knowledge. So, too, in other fields.We in the AI community may share many aims, but the first step toward being more effective as a community is to converge on a set of values we can all stand behind. I believe that if we do this, we can tackle much bigger problems with much greater success.So what, my fellow deep learners, does the AI community stand for? The task of organizing ourselves to tackle big problems together will come later. But first, we need to define the common ground on which we will stand. Many of us hold a strong belief in lifelong learning, sharing information, and working on projects that make society better off. What else? I have ideas of my own, but I would love to hear yours. Please reply to [email protected] or let me know on LinkedIn, Twitter, or Facebook.None of us can solve even one of these issues single-handedly. But working together, I’m optimistic that we can have a huge impact on all of them.Keep learning!Andrew",
        "summary": "*   The AI community identified climate change, combating misinformation, healthcare (including Covid-19), and explainable/ethical AI as the most important problems to work on.\n*   AI can play an important role in tackling these problems, even if they are not intrinsically AI problems.\n*   The AI community needs to converge on a set of shared values to be more effective in tackling big problems.\n*   Potential shared values for the AI community include lifelong learning, sharing information, and working on projects that make society better off.",
        "publish_date": "Aug 12, 2020",
        "keywords": [
            "explainable/ethical AI",
            "climate change",
            "combating misinformation",
            "healthcare",
            "Covid-19"
        ],
        "url": "https://www.deeplearning.ai/the-batch/values-for-the-ai-community/"
    },
    {
        "content": "Dear friends, I’d like to share a programming tip that I’ve used for years. A large part of programming involves googling for code snippets you need on Stack Overflow and other websites. (Shh. Don’t tell the nondevelopers. ????) But that’s not enough if your goal is to maximize your own learning. When the relevant code snippet is just several lines, rather than copy-pasting them from a web page into my code, I usually retype them myself. The physical practice helps train my brain to internalize the concept and syntax.To gain skill as a programmer, you need to internalize both the concepts and the syntax. When I’m trying to help friends get started on coding, I ask them to type print(“Hello World”). By typing it out, you can be sure you know the command’s syntax, such as whether it requires parentheses ( ), square brackets [ ], and so on.You can’t learn to ride a bicycle by reading a book on the theory of bicycling. You have to do it yourself! Coding is more similar to this type of physical skill than most people realize, and practice makes perfect.When you’re trying to master a programming technique, consider these practices:Read a line of code, then type it out yourself. (Bonus points for doing it without looking at the reference code while typing.)Learn about an algorithm, then try to implement it yourself.Read a research paper and try to replicate the published result.Learn a piece of math or a theorem and try to derive it yourself starting with a blank piece of paper.Many creative artists start by replicating the works of artists who came before; so, too, in coding. By replicating examples of good programming (being mindful of copyright and attribution, of course), your brain masters the ability to create them. This frees you to focus on higher-level tasks so you can rearrange what you’ve learned into new, original works.So next time you’re tempted to copy and paste a few lines of code, I hope you’ll start typing instead.Keep learning!Andrew",
        "summary": "* Learning programming concepts and syntax requires active practice, such as retyping code snippets instead of copy-pasting, to internalize knowledge.\n* Mastering programming techniques, like implementing algorithms or replicating research results, is a practical skill similar to learning a physical skill, best developed through hands-on practice.\n* Replicating examples of good programming helps internalize skills, enabling focus on higher-level tasks and the creation of original work.",
        "publish_date": "Sep 16, 2020",
        "keywords": [
            "Implementing algorithms",
            "Replicating research results",
            "Programming concepts",
            "Programming techniques",
            "Active practice"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-learn-coding/"
    },
    {
        "content": "Dear friends,  Did you ever spend days obsessing over a technical problem? If so, I applaud you. Determined pursuit of solutions to hard problems is an important step toward building deep expertise. I’ve been privileged to have worked with several of today’s AI leaders when they were still students. Every one of them spent days, weeks, and months relentlessly trying out different approaches to a range of problems, coming up with hypotheses and performing experiments to hone their intuition. This gave them a thorough understanding of machine learning. It takes many judgement calls to build an effective AI system. How do you tune a particular hyperparameter? What are the tradeoffs between model size, real-time throughput, and accuracy for an application? What type of data pre-processing will yield the best results? When facing complex questions, engineers with deep expertise will come up with better answers. Lately I’ve been thinking about how to train neural networks on small amounts of data. I try to find quiet time to brainstorm, and sometimes I end up with many pages of handwritten notes. After I’ve obsessed over a problem during the day, before I fall asleep I remind my brain that I want to make progress on it. Then, if I’m lucky, I awaken in the morning with new ideas. The world is complex and becoming more so. We need people, in AI and other disciplines, who will take the time and effort to build deep expertise. When a worthy problem taps you on the shoulder, I encourage you to give it your attention. Give yourself the time you need to explore a solutions, and keep at it. It’s not a weird thing to do. Even if you don’t succeed — as a student, I spent countless hours trying, and failing, to prove P ≠ NP, and I don’t regret a minute of it — the journey will make you better. Keep learning! Andrew",
        "summary": "*   Deep expertise in machine learning is built through relentless experimentation, trying different approaches, forming hypotheses, and performing experiments.\n*   Building effective AI systems requires many judgement calls on aspects like hyperparameter tuning, balancing model size, real-time throughput, and accuracy, and selecting data pre-processing methods.\n*   Engineers with deep expertise are better equipped to make complex decisions in AI development.\n*   The author emphasizes the importance of building deep expertise in AI to tackle complex problems, citing personal focus on training neural networks with small amounts of data.",
        "publish_date": "Sep 02, 2020",
        "keywords": [
            "hyperparameter tuning",
            "data pre-processing methods",
            "neural networks",
            "small amounts of data",
            "real-time throughput"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-deep-expertise/"
    },
    {
        "content": "Dear friends,  Today we take it for granted that many people know how to read and write. Someday, I hope, it will be just as common that people know how to write code. Several hundred years ago, society didn’t view language literacy as a necessary skill. A small number of people learned to read and write, and everyone else let them do the reading and writing. It took centuries for literacy to spread, and now society is far richer for it. Words enable deep human-to-human communication. Code is the deepest form of human-to-machine communication. As machines become more central to daily life, that communication becomes ever more important. Traditional software engineering — writing programs that explicitly tell a computer sequences of steps to execute — has been the main path to code literacy. But AI, machine learning, and data science offer a new paradigm in which computers extract knowledge from data. This technology offers another pathway to coding — one that strikes me as even more promising. Many Sundays, I buy a slice of pizza from my neighborhood pizza parlor. The gentleman behind the counter may have little reason to learn how to build software applications (beyond personal growth and the pleasure of gaining a new skill). But AI and data science have great value even for a pizza maker. A linear regression model might enable him to better estimate demand so he could optimize the restaurant’s staffing and supply chain. He could better predict sales of Hawaiian pizza — my favorite! — so he could make more Hawaiian pies in advance and reduce the amount of time customers had to wait for them. Uses of AI and data science can be found in almost any situation that produces data, and I believe that a wide variety of professions will find more uses for custom AI applications and data-derived insights than for traditional software engineering. This makes literacy in AI-oriented coding even more valuable than traditional skills. It could enable countless individuals to harness data to make their lives richer. I hope the promise of building basic AI applications, even more than that of building basic traditional applications, encourages more people to learn how to code. If society embraces this new form of literacy as it has the ability to read and write, we will all benefit. Keep learning! Andrew",
        "summary": "*   AI, machine learning, and data science offer a new paradigm for coding literacy where computers extract knowledge from data, distinct from traditional software engineering.\n*   AI and data science provide significant value across various professions, enabling data-driven insights and applications like using linear regression for demand estimation and optimization.\n*   Uses of AI and data science are found in almost any situation producing data, suggesting many professions will find more utility in custom AI applications and data-derived insights than traditional software engineering.\n*   Literacy in AI-oriented coding is presented as potentially more valuable than traditional coding skills, encouraging more people to learn coding by building basic AI applications.",
        "publish_date": "Sep 09, 2020",
        "keywords": [
            "linear regression",
            "demand estimation",
            "optimization",
            "data-driven insights",
            "custom AI applications"
        ],
        "url": "https://www.deeplearning.ai/the-batch/coding-ai-is-the-new-literacy/"
    },
    {
        "content": "Dear friends, Last week, I asked what values the AI community stands for. Thank you to everyone who replied! The email responses in aggregate ran to 55 pages of text, and I enjoyed reading all of them.A reader who works for a large company wrote, “A purely commercial objective of work is not my calling and I often find myself dreaming about how to break out of the corporate shackles and contribute the rest of my life to doing something meaningful.” These words struck a chord with me. Many of us have the good fortune to find meaning in our work. But if you don’t currently, I hope the AI community will help you do so.Some other comments stood out to me (lightly edited):“A challenge for all of us working in AI is to reimagine the world with respect to concerns like healthcare, education, justice, and environmental protection.” — Shane Ó Seasnáin, Program Manager, Eindhoven AI Systems Institute, Eindhoven“The foundation of our shared values should be refusal to participate in works that would bring harm, regardless of political pressure and monetary rewards.” — Cecilia Cheung, Member, British Computer SocietyWe stand for “fair treatment for all, establishment of trust throughout society, and decreasing the gap between the haves and have-nots.” — Shira L. Broschat, Professor, Washington State University, PullmanThe community “believes in science, data, and facts.” — Nick Brestoff, Chief Inventor, Intraspexion, Seattle“AI has to be made accessible to as many people as possible.” — Benjamin Freisberg, Data Scientist, Substring, BernThe AI community should “engage and empower the community to contribute to all levels of the conversation.” — Reece Robinson, VP Engineering, Orion Health, AucklandWe ought to “push harder on compassion and squeeze out the cruelty.” — Natalie Smithson, Digital Innovation Copywriter, WarwickThese thoughts, and many, many others you sent, are wonderful. But one challenge of pushing on compassion (as in the last comment) is that compassion means different things to different individuals. To one person, it may mean mentoring an underprivileged student. To another, it may mean tuning an algorithm to reduce hate speech in social media.Concepts like compassion, empowerment, and being human are easy to agree on in the absence of specifics, but difficult to define and realize in a concrete way. We all want to be compassionate. But what does that mean in practice?We will reach a common understanding only by considering such abstractions in light of a wide variety of ways they might translate into action. This will require tireless discussion as a community. When we have a chance to talk to one another, let’s take the opportunity to discuss the values we hold in common and what it would mean to stand for them in real life. That way, the next time we feel the urge to take a stand — say, tuning a hyperparameter to reduce hate speech at the cost of revenue — we’re more likely to act in a consistent and principled way.I’m heartened by your responses and encouraged that so many of you are looking for greater meaning and positive impact. I will continue to think about how we can come together as a community and keep the conversation going.Keep learning!Andrew",
        "summary": "* The AI community expresses a desire for work with meaning beyond commercial objectives, aiming to apply AI to societal challenges like healthcare, education, justice, and environmental protection.\n* Stated values include refusing harmful work, promoting fair treatment, trust, equality, and a belief in science, data, and facts.\n* Making AI accessible to as many people as possible and empowering community contribution are highlighted as important values.\n* The community values compassion (e.g., tuning algorithms to reduce hate speech), but translating abstract values into concrete actions requires tireless discussion to reach common understanding.",
        "publish_date": "Aug 19, 2020",
        "keywords": [
            "tuning algorithms",
            "healthcare",
            "education",
            "justice",
            "environmental protection"
        ],
        "url": "https://www.deeplearning.ai/the-batch/values-for-the-ai-community-part-2-readers-respond/"
    },
    {
        "content": "Dear friends, I’ve been trying to teach my toddler the alphabet. Despite having some educational experience, when she mispronounces a vowel for the nth time, I can’t help but feel like I’m doing it wrong. I hope that Nova somehow will still grow up to be literate and consider my efforts to have been adequate.Teachers have been instructing young people in languages for centuries, yet our methods strike me as remarkably uneven. I’ve tried many alphabet instruction software apps, a number of them featuring dancing animals and the like. But my favorite tools have turned out to be a word processor, which lets me type words against a plain white canvas for Nova to read, and letter-shaped stickers that I can rearrange on my kitchen wall.I was struck by how often Nova, like a neural network, wound up in local minima. She learned to count out loud from one to five by uttering a sequence of sounds without understanding the concept of numbers, much like a recurrent neural network generates plausible text without understanding the meanings of the words it uses. I fed her the sequence of sounds, and she overfit to it. Watching her generalize (and sometimes fail to generalize) gave me fresh appreciation for the difficulty of learning from a small number of examples and how crafting a training dataset with care — curriculum learning? — can promote learning.Amid the pandemic, schools worldwide find themselves in varying states of chaos, and many parents are juggling their children’s education with working from home. Many of us have insufficient time and energy to do both well. It can feel like a no-win situation.My heart goes out to everyone who is caught in this bind. I think the best thing a parent can do is to keep loving your kids. As long as you do that, it will be more than enough. Educational apps can be great, and I hope the AI community will come up with better ones, but an attentive parent armed with a pack of post-its and a loving touch or smile is all a child really needs to learn the basics. Beyond the education you impart, the relationship you build will widen the channels of learning for a lifetime.Keep learning!Andrew",
        "summary": "*   The author compares toddler learning to neural networks getting stuck in local minima.\n*   Toddler rote learning is likened to recurrent neural networks generating text without understanding, noting overfitting to input sequences.\n*   Observing toddler generalization highlights the difficulty of learning from small examples and the value of careful training dataset design, potentially using curriculum learning.\n*   The author expresses hope for the AI community to create improved educational apps.",
        "publish_date": "Aug 26, 2020",
        "keywords": [
            "recurrent neural networks",
            "local minima",
            "overfitting",
            "curriculum learning",
            "neural networks"
        ],
        "url": "https://www.deeplearning.ai/the-batch/learning-for-humans-learning-for-machines/"
    },
    {
        "content": "Dear friends, AI researchers keep coming up with impressive innovations: transformer-based language models, self-supervised learning, deep reinforcement learning, small data. All of these developments hold great promise. But some will continue to improve over time and set new directions for AI, and others will turn out to have less impact.How can you tell which is which?I remember seeing early data, over a decade ago, that indicated deep learning algorithms could scale up to become very useful. Similarly, I remember thinking that sequence-to-sequence models, when they were first presented and not yet working well, set a new direction. In these instances, my instincts turned out to be right. But I’ve been wrong, too. For example, in the mid-2000s, I thought that mobile manipulation would take off faster than it has so far.I’ve thought about how to evaluate whether an exciting idea that doesn’t yet work well is likely to become a winner or whether it’s unlikely to improve much for a long time. Over the past decade, three major drivers of improvement in AI performance have been:Computational scaling: Does running an algorithm on computers 10 or 100 times faster result in better performance?Data scaling: Does feeding an AI system more data improve its performance?Algorithmic improvements: Does the data available still hold a significant amount of information that current algorithms do not  extract?I believe these three factors will continue to drive AI performance for years to come. Thus, nascent ideas that can take advantage of them seem more promising to me. If the “only” thing a new algorithm requires to be useful is a 10x improvement in computation speed, you have Nvidia, Intel, and AMD working hard to make that improvement, so it’s a good bet that it will happen.This reasoning leads me to believe that GPT-3 is setting a new direction for building language models and applications. I see a clear path toward scaling computation (by making models cheaper to run or building bigger ones) and algorithmic improvements. At AI Fund (where I’m managing general partner), we’re seeing many entrepreneurs looking to build new companies using GPT-3.On the other hand, I don’t expect quantum computing to have a dramatic impact on AI any time soon. I look forward to quantum AI and I’m glad that many groups are investing in it. But it doesn’t appear to ride any of the three drivers above, and I believe it will take a significant amount of time to become practical for machine learning.Regarding algorithmic improvements, it’s important to note that the information must be in the data for an algorithm to extract it. If someone’s DNA doesn’t contain enough information to determine whether that person will develop diabetes, then no amount of algorithmic work will yield the ability to predict the disease from only the genetic sequence. If humans can perform a task, that’s strong evidence that the data available to humans holds information helpful for completing that task — and that points to the possibility that algorithmic improvements can enable AI to complete it, too.This is why I believe that small data is a promising area: A handful of pictures contains sufficient information for a human to learn to recognize a new object. This offers hope that improved algorithms will be able to extract that information and learn from far fewer examples than are required today.When you hear about an exciting category of emerging AI technology, you might ask yourself whether it can ride on the backs of computational scaling, data scaling, and algorithmic improvement. If so, it’s more likely to make a big impact in the future. We can create immense value if we can get better at recognizing new ideas that, although they may not yet work well today, have potential to become tomorrow’s top performers.Keep learning!Andrew",
        "summary": "*   Three major drivers of AI performance improvement over the past decade have been Computational scaling, Data scaling, and Algorithmic improvements.\n*   Nascent AI ideas that can leverage Computational scaling, Data scaling, and Algorithmic improvements are considered more promising for future impact.\n*   Examples illustrating this evaluation include GPT-3 (promising due to scaling/algorithmic potential), Quantum computing (less promising soon as it doesn't ride these drivers), and small data (promising as human ability suggests algorithmic potential).",
        "publish_date": "Sep 23, 2020",
        "keywords": [
            "GPT-3",
            "Computational scaling",
            "Data scaling",
            "Algorithmic improvements",
            "Quantum computing"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-spot-high-impact-technologies/"
    },
    {
        "content": "Dear friends,This special issue of The Batch celebrates the launch of our new Generative Adversarial Networks Specialization!GANs are among the most exciting technologies to emerge from deep learning. These networks learn in a very different way than typical supervised methods for learning x-to-y mappings. By pitting a discriminator network and a generator network against one another (details below), they produce photorealistic images, medical training data, children’s book illustrations, and other types of output.Earlier today, we held an online panel discussion on “GANs for Good” with Anima Anandkumar, Alexei Efros, Ian Goodfellow, and our course instructor Sharon Zhou. I was struck by the number of new applications GANs are enabling, and the number that are likely to come.Ian explained that GAN-generated training examples for a particular application at Apple are one-fifth as valuable as real examples but cost much less than one-fifth as much to produce. Anima described exciting progress on disentanglement and how the ability to isolate objects in images is making it easier to control image generation (“add a pair of glasses to this face”). Alexei talked about the impact GANs are having on art through tools like Artbreeder.All the speakers talked about alternatives to reading research papers to keep up with the exploding literature. If you missed the live discussion, you can watch a video of the entire event here.We’re still in the early days of practical GAN applications, but I believe they will:Transform photo editing and make it easier to add or subtract elements such as background objects, trees, buildings, and cloudsGenerate special effects for media and entertainment that previously were prohibitively expensiveContribute to creative products from industrial design to fine artAugment datasets in small data problems in fields from autonomous driving to manufacturingAs an emerging technology, GANs have numerous untapped applications. This is a moment to dream up new ideas, because no one else may be working on them yet.I hope this technology will spark your hunger to learn more and invent new applications that will make life better for people all over the world.Keep learning!Andrew",
        "summary": "*   Generative Adversarial Networks (GANs) are deep learning networks that learn by pitting a discriminator and generator network against each other, differing from typical supervised x-to-y mapping.\n*   GANs produce outputs such as photorealistic images, medical training data, and children’s book illustrations.\n*   Current applications include generating training examples (e.g., Apple example: 1/5 value of real examples, cost much less than 1/5), enabling image control via disentanglement, and impacting art through tools like Artbreeder.\n*   Potential future applications include transforming photo editing, generating special effects, contributing to creative products, and augmenting datasets in small data problems in fields like autonomous driving and manufacturing.\n*   GANs are still in the early days of practical applications with numerous untapped possibilities.",
        "publish_date": "Sep 30, 2020",
        "keywords": [
            "Generative Adversarial Networks (GANs)",
            "discriminator network",
            "generator network",
            "image control via disentanglement",
            "augmenting datasets"
        ],
        "url": "https://www.deeplearning.ai/the-batch/introducing-the-generative-adversarial-networks-specialization/"
    },
    {
        "content": "Dear friends,Today Landing AI, where I am CEO, launched LandingLens, an AI-powered platform that helps manufacturers develop computer vision solutions that can identify defective products. For AI to benefit a wide range of industries, we need platforms that enable experts in a variety of fields to build and deploy models. LandingLens is a step in this direction, and it’s available to manufacturers immediately.A major challenge to taking advantage of AI throughout the economy is the sheer amount of customization needed. To use computer vision to inspect manufactured goods, we need to train a different model for each product we want to inspect: each smartphone model, each semiconductor chip, each home appliance, and so on. How can Landing AI build models for thousands of products without hiring thousands of machine learning engineers? It’s much better to empower the manufacturers to build and deploy these models themselves.LandingLens enables experts in manufacturing — rather than experts in machine learning — to collect data, train models, deploy them, and carry out continuous learning. It helps them make sure their models work and scale up deployments. If the test data distribution drifts and the algorithm’s performance suddenly degrades, they’re empowered to collect new data and retrain the model without being beholden to an outside team.Here are a few unique features of LandingLens:Rather than holding the training set fixed and trying to improve the model, we hold the model fixed and help manufacturers improve the training set. We’ve found that this approach leads to faster progress in production settings.Rather than focusing on building models that recognize defects better than humans can, our tools aim to improve human-level performance. The better humans can recognize defects, the more consistently they’ll label those defects in training data, and the better the trained models will be. This is a very different philosophy from usual in AI research, where the goal often is to beat human-level performance.Having led AI teams at large consumer internet companies, I believe it’s time to take AI beyond the technology industry, to all industries. We’ve been building this platform for over a year, and I’m excited to be able to talk about it publicly. I hope that LandingLens — and other verticalized AI development platforms to come — will lower the bar for industrial deep learning and spread the benefits of AI throughout the economy.Keep learning!Andrew",
        "summary": "*   LandingLens is an AI-powered platform enabling manufacturers to develop computer vision solutions for identifying defective products.\n*   It empowers manufacturing experts (not machine learning experts) to collect data, train, deploy, and continuously learn/retrain AI models for customized product inspection.\n*   The platform uniquely focuses on improving the training set while holding the model fixed, found to lead to faster progress in production settings.\n*   Its philosophy aims to improve human-level performance in defect recognition to enhance training data labeling, differing from the common AI research goal of solely beating human-level performance.",
        "publish_date": "Oct 21, 2020",
        "keywords": [
            "LandingLens",
            "computer vision",
            "improving the training set",
            "holding the model fixed",
            "training data labeling"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-rise-of-the-industrial-ai-development-platform/"
    },
    {
        "content": "Dear friends,There’s a lot we don’t know about the future: When will a Covid-19 vaccine be available? Who will win the next election? Or in a business context, how many customers will we have next year?With so many changes going on in the world, many people are feeling stressed about the future. I have a practice that helps me regain a sense of control. Faced with uncertainty, I try to:Make a list of plausible scenarios, acknowledging that I don’t know which will come to pass.Create a plan of action for each scenario.Start executing actions that seem reasonable.Review scenarios and plans periodically as the future comes into focus.For example, back in March, I did this scenario planning exercise. I imagined quick (three months), medium (one year), and slow (two years) recoveries from Covid-19 and made plans for managing each case. These plans have helped me prioritize where I can.The same method can apply to personal life, too. If you’re not sure you’ll pass an exam, get a job offer, or be granted a visa — all of which can be stressful — you can write out what you’d do in each of the likely scenarios. Thinking through the possibilities and following through on plans can help you navigate the future effectively no matter what it brings.Bonus: With a training in AI and statistics, you can calculate a probability to each scenario. I’m a fan of the Superforecasting methodology, in which the judgements of many experts are synthesized into a probability estimate. I refer to this site as a source of probability estimates as well.There will always be uncertainty, but with a little discipline, imagination, and foresight, we can still move forward with confidence.Keep learning!Andrew",
        "summary": "* Training in AI and statistics allows calculating a probability for each scenario.\n* The Superforecasting methodology synthesizes the judgments of many experts into a probability estimate.\n* A specific site is referred to as a source of probability estimates.",
        "publish_date": "Oct 07, 2020",
        "keywords": [
            "Superforecasting methodology",
            "probability estimate",
            "probability",
            "Training",
            "statistics"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-manage-uncertainty/"
    },
    {
        "content": "Dear friends,  My father recently celebrated a milestone: He has completed 146 online courses since 2012. His studies have spanned topics from creative writing to complexity theory. Ronald Ng is a great example of lifelong learning. For him, learning is not a task or a responsibility. It’s a joy. “The joy of learning helps keep the mind sharp and allows us to appreciate the beauty of the subject matter,” he says. “We need to remain mentally young and have the same sense of wonderment” we had as children. And he’s not just taking online courses because he has nothing else to do. At age 74, he continues to work as a hematologist and serves as a court-appointed mediator in his spare time. You never know when learning will show its true value. As a doctor, my father had a patient who suspected he had been poisoned by mercury. The patient’s blood work didn’t show any evidence of this. But my father recalled a course in forensic medicine from Nanyang Technological University, where he had learned that mercury accumulates in hair. He took a hair sample from the patient and found the toxic metal in it. Then he was able to treat the patient appropriately. Growing up, I enjoyed having a father who played violin in the Hong Kong Philharmonic and followed the stars through a telescope on the roof of our apartment building. He taught me a lesson he learned as a volunteer in the army, where he discovered a truth that transcends the knowledge he gained studying subjects like military medicine and leadership: “We need very little in life to make us happy, provided we have the frame of mind to enjoy whatever we have.” You can read an interview with him along with a list of courses he has taken  . I hope his story inspires you to keep learning until you are 74, and well past that, too. Keep learning! Andrew",
        "summary": "The provided content does not contain information related to Artificial Intelligence or Data Science.",
        "publish_date": "Oct 14, 2020",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "content",
            "information",
            "related"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-joy-and-value-of-life-long-learning/"
    },
    {
        "content": "Dear friends,  Over the weekend, my family celebrated my grandfather’s 102nd birthday on Zoom. We dialed in from Hong Kong (my grandfather), the U.S. (myself), the UK, Singapore, and New Zealand. In a normal year, I might not have made it to Hong Kong for the party. But because we now celebrate on Zoom, I was able to attend. For my family, the occasion was a bright spot amid the global tragedy of the pandemic. Many people are wondering when the world will go back to normal. I believe the world one day will become normal again, but the new normal will be very different from the normal of yesteryear. Just as the Covid crisis led me to attend my grandfather’s birthday party, once the virus recedes, our newfound ease with high-quality telecommunications will bring people together virtually for all kinds of purposes. My teams in Colombia now work with my U.S. staff more smoothly than they did pre-Covid — it matters less and less whether my teammates are in Medellin, Colombia, or Palo Alto, California. I look forward to a world where digital communications enable anyone anywhere to receive an education and have access to meaningful job opportunities. I hope all of you will live long, healthy lives like my grandfather. Although we find comfort in the past, it is by actively creating the future that we move forward. It’s up to each of us to constantly envision and create a better future. Keep learning! Andrew",
        "summary": "* The provided content does not contain information related to Artificial Intelligence or Data Science.",
        "publish_date": "Jul 29, 2020",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "Machine Learning",
            "Deep Learning",
            "Algorithm"
        ],
        "url": "https://www.deeplearning.ai/the-batch/global-communications-and-the-new-normal/"
    },
    {
        "content": "Dear friends,The image below shows two photos of the same gear taken under different conditions. From the point of view of a computer-vision algorithm — as well as the human eye — the imaging setup that produced the picture on the right makes a defect in the gear much easier to spot.This example illustrates the power of data-centric AI development. If you want to improve a neural network’s performance, often improving the data it analyzes is far quicker and easier than tinkering with its architecture. In this case, adjusting the imaging setup made the difference.How can you tell that your imaging setup has room for improvement? If you can look at a physical object from a given angle and spot a defect, but you don’t see it clearly in a photo taken from the same angle, then your imaging setup likely could be improved. Parameters that you can control includeIllumination: Is the scene well lit (with diffuse and/or spot lighting), at angles that make clearly visible the features you want your model to recognize? Have you controlled ambient sources such as windows and reflections that may make images less consistent? Are the resulting images consistent and free of glare?Camera position: Make sure the camera is well positioned to capture the relevant features. A defect in, say, a drinking glass or touch screen may be visible from one angle but not from another. And a camera that shakes or moves in response to surrounding vibrations can’t produce consistent images.Image resolution: The density of pixels that cover a given area should be high enough to capture the features you need to see.Camera parameters: Factors such as focus, contrast, and exposure time can reveal or hide important details. Are the features you aim to detect clearly in focus? Are the contrast and exposure chosen to make them easy to see?While deep learning has been used successfully with datasets in which the examples vary widely — say, recognizing faces against backgrounds that range from a crowded concert hall to an outdoor campsite — narrowing the data distribution simplifies computer vision problems. For example, if you want to detect diseased plants, deep learning may be your best bet if you have pictures of plants taken at various distances and under various lighting conditions. But if all the pictures are taken at a fixed distance under uniform lighting, the problem becomes much easier. In practical terms, that means the model will be more accurate and/or need a lot fewer examples. With a consistent dataset, I’ve seen neural networks learn to perform valuable tasks with just 50 images per class (even though I would love to have had 5,000!).Robotics engineers are accustomed to paying attention to the design of imaging systems (as well as audio and other sensor systems). Such attention also can benefit machine learning engineers who want to build practical computer vision systems.Recently I had the pleasure of writing an article with machine vision guru David Dechow that describes these ideas in greater detail. The article focuses on manufacturing, but the approach it describes applies to many computer vision projects where you can influence the imaging setup. Please take a look!Keep learning,Andrew",
        "summary": "*   Data-centric AI development suggests improving data quality (e.g., imaging setup) is often quicker and easier for performance gains than altering neural network architecture.\n*   Adjusting imaging setup parameters like illumination, camera position, resolution, and camera parameters can make features or defects much easier for computer vision algorithms to spot.\n*   Narrowing the data distribution (e.g., consistent lighting, fixed distance) simplifies computer vision problems, potentially leading to more accurate models or requiring significantly fewer examples (e.g., 50 images per class).\n*   Attention to imaging system design, common in robotics/machine vision, benefits machine learning engineers building practical computer vision systems, particularly where the imaging setup can be influenced.",
        "publish_date": "Oct 06, 2021",
        "keywords": [
            "Data-centric AI",
            "Imaging setup",
            "Neural network architecture",
            "Data distribution",
            "Machine vision"
        ],
        "url": "https://www.deeplearning.ai/the-batch/imaging-systems-for-data-centric-ai-development-2/"
    },
    {
        "content": "Dear friends,AI-enabled automation is often portrayed as a binary on-or-off: A process is either automated or not. But in practice, automation is a spectrum, and AI teams have to choose where on this spectrum to operate. It’s important to weigh the social impact of our work, and we must ameliorate automation’s impact on jobs. In addition to this important consideration, the best choice often depends on the application and what AI can and cannot do.Take the problem of diagnosing medical patients from X-rays. The deployment options include:Human only: No AI involved.Shadow mode: A human doctor reads an X-ray and decides on a diagnosis, but an AI system shadows the doctor with its own attempt. The system’s output doesn’t create value for doctors or patients directly, but it is saved for analysis to help a machine learning team evaluate the AI’s performance before dialing it up to the next level of automation.AI assistance: A human doctor is responsible for the diagnosis, but the AI system may supply suggestions. For example, it can highlight areas of an X-ray for the doctor to focus on.Partial automation: An AI system looks at an X-ray image and, if it has high confidence in its decision, renders a diagnosis. In cases where it’s not confident, it asks a human to make the decision.Full automation: AI makes the diagnosis.These options can apply to medical diagnosis, visual inspection, autonomous navigation, media content moderation, and many other tasks. In many cases, I’ve found that picking the right one is critical for a successful deployment, and that using either too much or too little automation can have a significant negative impact.When you’re choosing a point along the automation spectrum, it’s worth considering what degree of automation is possible given the AI system’s accuracy, availability of humans to assist with the task, and desired rate of decision making (for example, human-in-the-loop options won’t work if you need to select an ad to place on a webpage within 100 milliseconds). Today’s algorithms are good enough only for certain points on the spectrum in a given application. As an AI team gains experience and collects data, it might gradually move to higher levels of automation within ethical and legal boundaries.Some people say that we should focus on IA (intelligence augmentation) rather than AI — that AI should be used to help humans perform tasks rather than automate those tasks. I believe we should try to create value for society overall. Automation can transform and create jobs (as when taxi cabs created new opportunities for cab drivers) as well as destroy them. Even as we pick a point on this spectrum, let’s take others’ livelihoods into account and create value that is widely and fairly shared.Keep learning!Andrew",
        "summary": "*   AI-enabled automation exists on a spectrum, not as a binary state, requiring AI teams to choose where to operate.\n*   The spectrum includes options like Human only, Shadow mode, AI assistance, Partial automation, and Full automation, applicable to tasks such as medical diagnosis.\n*   Choosing the appropriate level of automation depends on factors including AI system accuracy, availability of humans, and desired rate of decision making.\n*   Selecting the right point on the spectrum is critical for successful deployment, as using too much or too little automation can have a significant negative impact.\n*   AI teams may gradually move to higher levels of automation with gained experience and collected data, within ethical and legal boundaries.",
        "publish_date": "Feb 24, 2021",
        "keywords": [
            "Shadow mode",
            "AI assistance",
            "Partial automation",
            "Full automation",
            "Medical diagnosis"
        ],
        "url": "https://www.deeplearning.ai/the-batch/choose-the-right-point-on-the-automation-spectrum/"
    },
    {
        "content": "Dear friends, One of the most important skills of an AI architect is the ability to identify ideas that are worth working on. Over the years, I’ve had fun applying machine learning to manufacturing, healthcare, climate change, agriculture, ecommerce, advertising, and other industries. How can someone who’s not an expert in all these sectors find meaningful projects within them? Here are five steps to help you scope projects effectively.Step 1: Identify a business problem (not an AI problem). I like to find a domain expert and ask, “What are the top three things that you wish worked better? Why aren’t they working yet?” For example, if you want to apply AI to climate change, you might discover that power-grid operators can’t accurately predict how much power intermittent sources like wind and solar might generate in the future.Step 2: Brainstorm AI solutions. When I was younger, I used to execute on the first idea I was excited about. Sometimes this worked out okay, but sometimes I ended up missing an even better idea that might not have taken any more effort to build. Once you understand a problem, you can brainstorm potential solutions more efficiently. For instance, to predict power generation from intermittent sources, we might consider using satellite imagery to map the locations of wind turbines more accurately, using satellite imagery to estimate the height and generation capacity of wind turbines, or using weather data to betterpredict cloud cover and thus solar irradiance. Sometimes there isn’t a good AI solution, and that’s okay too.Step 3: Assess the feasibility and value of potential solutions. You can determine whether an approach is technically feasible by looking at published work, what competitors have done, or perhaps building a quick proof of concept implementation. You can determine its value by consulting with domain experts (say, power-grid operators, who can advise on the utility of the potential solutions mentioned above).Step 4: Determine milestones. Once you’ve deemed a project sufficiently valuable, the next step is to determine the metrics to aim for. This includes both machine learning metrics such as accuracy and business metrics such as revenue. Machine learning teams are often most comfortable with metrics that a learning algorithm can optimize. But we may need to to stretch outside our comfort zone to come up with business metrics such as those related to user engagement, revenue, and so on. Unfortunately, not every business problem can be reduced to a matter of optimizing test set accuracy! If you aren’t able to determine reasonable milestones, it may be a sign that you need to learn more about the problem. A quick proof of concept can help supply the missing perspective.Step 5: Budget for resources. Think through everything you’ll need to get the project done including data, personnel, time, and any integrations or support you may need from other teams. For example, if you need funds to purchase satellite imagery, make sure that’s in the budget.This is an iterative process. If, at any step, you find that the current direction is infeasible, return to an earlier step and proceed with your new understanding. Is there a domain that excites you where AI might make a difference? I hope these steps will guide you in exploring it — even if you don’t yet have deep expertise in that field. AI won’t solve every problem, but as a community, let’s look for ways to make a positive impact wherever we can.Keep learning!Andrew",
        "summary": "*   AI project scoping starts by identifying a business problem (not an AI problem) with domain experts.\n*   Brainstorm and assess potential AI solutions for feasibility (technical, PoC) and value (domain expert consultation).\n*   Determine milestones using both machine learning metrics (e.g., accuracy) and business metrics (e.g., revenue), noting that not every business problem reduces to optimizing test set accuracy.\n*   Budget for necessary resources including data, personnel, time, and integrations.",
        "publish_date": "Mar 03, 2021",
        "keywords": [
            "accuracy",
            "test set",
            "machine learning metrics",
            "Proof of Concept (PoC)",
            "integrations"
        ],
        "url": "https://www.deeplearning.ai/the-batch/five-steps-to-scoping-ai-projects/"
    },
    {
        "content": "Dear friends,Years ago, whenever I had to do something boring or unpleasant — such as drive to work or go for a run — I used to listen to music to provide a distraction. Although I still appreciate music, as I got older I decided to cut out distractions. As a result, I’m more likely to sit in silence and enjoy being alone with my thoughts, or use the time more purposefully to learn something from an online course or audio book.Many people listen to music while studying or working. When is it helpful, and when is it distracting? People enjoy music — with good reason — and tend to have strong opinions about it. But some research shows that playing background music while trying to solve problems reduces creativity. Many people in the internet era are used to constant stimulation: scrolling of social media, consuming online news, filling empty hours with TV or video games. But finding quiet time when you can mull over your ideas remains an important part of being creative.To be fair, the findings of research into the effect of music on cognition are mixed. For example, music sometimes improves mood, which in turn leads to better cognitive performance. Music also can drown out background noise that otherwise would be even more distracting. But I’ve found that when working, driving, or exercising, I prefer not to have any distractions and am happy to be left with my own thoughts. Since I stopped listening to music while driving, I’ve noticed that I’m much more likely to end the drive with new ideas for things I want to do.Does this mean you shouldn’t listen to music? Of course not. Listening to music for sheer pleasure is a worthy use of time as well. But now I use music for enjoyment rather than distraction.In addition to listening, one of my favorite ways to take a break from work is to play a piano (not very well!), sometimes with my daughter Nova in my lap providing accompaniment via a random banging on the keys. This serves no utilitarian purpose, but it puts me (and her) in a good mood, and I certainly plan to keep up my efforts to play!Keep learning,Andrew 🎵",
        "summary": "The provided content does not contain information related to Artificial Intelligence or Data Science. Therefore, I cannot generate a summary based on the specified criteria.",
        "publish_date": "Nov 03, 2021",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "Machine Learning",
            "Algorithm",
            "Model"
        ],
        "url": "https://www.deeplearning.ai/the-batch/background-music-pleasure-or-pitfall/"
    },
    {
        "content": "Dear friends,  As I write this letter, the vote count is underway in yesterday’s U.S. presidential election. The race has turned out to be tight. In their final forecast last night, the political analysts at fivethirtyeight.com suggested an 89 percent chance that Joe Biden would win. What did that mean? In repeated trials, such as dice rolls or cohorts of patients with  potentially fatal illness, it’s easy to define the probability of a given event. We have a set of possible universes, and the probability is the fraction of those universes in which the event occurs. We can also ask if a set of probabilistic predictions is calibrated. If so, then out of all the events predicted to occur with an 89 percent chance, around 89 percent of them — neither many more nor many fewer — actually occur. We want our learning algorithms’ probabilistic outputs to be calibrated, and there is a body of literature on this topic. But an election is a one-time event. What does a probability mean in this case? When fivethirtyeight.com says that Biden has an 89 percent chance of winning, I mentally append the phrase “under a certain set of modeling assumptions made by the fivethirtyeight team.” The analysts made a set of assumptions under which they built a number of different universes — some that went for Biden, some Trump — and found that Biden won in 89 percent of them. It’s important to remember that these universes are artificial constructs built on the assumptions that Nate Silver and his team chose. I find that organizations such as fivethirtyeight.com generally make reasonable assumptions. For example, one assumption might be that a state’s vote tally for a given candidate follows a Gaussian distribution, with mean and variance estimated from the polling data. Yet every model has flaws and fails to capture some effects. A model might assume that each state’s outcome is independent of other states — but what if there are pervasive problems with the postal service delivery of mail-in ballots, or systematic biases in polling that result in undercounting some demographics? That’s why, while I consider election polls to be useful, I don’t take their predictions at face value. Even though every model is flawed, good ones allow us to understand the world better. No one knows with certainty if it will rain tomorrow, but my decision to carry an umbrella will differ depending on the probability. That’s why I use probabilities to quantify uncertainties when I make decisions. I find that if you think in probabilities consistently, you’ll start to develop an intuitive feeling for what the numbers mean. When someone tells me something has an 89 percent chance of happening, I’ve heard similar statements enough times in enough different contexts to have an intuition for what might happen next. Like many others, I stayed up late watching the election results trickle in, worried about the future of the U.S. and the potential global impact of this momentous election. Whatever the outcome, let us commit to keep on fighting for fairness, justice, and human decency, and to do our utmost to bring the greatest possible good to the greatest number of people. Keep learning! Andrew",
        "summary": "* Probabilistic forecasts for one-time events, such as fivethirtyeight.com's 89 percent chance for an election, are generated by building \"universes\" based on specific modeling assumptions.\n* It is important for learning algorithms’ probabilistic outputs to be calibrated, meaning that for events predicted with an 89 percent chance, around 89 percent should actually occur.\n* Models rely on assumptions (e.g., Gaussian distribution for vote tallies, independence of states) and parameter estimation from data (e.g., polling data), but every model has flaws and can be affected by biases (e.g., systematic biases in polling).\n* Despite model flaws, probabilistic outputs quantify uncertainties and are useful for informing decision-making.",
        "publish_date": "Nov 04, 2020",
        "keywords": [
            "Probabilistic forecasts",
            "probabilistic outputs",
            "calibrated",
            "Gaussian distribution",
            "parameter estimation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-think-about-probabilities-2/"
    },
    {
        "content": "Dear friends,On Halloween, the veil lifts between the spirit and AI worlds, allowing the two to pass through one another. The resulting paranormal — or, as AI practitioners call it, paragaussian — phenomena raise questions like these:What do you call it when it takes repeated practice to make a scary jack-o’-lantern?A learning carve.Responsible AI requires being candid about what it can do. Who’s the best person to help with this?Dr. Frank-enstein.The ghost of a machine learning engineer visited a museum and defaced all the paintings. Why?She was implementing image wreck-ognition.On Halloween night, when kids in costume go from house to house and only get unpopped popcorn, what do you call it?Kernel trick, or treat.Keep spooking!AndrewP.S. When my daughter Nova was six months old, I bought her a panda stuffed animal. She liked it, and after many panda-related requests, guess what my Halloween costume is? The lesson for me is: Be careful what presents you give, lest they lead to panda-monium.",
        "summary": "*   AI practitioners call paranormal phenomena \"paragaussian\".\n*   Repeated practice to make a scary jack-o’-lantern is called a \"learning carve\".\n*   Responsible AI requires being candid about what it can do.\n*   A ghost of a machine learning engineer implemented \"image wreck-ognition\".\n*   Getting unpopped popcorn on Halloween is called \"Kernel trick, or treat\".",
        "publish_date": "Oct 27, 2021",
        "keywords": [
            "Gaussian",
            "Learning curve",
            "Responsible AI",
            "Image recognition",
            "Kernel trick"
        ],
        "url": "https://www.deeplearning.ai/the-batch/when-the-ai-and-spirit-worlds-collide-category-just-for-fun/"
    },
    {
        "content": "Dear friends,In June, I announced the first Data-centric AI Competition. The deadline for submissions was in early September, and today I’m thrilled to announce the winners!A total of 489 individuals and teams submitted 2,458 unique datasets. By improving the data alone — not the model architecture, which was fixed — many contestants were able to improve on the baseline performance of 64.4% by over 20%. The winners in the Best Performance category achieved between 86.034% and 86.405%. The winners in the Most Innovative category, as well as the honorable mentions, achieved high performance using novel approaches.Congratulations to Divakar Roy, Team Innotescus, and Team Synaptic-AnN, who took the top three spots for Best Performance. Congratulations also to Mohammad Motamedi, Johnson Kuan, and Team GoDataDriven, winners of the Most Innovative category. Pierre-Louis Bescond and Team KAIST-AIPRLab earned honorable mentions. I couldn’t be more proud of you all.You can learn more about their approaches here. I hope you’ll apply these ideas to your own work.The winners joined me at a private roundtable event to discuss how to grow the data-centric AI movement. I was surprised to learn that almost all of them — some of whom have been involved in AI for a long time, and some of whom have little AI background — already have seen positive effects of data-centric techniques in their own work.We chatted about the potential benefits of data-centric AI development to entrepreneurs and startups that may not have access to large datasets, and how it opens machine learning to non-engineers who, although they may not have the skills to build models, can make important contributions by gathering and refining data.We also discussed how working with data is often wrongly viewed as the boring part of machine learning even though, in reality, it’s a critical aspect of any project. I was reminded that, 10 years ago, working with neural networks was viewed in a similar light — people were more interested in hand-engineering features and viewed neural networks as uninteresting. I’m optimistic that the AI community before long will take as much interest in systematically improving data as architecting models.Thank you to all the participants for helping build a foundation for future data-centric AI benchmarks. I hope this competition spurs you to innovate further systematic approaches to improving data. And I hope you’ll compete again in future data-centric AI challenges!Keep learning,Andrew",
        "summary": "*   A Data-centric AI Competition demonstrated that improving data alone, with a fixed model architecture, significantly boosts performance.\n*   Many contestants improved the baseline performance of 64.4% by over 20% through data improvements, with Best Performance winners reaching 86.034% to 86.405%.\n*   Data-centric AI offers benefits for entrepreneurs/startups with limited data and enables non-engineers to contribute through data refinement.\n*   The competition highlights the critical importance of systematic data improvement in ML, aiming to increase community focus on this area.",
        "publish_date": "Oct 20, 2021",
        "keywords": [
            "Data-centric AI",
            "Data-centric AI Competition",
            "fixed model architecture",
            "data refinement",
            "baseline performance"
        ],
        "url": "https://www.deeplearning.ai/the-batch/congrats-to-the-data-centric-competition-winners-category-deeplearning-ai-news/"
    },
    {
        "content": "Dear friends, Welcome to this special Halloween issue of The Batch!In AI, we use many challenging technical terms. To help you keep things straight, I would like to offer some definitions that I definitely would not use. I hope you’ll find this alternative AI glossary a breath of fresh scare:Activation function: An incantation used to raise the deadDropout: A portal to another dimension that suddenly appears underfootEarly stopping: When you’re tired of collecting candy and you go home to bedFeature extraction: Getting a vampire’s fangs out of your neckGreedy policy: Self-explanatory when trick-or-treatingHinge loss: When the squeaky door falls off of a haunted houseLearning rate: How quickly werewolves realize they can’t break down your door but can climb through your windowMini-batch: The amount of candy you have after early stoppingOverfit: When you’ve eaten so much Halloween candy you can’t button your clothesRandom forest: Where random witches liveHappy Halloween to all who celebrate it. Now let’s get this party started!Keep learning,Andrew",
        "summary": "*   An \"alternative AI glossary\" is presented, offering humorous, Halloween-themed analogies for technical terms.\n*   The glossary includes definitions for AI terms such as Activation function, Dropout, Early stopping, Feature extraction, Greedy policy, Hinge loss, Learning rate, Mini-batch, Overfit, and Random forest.\n*   Each term is defined using a Halloween analogy (e.g., Dropout: \"A portal to another dimension\", Overfit: \"When you’ve eaten so much Halloween candy you can’t button your clothes\").",
        "publish_date": "Oct 28, 2020",
        "keywords": [
            "Random forest",
            "Dropout",
            "Hinge loss",
            "Greedy policy",
            "Early stopping"
        ],
        "url": "https://www.deeplearning.ai/the-batch/happy-halloween-an-alternative-ai-glossary/"
    },
    {
        "content": "Dear friends, Building AI products and businesses requires making tough choices about what to build and how to go about it. I’ve heard of two styles:Ready, Aim, Fire: Plan carefully and carry out due diligence. Commit and execute only when you have a high degree of confidence in a direction.Ready, Fire, Aim: Jump into development and start executing. This allows you to discover problems quickly and pivot along the way if necessary.Say you’ve built a customer-service chatbot for retailers, and you think it could help restaurants, too. Should you take time to study the restaurant market before starting development, moving slowly but cutting the risk of wasting time and resources? Or jump in right away, moving quickly and accepting a higher risk of pivoting or failing?Both approaches have their advocates, but I think the best choice depends on the situation.Ready, Aim, Fire tends to be superior when the cost of execution is high and a study can shed light on how useful or valuable a project could be. For example, if your team can brainstorm a few other use cases (restaurants, airlines, telcos, and so on) and evaluate these cases to identify the most promising one, it may be worth taking the extra time before committing to a direction.Ready, Fire, Aim tends to be better if you can execute at low cost and, in doing so, determine whether the direction is feasible and discover tweaks that will make it work. For example, if you can build a prototype quickly to figure out if users want the product, and if canceling or pivoting after a small amount of work is acceptable, then it makes sense to consider jumping in quickly. (When taking a shot is inexpensive, it also makes sense to take many shots. In this case, the process is actually Ready, Fire, Aim, Fire, Aim, Fire, Aim, Fire.)After agreeing upon a product direction, when it comes to building a machine learning model that’s part of the product, I have a bias toward Ready, Fire, Aim. Building models is an iterative process. For many applications, the cost of training and conducting error analysis is not prohibitive. Furthermore, it is very difficult to carry out a study that will shed light on the appropriate model, data, and hyperparameters. So it makes sense to build an end-to-end system quickly and revise it until it works well.But when committing to a direction means making a costly investment or entering a one-way door (meaning a decision that’s hard to reverse), it’s often worth spending more time in advance to make sure it really is a good idea.Keep learning!Andrew",
        "summary": "*   Building AI products involves choosing between Ready, Aim, Fire (careful planning before committing) and Ready, Fire, Aim (quick execution to discover issues and pivot).\n*   Ready, Aim, Fire is superior when execution costs are high and studies can clarify project usefulness (e.g., evaluating multiple use cases).\n*   Ready, Fire, Aim is better when execution costs are low and execution itself determines feasibility and necessary tweaks (e.g., building quick prototypes).\n*   For building machine learning models specifically, the author biases towards Ready, Fire, Aim due to the iterative process and difficulty of upfront studies on models, data, and hyperparameters.\n*   Ready, Aim, Fire is preferred for costly, hard-to-reverse investments (\"one-way doors\").",
        "publish_date": "Sep 01, 2021",
        "keywords": [
            "Ready, Aim, Fire",
            "Ready, Fire, Aim",
            "machine learning models",
            "hyperparameters",
            "iterative process"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-build-ai-products-and-businesses-two-strategies/"
    },
    {
        "content": "Dear friends, How much math do you need to know to be a machine learning engineer? It’s always nice to know more math! But there’s so much to learn that, realistically, it’s necessary to prioritize. Here are some thoughts about how you might go about strengthening your math background.To figure out what’s important to know, I find it useful to ask what you need to know to make the decisions required for the work you want to do. At DeepLearning.AI, we frequently ask, “What does someone need to know to accomplish their goals?” The goal might be building a machine learning model, architecting a system, or passing a job interview.Understanding the math behind algorithms you use is often helpful, since it enables you to debug them. But the depth of knowledge that’s useful changes over time. As machine learning techniques mature and become more reliable and turnkey, they require less debugging, and a shallower understanding of the math involved may be sufficient to make them work.For instance, in an earlier era of machine learning, linear algebra libraries for solving linear systems of equations (for linear regression) were immature. I had to understand how these libraries worked so I could choose among different libraries and avoid numerical roundoff pitfalls. But this became less important as numerical linear algebra libraries matured.Deep learning is still an emerging technology, so when you train a neural network and the optimization algorithm struggles to converge, understanding the math behind gradient descent, momentum, and the Adam optimization algorithm will help you make better decisions. Similarly, if your neural network does something funny — say, it makes bad predictions on images of a certain resolution, but not others — understanding the math behind neural network architectures puts you in a better position to figure out what to do.Sometimes, we’re told that an idea is “foundational.” While there’s a lot to be said for understanding foundations, often this designation is arbitrary and thus not very useful for prioritizing what to study next. For example, computing happens on processors that are packed with transistors. Do you need a deep understanding of how transistors work to write software? It's hard to imagine an AI application where a detailed knowledge of the physics of transistors would affect your decisions.Rather than accepting an authority’s decree that a topic is foundational, it’s worth asking what circumstances would require specific knowledge to help you make better decisions.Of course, I also encourage learning driven by curiosity. If something interests you, go ahead and learn it regardless of how useful it will be in the foreseeable future. Maybe this will lead to a creative spark or technical breakthrough.Keep learning!Andrew",
        "summary": "* Prioritize math knowledge for ML engineers based on its utility in making decisions for tasks like building models or debugging algorithms.\n* The necessary depth of math understanding changes; less is needed for mature techniques (e.g., linear regression libraries), while more is needed for emerging tech like deep learning.\n* Understanding the math behind deep learning optimization algorithms (gradient descent, momentum, Adam) and neural network architectures aids in debugging convergence and prediction issues.\n* Prioritize math based on its practical application in making better decisions in specific ML scenarios, rather than relying on arbitrary \"foundational\" designations.",
        "publish_date": "Aug 11, 2021",
        "keywords": [
            "gradient descent",
            "momentum",
            "Adam",
            "neural network architectures",
            "linear regression libraries"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-learn-math-for-machine-learning/"
    },
    {
        "content": "Dear friends,  Since the pandemic started, several friends and teammates have shared with me privately that they were not doing well emotionally. I’m grateful to each person who trusted me enough to tell me this. How about you — are you doing okay? Last week, the Olympic gymnastic champion Simone Biles temporarily withdrew from competition because she didn’t feel mentally healthy enough to do her best and perhaps avoid a career-ending injury. She’s not alone in struggling with mental health. About 4 in 10 adults in the U.S. reported symptoms of anxiety or depression during the pandemic, according to one  . Once, after looking over a collaborator’s project, I said, “That’s really nice work” and got back a sad facial expression as though my collaborator was near tears. I asked if they were okay, wondering if I had said something wrong, but they paused and shook their head. After I probed gently a little more, they burst out crying and told me that my remark was the first appreciation they had received in longer than they could remember. Many people outwardly look like they’re doing well, but inside they’re lonely, anxious, or uncertain about the future. If you’re feeling fine, that’s great! But if you’re among the millions who feel that something is off-balance, I sympathize, and I want you to know that I care about you and appreciate you. As the pandemic wears on, many of us are hungry to connect with others more deeply. If this describes you, or if you want to help someone else who might feel this way, perhaps you can start by letting someone know you appreciate them or something they did. I think this will make them — and maybe you, too — feel better. Love, Andrew",
        "summary": "The provided content does not contain information related to Artificial Intelligence or Data Science. Therefore, a summary based on these topics cannot be generated from this text.",
        "publish_date": "Aug 04, 2021",
        "keywords": [
            "Neural Network",
            "Convolutional Neural Network",
            "Natural Language Processing",
            "Computer Vision",
            "Gradient Descent"
        ],
        "url": "https://www.deeplearning.ai/the-batch/are-you-ok-taking-care-of-emotional-and-mental-health/"
    },
    {
        "content": "Dear friends, In a recent letter, I mentioned some challenges to building AI products. These problems are distinct from the issues that arise in building traditional software. They include unclear technical feasibility and complex product specification. A further challenge is the need for data to start development.To develop a traditional software product, interviews with potential users might be sufficient to scope out a desirable product, after which you can jump into writing the code. But AI systems require both code and data. If you have an idea for, say, automating the processing of medical records or optimizing logistics networks, you need medical records data or logistics data to train a model. Where can you get it?I see different answers for consumer-facing and business-facing AI products. For consumer-facing (B2C) products, it is generally easier to ask a small group of alpha testers to try out a product and provide data. This may be sufficient to bootstrap the development process. If the data you need is generic to many users — for example, photos on smartphones — it’s also more likely that a team will be able to find or acquire enough data to get started.For business-facing (B2B) AI projects, it’s often difficult to get the data necessary to build a prototype because a lot of highly specialized data is locked up within the companies that produce it. I’ve seen a couple of general ways in which AI teams get around this problem.Some AI teams start by doing NRE (non-recurring engineering, or consulting) work, in which they build highly customized solutions for a handful of customers. This approach doesn’t scale, but you can use it to obtain enough data to learn the lessons or train the models needed to build a repeatable business. Given their need for data, AI startups seem to take this path more often than traditional software startups.Some AI entrepreneurs have worked with multiple companies in a vertical market. For example, someone who has worked for a large public cloud company may have exposure to data from multiple companies in a given industry and witnessed similar issues play out in multiple companies. I’ve also had friends in academia who consulted for multiple companies, which enabled them to recognize patterns and come up with general solutions. Experience like this puts entrepreneurs in a better position to build a nascent product that helps them approach companies that can provide data.If you lack data to get started on an AI project, these tactics can help you get an initial dataset. Once you’ve built a product, it becomes easier to find customers, get access to even more data, and scale up from there.Keep learning!Andrew",
        "summary": "* AI product development requires both code and data, unlike traditional software, presenting challenges such as the need for data to start development, unclear technical feasibility, and complex product specification.\n* Acquiring initial data is generally easier for consumer-facing (B2C) AI products (e.g., via alpha testers or generic data) compared to business-facing (B2B) projects.\n* For business-facing (B2B) AI projects, obtaining necessary data is often difficult as specialized data is locked within companies.\n* Strategies to acquire data for B2B AI projects include starting with NRE/consulting work for specific customers or leveraging prior experience across multiple companies in a vertical market.",
        "publish_date": "Jul 21, 2021",
        "keywords": [
            "AI product development",
            "Data acquisition",
            "B2B AI projects",
            "B2C AI products",
            "Specialized data"
        ],
        "url": "https://www.deeplearning.ai/the-batch/developing-ai-products-part-4-getting-data-to-start-development/"
    },
    {
        "content": "Dear friends, In earlier letters, I discussed some differences between developing traditional software and AI products, including the challenges of unclear technical feasibility, complex product specification, and need for data to start development. This time, let’s examine the further challenge of additional maintenance cost.Some engineers think that when you deploy an AI system, you’re done. But when you first deploy, you may only be halfway to the goal. Substantial work lies ahead in monitoring and maintaining the system. Here are some reasons why:Data drift. The model was trained on a certain distribution of inputs, but this distribution changes over time. For example, a model may have learned to estimate demand for electricity from historical data, but climate change is causing unprecedented changes to weather, so the model’s accuracy degrades.Concept drift. The model was trained to learn an x->y mapping, but the statistical relationship between x and y changes, so the same input x now demands a different prediction y. For example, a model that predicts housing prices based on square footage will lose accuracy as inflation causes prices to rise.Changing requirements. The model was built to perform a particular task, but the product team decides to modify its capabilities. For instance, a model detects construction workers who wander into a dangerous area without a hard hat for more than 5 seconds. But safety requirements change, and now it must flag hatless workers who enter the area for more than 3 seconds. (This issue sometimes manifests as concept drift, but I put it in a different category because it’s often driven by changes in the product specification rather than changes in the world.)Detecting concept and data drift is challenging, because AI systems have unclear boundary conditions. For traditional software, boundary conditions — the range of valid inputs — are usually easy to specify. But for AI software trained on a given data distribution, it’s challenging to recognize when the data distribution has changed sufficiently to compromise performance.This problem is exacerbated when one AI system’s output is used as another AI’s input in what’s known as a data cascade. For example, one system may detect people and a second may determine whether each person detected is wearing a hard hat. If the first system changes — say, you upgrade to a better person detector — the second may experience data drift, causing the whole system to degrade.Even if we detect these issues, our tools for fixing them are immature. Over the past few decades, software engineers have developed relatively sophisticated tools for versioning, maintaining, and collaborating on code. We have processes and tools that can help you fix a bug in code that a teammate wrote 2 years ago. But AI systems require both code and data. If you need to fix a few training examples that a teammate collected and labeled 2 years ago, will you be able to find the documentation and the exact version of the data? Can you verify that your changes are sound and retrain the model on the revised dataset? Tools for data management, unlike tools for code management, are still nascent.Beyond data maintenance, we still have traditional software maintenance to deal with. For instance, many teams had to upgrade from TensorFlow 1 to TensorFlow 2.These problems will recede as data-centric AI tools and methodologies evolve. But for now, being aware of them and planning projects around them can help you build better models and reduce costs.Keep learning!Andrew",
        "summary": "*   AI systems require substantial post-deployment maintenance due to data drift (input distribution changes) and concept drift (relationship changes).\n*   Detecting data and concept drift is challenging due to unclear boundary conditions and is exacerbated by data cascades.\n*   Tools for managing and versioning data, essential for AI maintenance, are less mature than code management tools.\n*   Changing requirements and traditional software maintenance (e.g., library upgrades like TensorFlow 1 to TensorFlow 2) also contribute to AI system maintenance costs.",
        "publish_date": "Jul 28, 2021",
        "keywords": [
            "data drift",
            "concept drift",
            "data cascades",
            "data versioning",
            "TensorFlow 1"
        ],
        "url": "https://www.deeplearning.ai/the-batch/developing-ai-products-part-5-data-drift-concept-drift-and-other-maintenance-issues/"
    },
    {
        "content": "Dear friends, I’ve been following with excitement the recent progress in space launches. Earlier this week, Richard Branson and his Virgin Galactic team flew a rocket plane 53 miles up, earning him astronaut wings. Next week, Jeff Bezos’ Blue Origin is expected to attempt a similar feat and achieve an even greater altitude. (I once also sat in a Blue Origin passenger capsule; see the picture below. I remained firmly on planet Earth.)The first space race was between the U.S. and the Soviet Union, a competition between rival superpowers with dramatically different visions for civilization. Some pundits have panned the current space race as a contest between billionaires, but I’m glad that Bezos, Branson, and Elon Musk are pushing the boundaries of commercial flight.I’ve found space exploration exhilarating since I was a child. My father had a passion for astronomy. We spent many hours on the rooftop of our apartment complex in Singapore — often staying up way past the bedtime designated by my mother 😅 — peering through my dad’s telescope at the planets in our solar system. I remember peering at Alpha Centauri (the closest star system to ours) and wondering if I would visit someday.Space exploration has been criticized as a waste of resources, given the problems we have here at home. Of course, we need to work on problems such as the still-rampaging Covid-19, climate change, poverty, and injustice. I believe society will be best off if we pursue multiple meaningful projects simultaneously.As we push further into space, AI will play an increasing role. Our robots will need to be increasingly autonomous because, even though radio waves travel at the speed of light, there won’t be sufficient time to wait for guidance from human operators on Earth. (Mars averages 13 light minutes from Earth, and the more distant Neptune about 250 light minutes.) I was excited when ROS, the open-source Robot Operating System framework launched by Morgan Quigley out of my Stanford group, started running in the International Space Station. And we still have much work ahead!Private entities are at the center of this week’s space boom, but I would love to see public entities play a bigger role. NASA’s innovations have been widely shared. I’m excited about the Perseverance rover and Ingenuity helicopter now roaming Mars (over 1 million times farther than Branson has yet to travel). So let’s make sure to strongly support public space exploration as well. Further advances will come even faster with their help.Keep learning! 🚀Andrew",
        "summary": "*   AI will play an increasing role as space exploration pushes further.\n*   Space robots will need increasing autonomy due to communication delays (e.g., Mars averages 13 light minutes, Neptune about 250 light minutes).\n*   ROS, the open-source Robot Operating System framework, is running in the International Space Station.",
        "publish_date": "Jul 14, 2021",
        "keywords": [
            "ROS",
            "Autonomy",
            "Space robots",
            "Robotics",
            "International Space Station"
        ],
        "url": "https://www.deeplearning.ai/the-batch/the-new-space-race/"
    },
    {
        "content": "Dear friends, In a recent  , I noted that one difference between building traditional software and AI products is the problem of complex product specification. With traditional software, product managers can specify a product in ways that communicate clearly to engineers what to build — for example, by providing a wireframe drawing. But these methods don’t work for AI products. For an AI product, among the most important parts of the specification are: The space of acceptable operating conditions (also called the operational design domain) The level of performance required under various conditions, including machine learning metrics such as accuracy and software metrics such as latency and throughput Consider the problem of how to build a self-driving car. We might decide the acceptable road conditions for autonomous operation and the acceptable rate of collisions with particular objects at various speeds (for example, gently bumping a traffic cone at five miles per hour every 1 million miles may be okay, but hitting a pedestrian at 20 miles per hour every 1,000 miles is not). Or take reading electronic health records. What is an acceptable error rate when diagnosing a serious disease? How about the error rate when diagnosing a minor disease? What if human-level performance for a particular illness is low, so physicians tend to misdiagnose it, too? Specifying the metrics, and the dataset or data distribution on which the metrics are to be assessed, gives machine learning teams a target to aim for. In this process, we might decide how to define a serious versus a minor disease and whether these are even appropriate concepts to define a product around. Engineers find it convenient to optimize a single metric (such as average test-set accuracy), but it’s not unusual for a practical specification to require optimizing multiple metrics. Here are some ideas that I have found useful for specifying AI products. Clearly define   (or subsets) of data that raise concerns about the system’s performance. One slice might be minor diseases and another major diseases. If the system is intended to make predictions tied to individuals, we might check for undesirable biases by specifying slices that correspond to users of different age groups, genders, ethnicities, and so on. For each slice, specify a level of performance that meets the user’s need, if it’s technically feasible. Also, examine performance across slices to ensure that the system meets reasonable standards of fairness. If the algorithm performs poorly on one slice, it may not be fruitful to tweak the code. Consider using a data-centric approach to improve the quality of data in that slice. Often this is the most efficient way to address the problem. I’ve found it very helpful to have sufficient data and a clear target specification for each slice. This isn’t always easy or even possible, but it helps the team advance toward a reasonable target. As a team performs experiments and develops a sense of what’s possible as well as where the system might falter, the appropriate slices can change. If you’re a machine learning engineer who is part-way through the project, and the product manager changes the product specification, don’t be frustrated! Ask them to buy you a coffee (or tea or other beverage of your choice) for your trouble, but recognize that this is part of developing a machine learning system. Hopefully such changes will happen less frequently as the team gains experience. Keep learning! Andrew",
        "summary": "*   Specifying AI products is complex, requiring definition of acceptable operating conditions (operational design domain) and required performance levels using machine learning (e.g., accuracy) and software metrics (e.g., latency, throughput).\n*   Specifying metrics and the dataset or data distribution for assessment provides machine learning teams a target, noting practical specifications often require optimizing multiple metrics.\n*   Effective specification involves defining data slices (subsets) to identify performance concerns (e.g., bias across demographics) and specifying performance targets for each slice, often addressed by improving data quality in that slice (data-centric approach).\n*   Product specification, including appropriate data slices, can change iteratively as teams understand system capabilities and limitations.",
        "publish_date": "Jul 07, 2021",
        "keywords": [
            "operational design domain (ODD)",
            "data slices",
            "data-centric approach",
            "accuracy",
            "latency"
        ],
        "url": "https://www.deeplearning.ai/the-batch/developing-ai-products-part-3-coping-with-product-specification/"
    },
    {
        "content": "Dear friends,Last week, I mentioned that one difference between traditional software and AI products is the problem of unclear technical feasibility. In short, it can be hard to tell whether it’s practical to build a particular AI system. That’s why it’s worthwhile to quickly assess technical feasibility before committing resources to build a full product.If you have no data or only a handful of examples (enough to get a sense of the problem specification but too few to train an algorithm), consider the following principles:For problems that involve unstructured data (images, audio, text), if even humans can’t perform the task, it will be very hard for AI to do it.A literature review or analysis of what other teams (including competitors) have done may give you a sense of what’s feasible.If you have a small amount of data, training on that data might give you some signals. At the proof-of-concept stage, often the training and test sets are drawn from the same distribution. In that case:If your system is unable to do well on the training set, that’s a strong sign that the input features x do not contain enough information to predict y. If you can’t improve the input features x, this problem will be hard to crack.If the system does well on the training set but not the test set, there’s still hope. Plotting a learning curve (to extrapolate how performance might look with a larger dataset) and benchmarking human-level performance (HLP) can give a better sense of feasibility.If the system does well on the test set, the question remains open whether it will generalize to real-world data.If you’re building a product to serve multiple customers (say, a system to help different hospitals process medical records) and each customer will input data from a different distribution (say, each hospital has a different way of coding medical records), getting data from a few hospitals will also help you assess technical feasibility.Given the heightened technical risk of building AI products, when AI Fund (Deeplearning.AI’s sister company that supports startups) looks at a company, it pays close attention to the team’s technical expertise. Teams with higher technical expertise are much more likely to get through whatever technical risk a business faces.Keep learning!Andrew",
        "summary": "*   AI products have unclear technical feasibility, making quick assessment crucial before committing resources.\n*   For unstructured data, if humans cannot perform the task, AI will find it very hard; training on small data can indicate if input features (x) are insufficient (poor train performance) or if generalization is the issue (good train, poor test; assess with learning curves/HLP).\n*   Assessing feasibility for products serving multiple customers with different data distributions requires obtaining data from a few customers.\n*   Due to heightened technical risk in AI, a team's technical expertise is critical for navigating challenges.",
        "publish_date": "Jun 30, 2021",
        "keywords": [
            "learning curves",
            "HLP",
            "generalization",
            "data distributions",
            "input features"
        ],
        "url": "https://www.deeplearning.ai/the-batch/developing-ai-products-part-2-how-to-assess-technical-feasibility/"
    },
    {
        "content": "Dear friends,Beating human-level performance (HLP) has been a goal of academic research in machine learning from speech recognition to X-ray diagnosis. When your model outperforms humans, you can argue that you’ve reached a significant milestone and publish a paper! But when building production systems, I’ve found that the goal of exceeding HLP isn’t always as useful. I believe the time has come to rethink it.Landing AI, where I’m CEO, has been automating visual inspection for manufacturers. We’ve built computer vision systems that can look at photos of products on an assembly line and classify defects such as scratches and dents. But we’ve run into an interesting challenge: Human experts don’t always agree on the appropriate label to describe the damage. “Is this really a scratch?” If even human experts disagree on a label, what is an AI system to do?In the past, when I built speech recognition systems, I encountered a similar problem. In some audio clips, the person speaking mumbles, or noise in the background overwhelms their words. Despite several listens, no human can transcribe them with confidence.  Even when the words spoken are clear, transcriptions can be inconsistent. Is the correct transcription, “Um, today’s weather,” or “Erm . . . today’s weather”? If humans transcribe the same speech in different ways, how is a speech recognition system supposed to choose among the options?In academic research, we often test AI using a benchmark dataset with (noisy) labels. If a human achieves 90 percent accuracy measured against those labels and our model achieves 91 percent, we can celebrate beating HLP!But when building commercial systems, I’ve found this concept to be only occasionally useful. For example, if an X-ray diagnosis system outperforms human radiologists, does that prove — via incontrovertible logic — that hospital administrators should use it? Hardly. In practice, hospital administrators care about more than beating HLP on test-set accuracy. They also care about safety, bias, performance on rare classes, and other factors on which beating HLP isn’t feasible. So even if you beat HLP on test-set accuracy, your system isn’t necessarily superior to what humans do in the real world.I’ve found that there are better ways to use the concept of HLP. Briefly, our goal as machine learning engineers should be to raise, rather than beat, HLP. I’ll expand on that thought in a future letter.Working on visual inspection, my team has developed a lot of insights into applications of AI in this domain. I’ll keep sharing insights that are generally useful for machine learning practitioners here and in DeepLearning.AI’s courses. But I would like to share manufacturing-specific insights with people who are involved in that field. If you work in ML or IT in manufacturing, please drop me a note at [email protected]. I’d like to find a way to share insights and perhaps organize a discussion group.Keep learning!Andrew",
        "summary": "*   Beating Human-Level Performance (HLP) is a common goal in academic machine learning research but is not always useful for building production systems.\n*   Challenges arise in production systems (e.g., visual inspection, speech recognition) where human experts may disagree on labels, complicating HLP benchmarks.\n*   Commercial systems (e.g., X-ray diagnosis) require factors beyond test-set accuracy on HLP, including safety, bias, and performance on rare classes.\n*   The author suggests the goal for machine learning engineers should be to raise, rather than beat, HLP.",
        "publish_date": "Nov 11, 2020",
        "keywords": [
            "Human-Level Performance (HLP)",
            "visual inspection",
            "speech recognition",
            "X-ray diagnosis",
            "performance on rare classes"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-think-about-probabilities/"
    },
    {
        "content": "Dear friends, Over the past weekend, I happened to walk by a homeless encampment and went over to speak with some of the individuals there. I spoke with a homeless man who seemed to be partially speaking with me, and partially speaking with other people that I could not see. I also spoke with a woman who said she fled her abusive home at the age of 21, and wished that she had a tent — like some of the others — so she could sleep with something over her head rather than be exposed to the elements at night. I feel grateful and privileged every day to have enough food, to have a place to live, and to even have a modern computer with internet access. I’m going to come out and say this (knowing some people will disagree): Every one of us has an obligation to serve others. While we can try to help a handful of people at a time with a meal or a donation — and this is to be celebrated — I don’t know how to systematically help the large and growing number of homeless. But I will keep thinking on this, and am determined to find a way. Even as we build amazing products and technologies, let’s keep thinking about how we can scalably serve the many wonderful, resilient individuals like the ones I met last weekend. Keep learning! Andrew",
        "summary": "Based on the provided content, there is no information strictly related to Artificial Intelligence or Data Science that meets the requirements for summarization (e.g., technical terms, specific findings, numbers, or concepts within AI/Data Science). The text focuses on a personal reflection on homelessness and the challenge of scalable service, mentioning \"amazing products and technologies\" and \"scalably serve\" in a general context, but without specific details pertaining to AI or Data Science applications or research.",
        "publish_date": "Mar 17, 2021",
        "keywords": [
            "Artificial Intelligence",
            "Data Science",
            "technical terms",
            "concepts",
            "applications"
        ],
        "url": "https://www.deeplearning.ai/the-batch/privilege-and-obligation/"
    },
    {
        "content": "Dear friends,On Monday, Landing AI (where I’m CEO) announced the close of a $57 million Series A funding round. The investment enables the company to continue building its data-centric MLOps platform for computer vision, with a focus on manufacturing visual inspection.Studies estimate that AI will create trillions of dollars of value, and machine learning already has changed the trajectory of consumer-internet companies like Google and Facebook. Yet the technology has barely penetrated most other industries. Making AI work in more traditional industries will require a different recipe than internet companies use. I explained why this week at Fortune’s Brainstorm A.I. event, pictured below.Datasets are much smaller. I once built a face recognition system using about 350 million images. But when I asked people in the manufacturing industry how many images they had of each defect they wanted to recognize, 50 or fewer was the most common answer. Techniques developed for learning from hundreds of millions of examples will struggle to work with only 50. But the situation improves if you choose those examples well. Data-centric AI tools can help you get there.Applications are more diverse. If we took all current and potential machine learning projects and sorted them in decreasing order of value, we might find that the “head” of the distribution comprises applications like a large company’s web search engine, online ad system, or product recommendation engine. This is followed by a “long tail” of applications that have lower value individually but massive value in aggregate. As a community, we’ve figured out how to organize dozens or hundreds of engineers to build these large applications, some of which can generate over $1 billion of value. But this recipe doesn’t work for other industries where applications are more heterogeneous and where each of 10,000 machine learning models generates $1 million to $5 million each.For example, in manufacturing, each plant makes a different product, and thus will need a different trained model to detect defects. In healthcare, every hospital codes its electronic health records (EHR) differently. Rather than a single monolithic model to read every hospital’s EHR, each hospital needs a system trained on its own data. The total value of these applications is enormous. But how can any company help build, deploy and maintain 10,000 custom models without hiring 10,000 machine learning engineers?This “long tail” problem helps to explain why many proof-of-concept implementations and demos don’t make it into production. While a team of engineers can build a one-off application, we still need better tools to make this type of work scalable and economically viable.Landing AI is building tools to make it fast and easy for manufacturers to engineer the data so as to train, deploy, and maintain their own computer vision systems. This design pattern addresses the widespread problems of small datasets and diverse applications. If you’re working in a sector other than manufacturing, consider if your sector has a long tail of applications and if building an MLOps platform to let customers do their own customization — as Landing AI is doing in manufacturing — might advance machine learning in your industry.Keep learning!Andrew",
        "summary": "*   Traditional industries face challenges deploying AI due to smaller datasets (e.g., 50 or fewer images of defects vs. 350 million for face recognition), where techniques for large datasets struggle; data-centric AI tools are suggested to help.\n*   These industries also have a \"long tail\" of diverse applications requiring many custom models (e.g., 10,000 models generating $1M-$5M each), unlike internet companies' few high-value applications.\n*   The \"long tail\" problem, stemming from small datasets and diverse applications, makes scaling deployment and maintenance of numerous custom models difficult, hindering production adoption.\n*   Landing AI is building a data-centric MLOps platform for computer vision in manufacturing to address small datasets and diverse applications by enabling customers to engineer data and manage their own systems.",
        "publish_date": "Nov 10, 2021",
        "keywords": [
            "data-centric AI",
            "MLOps platform",
            "computer vision",
            "small datasets",
            "long tail"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-unlock-ai-value/"
    },
    {
        "content": "Dear friends,Engineers need strong technical skills to be successful. But many underestimate the importance of developing strong communication skills as well.Many AI products are so complex that it’s hard for any single person — no matter how talented — to build the whole thing. As teamwork becomes more central to AI development, clear communication is becoming more important, too.In large and small companies, I’ve seen senior engineers with no management responsibility (often called individual contributors) whose words carried more weight than those of VPs who managed large teams. They often had a massive positive impact on the projects they took part in. How did they accomplish this? These individuals are generally:Technically sophisticated, with a deep understanding of the most promising technical approach to a problem.Cross-functional collaborators who can help match technology with business goals.Positive contributors to the company’s culture. For example, they foster a transparent and safe environment where ideas are evaluated based on merit and all voices can be heard.Clear communicators who help others understand their thinking through speaking or writing.What if you’re not yet a strong communicator? That’s okay! I used to struggle with my writing and speaking as well, and I still have ample room for improvement. Last week, while I was giving a practice talk on a new way to think about data (yes, I do practice talks), a friend told me that a section of my presentation was confusing. He was right! I try to embrace critical feedback on my communications and hope you will, too.There’s no need to set an impossible standard for yourself; just aim to improve a little every month. The only person you should compare yourself to is the person you used to be. Let us all keep trying to be better than our previous selves.Keep learning!Andrew",
        "summary": "*   Clear communication is becoming increasingly important in AI development due to the complexity of AI products and the growing centrality of teamwork.\n*   Highly impactful individual contributor engineers are typically technically sophisticated, cross-functional collaborators, positive cultural contributors, and clear communicators.\n*   Improving communication skills is an ongoing process, and embracing critical feedback is encouraged for continuous improvement.",
        "publish_date": "Mar 10, 2021",
        "keywords": [
            "AI development",
            "AI products",
            "individual contributor engineers",
            "technically sophisticated",
            "cross-functional collaborators"
        ],
        "url": "https://www.deeplearning.ai/the-batch/theres-no-substitute-for-communication-skills/"
    },
    {
        "content": "Dear friends, Last Sunday was my birthday. That got me thinking about the days leading to this one and those that may lie ahead.As a reader of The Batch, you’re probably pretty good at math. But let me ask you a question, and please answer from your gut, without calculating.How many days is a typical human lifespan?20,000 days100,000 days1 million days5 million daysWhen I ask friends, many choose a number in the hundreds of thousands. (Many others can’t resist calculating the answer, to my annoyance!)When I was a grad student, I remember plugging my statistics into a mortality calculator to figure out my life expectancy. The calculator said I could expect to live a total of 27,649 days. It struck me how small this number is. I printed it in a large font and pasted it on my office wall as a daily reminder.That’s all the days we have to spend with loved ones, learn, build for the future, and help others. Whatever you’re doing today, is it worth 1/30,000 of your life?Let’s make every day count.Keep learning!AndrewP.S. Don’t worry about me. I’m healthy and plan to stick around for awhile.P.P.S. A huge thank-you to everyone who responded to my earlier online note about my birthday! ❤️",
        "summary": "*   Statistics were used in a mortality calculator to figure out life expectancy.\n*   The mortality calculator indicated a life expectancy of 27,649 days.",
        "publish_date": "Apr 21, 2021",
        "keywords": [
            "Statistics",
            "mortality calculator",
            "life expectancy",
            "Data",
            "Modeling"
        ],
        "url": "https://www.deeplearning.ai/the-batch/make-every-day-count/"
    },
    {
        "content": "Dear friends,In school, most questions have only one right answer. But elsewhere, decisions often come down to a difficult choice among imperfect options. I’d like to share with you some approaches that have helped me make such decisions.When I was deciding where to set up a satellite office outside the U.S., there were many options. My team and I started by listing important criteria such as supply of talent, availability of local partners, safety and rule of law, availability of visas, and cost. Then we evaluated different options against these criteria and built a matrix with cities along one axis and our criteria along the other. That clarified which country would make a great choice.When I feel stuck, I find it helpful to write out my thoughts:What options am I choosing among?What criteria are driving the choice?How does each option rate with respect to the criteria?If I need more information, how can I get it?Documenting decisions in this way also builds a foundation for further choices. For example, over the years, I’ve collected training data for many different kinds of problems. When I need to select among tactics for acquiring data, having been through the process many times, I know that some of the most important criteria are (i) the time needed, (ii) the number of examples, (iii) accuracy of the labels, (iv) how representative the input distribution is, and (v) cost.If I’m making a decision as part of a team, I check with teammates at each step to make sure we’re accurately capturing the top options, criteria, and so on. (The comments feature in Google Docs is a great way to facilitate open debate within a team.) This helps me avoid losing track of some criteria and acting based on an incomplete set; for example, picking the satellite office’s location based only on the availability of talent. It also helps align everyone on the final decision.As you may know, I wound up setting up a satellite office in Colombia because of the availability of talent and a supportive ecosystem of partners. The team there has become a key part of many projects. Lately I’ve worried about their wellbeing amid Covid-19 and widespread unrest. But in hindsight, setting up in Colombia was one of my best decisions, and I remain as committed as ever to supporting my friends there.Keep learning!Andrew",
        "summary": "*   A structured decision-making approach, involving criteria and evaluation, is applicable to selecting tactics for acquiring training data.\n*   Experience in collecting training data informs the identification of important criteria for data acquisition tactics.\n*   Key criteria for acquiring data include time needed, number of examples, accuracy of labels, representativeness of the input distribution, and cost.",
        "publish_date": "Jun 02, 2021",
        "keywords": [
            "training data",
            "data acquisition",
            "accuracy of labels",
            "representativeness of the input distribution",
            "number of examples"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-make-tough-decisions/"
    },
    {
        "content": "Dear friends, Every day, I’m inspired by the efforts of you who take our courses, gain technical skills, find jobs, or build things that I never would have imagined. To each one of you who is learning about and building AI, thank you. The world needs more people like you! The DeepLearning.AI   highlights a few individuals who have made their way into the field. Each post describes one person’s path to building an AI career: their struggles, breakthroughs, and career tips. Perhaps someday we'll highlight your story as well! Despite these successes and many others, the AI community still has a lot of room to grow. Even though we’ve collectively built amazing systems — web search engines, smart speakers, self-driving cars — every time I speak with any CEO, government leader, or academic official, I become aware of valuable AI projects that no one is working on because there are simply too few of us. For the world to reap the bounty of AI, the community of AI developers needs to grow much larger. To that end, in this special issue of The Batch, we offer a set of articles designed to help people who are wondering how to take the next step forward. I hope you’ll find them useful whether you’re debating whether to take your first course, starting to look for a job, or aiming to advance an established career. I still find building AI systems to be the most fun thing I can imagine doing professionally, and I hope you will, too! Wherever you are in your AI journey, let’s take the next step together. Keep learning, Andrew",
        "summary": "*   DeepLearning.AI highlights individuals' paths to building an AI career, including struggles, breakthroughs, and career tips.\n*   Despite successes in building systems like web search engines, smart speakers, and self-driving cars, the AI community needs significant growth as there are too few people for valuable AI projects.\n*   Articles are provided to help people take the next step in their AI journey, from starting to advancing a career.",
        "publish_date": "Jan 19, 2022",
        "keywords": [
            "DeepLearning.AI",
            "web search engines",
            "smart speakers",
            "self-driving cars",
            "AI career"
        ],
        "url": "https://www.deeplearning.ai/the-batch/deep-learners-we-salute-you/"
    },
    {
        "content": "Dear friends,It can take 6 to 24 months to bring a machine learning project from concept to deployment, but a specialized development platform can make things go much faster.My team at Landing AI has been working on a platform called LandingLens for efficiently building computer vision models. In the process, I’ve learned important lessons about how such platforms can help accelerate the machine learning project lifecycle:Data collection: Ambiguity in labels (what is the “correct” value of y?) plagues many projects. If the labels are inconsistently defined, it’s impossible to achieve a high test-set accuracy. But it’s difficult to find these inconsistencies manually and to convince stakeholders (often subject-matter experts) to resolve them. An MLOps platform can identify problems and encourage consistency.Model training: The ability to write code to train a model in TensorFlow or PyTorch is a valuable skill. But even for skilled engineers, it’s faster to use a no-code platform that lets you do this via mouse clicks (to manage data augmentation, link the data and model, manage GPU training resources, keep track of data/model versions, and provide visualizations and metrics for error analysis).Production deployment: Many teams can execute a successful proof of concept and achieve high-test set accuracy. But to secure budgets and approval for deployment, a small demo can help others see a project’s value. A platform can make it easy to implement a demo that runs not just in a Jupyter notebook but in a lightweight deployment environment such as a mobile app or simple edge device.It used to take me months to deploy a model. With a no-code platform, I can train a RetinaNet demo, carry out error analysis, use a data-centric approach to clean up inconsistent data, retrain, and deploy to an edge device — all in 60 minutes. I get a thrill every time I go through the machine learning project lifecycle so quickly.Platforms like this can help a variety of AI projects across all industries. LandingLens works well for visual inspection in areas as diverse as automotive, semiconductor, and materials, I’m hoping to make it more widely available. Its sweet spot is computer vision problems (detection or segmentation) with 30 to 10,000 images. If you have a business problem in computer vision that falls in this sweet spot, I’d like to hear from you. Please get in touch by filling out this form.Keep learning!Andrew",
        "summary": "*   Specialized platforms accelerate the machine learning project lifecycle, which can take 6 to 24 months from concept to deployment.\n*   Platforms help improve data collection quality by identifying label inconsistencies critical for achieving high test-set accuracy.\n*   No-code platforms enable faster model training via clicks, managing data augmentation, GPU resources, versioning, and providing error analysis metrics/visualizations.\n*   They facilitate production deployment by simplifying demo creation for lightweight environments like mobile apps or edge devices.\n*   Using a no-code platform, a computer vision model demo (RetinaNet) can be trained, analyzed, refined, and deployed to an edge device in 60 minutes.",
        "publish_date": "May 05, 2021",
        "keywords": [
            "RetinaNet",
            "data augmentation",
            "label inconsistencies",
            "error analysis",
            "computer vision"
        ],
        "url": "https://www.deeplearning.ai/the-batch/data-centric-ai-development-the-platform-approach/"
    },
    {
        "content": "Dear friends,Russian troops have invaded Ukraine, and the terrifying prospect of a war in Europe weighs on my mind. My heart goes out to all the civilians affected, and I hope we won’t see the loss of life, liberty, or property that many people fear.I’ve often thought about the role of AI in military applications, but I haven’t spoken much about it because I don’t want to contribute to the proliferation of AI arms. Many people in AI believe that we shouldn’t have anything to do with military use cases, and I sympathize with that idea. War is horrific, and perhaps the AI community should just avoid it. Nonetheless, I believe it’s time to wrestle with hard, ugly questions about the role of AI in warfare, recognizing that sometimes there are no good options.Full disclosure: My early work on deep learning was funded by the U.S. Defense Research Projects Agency, or DARPA. Last week, Wired mentioned my early work on drone helicopters, also funded by DARPA. During the U.S.-Iraq war, when IEDs (roadside bombs) were killing civilians and soldiers, I spent time thinking about how computer vision can help robots that dispose of IEDs.What may not be so apparent is that forces that oppose democracy and civil liberties also have access to AI technology. Russian drones have been found to contain parts made in the U.S. and Europe. I wouldn’t be surprised if they also contain open-source software that our community has contributed to. Despite efforts to control exports of advanced chips and other parts that go into AI systems, the prospects are dim for keeping such technology out of the hands of people who would use it to cause harm.So I see little choice but to make sure the forces of democracy and civil liberties have the tools they need to protect themselves.Several organizations have come to the same conclusion, and they’ve responded by proposing principles designed to tread a fine line between developing AI’s capacity to confer advantage on the battlefield and blunting its potential to cause a catastrophe. For example, the United Nations has issued guidance that all decisions to take human life must involve human judgment. Similarly, the U.S. Department of Defense requires that its AI systems be responsible, equitable, traceable, reliable, and governable.I support these principles. Still, I’m concerned that such guidelines, while necessary, aren’t sufficient to prevent military abuses. User interfaces can be designed to lead people to accept an automated decision — consider the pervasive “will you accept all cookies from this website?” pop-ups that make it difficult to do anything else. An automated system may comply technically with the U.N. guidance, but if it provides little context and time for its human operator to authorize a kill mission, that person is likely to do so without the necessary oversight or judgment.While it’s important to establish high-level principles, they must be implemented in a way that enables people to make fateful decisions — perhaps the most difficult decisions anyone can make — in a responsible way. I think of the protocols that govern the use of nuclear weapons, which so far have helped to avoid accidental nuclear war. The systems involved must be subject to review, auditing, and civilian oversight. A plan to use automated weapons could trigger protocols to ensure that the situation, legality, and schedule meet strict criteria, and that the people who are authorized to order such use are clearly identified and held accountable for their decisions.War is tragic. Collectively we’ve invented wondrous technologies that also have unsettling implications for warfare. Even if the subject presents only a menu of unpalatable options, let’s play an active role in navigating the tough choices needed to foster democracy and civil liberties.Keep learning,Andrew",
        "summary": "*   AI has a significant and debated role in military applications and warfare, with concerns about the proliferation and accessibility of AI technology to various actors.\n*   Principles for military AI use, such as the UN guidance requiring human judgment for decisions taking human life and the U.S. Department of Defense's requirements for AI systems to be responsible, equitable, traceable, reliable, and governable, are considered necessary but potentially insufficient.\n*   Effective implementation of AI systems, including user interface design and providing sufficient context and time for human operators, is crucial for ensuring meaningful oversight and preventing potential abuses of automated decisions.\n*   Robust governance, review, auditing, oversight, and clear accountability protocols are needed for automated weapons systems to ensure responsible use.",
        "publish_date": "Feb 23, 2022",
        "keywords": [
            "Military AI",
            "Human Judgment",
            "Automated Decisions",
            "Responsible AI",
            "Accountability Protocols"
        ],
        "url": "https://www.deeplearning.ai/the-batch/does-ai-have-a-role-in-warfare/"
    },
    {
        "content": "Dear friends,I decided last weekend not to use a learning algorithm. Sometimes, a non-machine learning method works best.Now that my daughter is a little over two years old and highly mobile, I want to make sure the baby gate that keeps her away from the stairs is always shut. It’s easy to forget and leave it open when walking through. How do you do this?I started designing a system where I’d collect images of the gate both open and shut, and train a neural network to distinguish between the two. Then I would use TensorRT to deploy the model on a Raspberry Pi computer, which would beep if the gate were left open for more than 60 seconds.I got as far as wiring up the system. Then I found a refrigerator-door alert widget that does the same job by sensing when a magnet is separated from a detector.It goes to show that sometimes you don’t need a big neural network to do the job. (But when you do need one, it’s handy.) That’s why it’s nice to have a portfolio of techniques. Then we can better pick the right one for a given job.Perhaps one lesson here is to pick the right sensor: To do the job with a camera, I needed a computer vision algorithm. But with a magnetic sensor, making the decision to beep when the gate is left open becomes trivial.Keep learning!Andrew",
        "summary": "*   An AI approach considered for a baby gate monitor involved training a neural network on images (open/shut) and deploying on a Raspberry Pi with TensorRT to alert if open > 60 seconds.\n*   A simpler, non-machine learning solution using a magnetic sensor was found to be more suitable for the task.\n*   The example highlights that sometimes non-machine learning methods are more effective than complex AI/computer vision algorithms.\n*   Choosing the right technique from a portfolio is crucial, as sensor choice (camera vs. magnetic) impacts algorithm complexity.",
        "publish_date": "May 19, 2021",
        "keywords": [
            "neural network",
            "TensorRT",
            "Raspberry Pi",
            "magnetic sensor",
            "computer vision"
        ],
        "url": "https://www.deeplearning.ai/the-batch/when-not-to-use-machine-learning/"
    },
    {
        "content": "Dear friends,Machine learning development is highly iterative. Rather than designing a grand system, spending months to build it, and then launching it and hoping for the best, it’s usually better to build a quick-and-dirty system, get feedback, and use that feedback to improve the system.The iterative aspect of machine learning applies to many steps. For example:Data labeling: It’s hard to come up with fully fleshed-out labeling guidelines that result in clean and consistent labels on your first attempt. It might be better to use an initial set of guidelines to label some data, see what problems arise, and then improve the guidelines.Model training: Building an AI system requires deciding what data, hyperparameters, and model architecture to use. Rather than overthinking these choices, it’s often better to train an initial model, then use error analysis to drive improvements.Deployment and monitoring: When deploying a machine learning system, you might implement dashboards that track various metrics to try to spot concept drift or data drift. For example, if you’re building a product recommendation system, you might track both software metrics such as queries per second and statistical metrics such as how often the system recommends products of different categories. What metrics should we track? Rather than try to design the perfect set of dashboards before launch, I find it more fruitful to pick a very large set of metrics, evolve them, and prune the ones that prove less useful.Iteration is helpful in other phases of machine learning development as well. It make sense to take an empirical, experimental approach to decision making whenever:Multiple options are available and it's hard to know the best choice in advance.We can run experiments to get data quickly about the performance of different options.These two properties hold true for many steps in a typical ML project.One implication is that, if we can build tools and processes that enable high-throughput experimentation, we can make faster progress. For instance, if you have an MLOps platform that enables you to quickly train and evaluate new models, this will allow you to improve models more quickly.This principle applies to other aspects of ML development that are iterative. That’s why time spent optimizing your team's capacity to run many experiments can pay off well.Keep learning!Andrew",
        "summary": "*   Machine learning development is highly iterative, favoring building quick systems, getting feedback, and using that feedback for improvement over designing grand systems upfront.\n*   This iterative approach applies to steps like data labeling (improving guidelines), model training (using error analysis to drive improvements), and deployment/monitoring (evolving metrics).\n*   Iteration is beneficial when multiple options are available, the best choice is hard to know in advance, and experiments can quickly provide performance data.\n*   Building tools and processes that enable high-throughput experimentation, such as MLOps platforms, can significantly accelerate progress in iterative ML development.",
        "publish_date": "Apr 14, 2021",
        "keywords": [
            "data labeling",
            "model training",
            "error analysis",
            "deployment/monitoring",
            "MLOps platforms"
        ],
        "url": "https://www.deeplearning.ai/the-batch/iteration-in-ai-development/"
    },
    {
        "content": "Dear friends, How much data do you need to collect for a new machine learning project? If you’re working in a domain you’re familiar with, you may have a sense based on experience or from the literature. But when you’re working on a novel application, it’s hard to tell. In this circumstance, I find it useful to ask not how much data to collect but how much time to spend collecting data.For instance, I’ve worked on automatic speech recognition, so I have a sense of how much data is needed to build this kind of system: 100 hours for a rudimentary one, 1,000 hours for a basic one, 10,000 hours for a very good one, and perhaps 100,000-plus hours for an absolutely cutting-edge system. But if you were to give me a new application to work on, I might find it difficult to guess whether we need 10 or 10,000 examples.When starting a project, it's useful to flip the question around. Instead of asking:           How many days will we need to collect m training examples? I ask,          How many training examples can we collect in d days?Taking a data-centric approach to model development, let’s say it takes about two days to train a model and two days to perform error analysis and decide what additional data to collect (or how to tweak the model). How many days should you spend collecting data before training and error analysis? Allocating comparable amounts of time to each step seems reasonable, so I would advocate budgeting a couple of days — a week at most — for data collection. Then iterate through the loop.I’ve seen many teams spend far too much data collecting data before jumping into the model development loop. I’ve rarely seen a team spend too little time. If you don’t collect enough data the first time around, usually there’s time to collect more, and your efforts will be more focused because they’ll be guided by error analysis.When I tell a team, “Let’s spend two days collecting data,” the time limit often spurs creativity and invention of scrappy ways to acquire or synthesize data. This is much better than spending two months collecting data only to realize that we weren’t correcting the right data (say, the microphone we used was too noisy, leading to high Bayes/irreducible error).So, next time you face an unfamiliar machine learning problem, get into the model iteration loop as quickly as possible, and set a limited period of time for collecting data the first time around, at least. You’re likely to build a better model in less time.Keep learning!AndrewP.S. Once I created an unnecessarily scramble when asked a team to make sure that data collection took no longer than two days. Because of a bad Zoom connection, they thought I said “today.” Now I've learned to hold up two fingers whenever I say “two days” on a video call.",
        "summary": "*   For novel machine learning applications, instead of asking how much data is needed, it is useful to ask how much time to spend collecting data.\n*   Specific data needs vary; for automatic speech recognition, rudimentary systems need 100 hours, basic 1,000 hours, very good 10,000 hours, and cutting-edge 100,000-plus hours.\n*   Budget a couple of days (a week at most) for initial data collection before entering the model iteration loop (training, error analysis).\n*   Limiting initial data collection time allows error analysis to guide subsequent efforts, spurs creativity, and helps avoid collecting data with high Bayes/irreducible error.",
        "publish_date": "Apr 28, 2021",
        "keywords": [
            "automatic speech recognition",
            "model iteration loop",
            "error analysis",
            "Bayes/irreducible error",
            "data collection"
        ],
        "url": "https://www.deeplearning.ai/the-batch/data-centric-ai-development-part-3-limit-data-collection-time/"
    },
    {
        "content": "Dear friends,Benchmarks have been a significant driver of research progress in machine learning. But they've driven progress in model architecture, not approaches to building datasets, which can have a large impact on performance in practical applications. Could a new type of benchmark spur progress in data-centric AI development?Remember: AI System = Code (model/algorithm) + DataMost benchmarks provide a fixed set of Data and invite researchers to iterate on the Code. This makes it possible to compare algorithms: By running many models on the same dataset, we can find the ones that perform best. To spur innovation on data-centric AI approaches, perhaps it’s time to hold the Code fixed and invite researchers to improve the Data.A huge amount of innovation — in algorithms, ideas, principles, and tools — is needed to make data-centric AI development efficient and effective.When AI was shifting toward deep learning over a decade ago, I didn’t foresee how many thousands of innovations and research papers would be needed to flesh out core tenets of the field. But now I think an equally large amount of work lies ahead to support a data-centric approach. For example, we need to develop good ways to:Surface and address inconsistencies in data labelsDetect and address data drift and concept driftHelp developers with error analysisSelect and apply the most effective data augmentation techniquesDecide what additional data to collect (rather than collecting more of everything)Merge inconsistent data sourcesTrack data provenance and lineage, so we can address problems in the data, such as bias, that may be discovered laterBenchmarks and competitions in which teams are asked to improve the data rather than the code would better reflect the workloads of many practical applications. I hope that such benchmarks also will spur research and help engineers gain experience working on data. The Human Computer Interface (HCI) community also has a role in designing user interfaces that help developers and subject-matter experts work efficiently with data.I asked for feedback on the idea of a data-centric competition on social media (Twitter, LinkedIn, Facebook). I’ve read all the responses so far — thanks to all who replied. If you have thoughts on this, please join the discussion there.Keep learning!Andrew",
        "summary": "*   Benchmarks have primarily driven progress in model architecture (Code) by fixing data, neglecting data-centric AI development (Data).\n*   A new benchmark type is proposed where Code is fixed and researchers improve the Data to spur data-centric innovation.\n*   Significant innovation is needed for data-centric AI, including methods for handling data inconsistencies, drift, error analysis, augmentation, collection, merging, provenance, and bias.\n*   Data-centric benchmarks would better reflect practical AI workloads and foster engineer experience in data work.",
        "publish_date": "May 26, 2021",
        "keywords": [
            "data-centric AI",
            "data inconsistencies",
            "drift",
            "error analysis",
            "augmentation"
        ],
        "url": "https://www.deeplearning.ai/the-batch/data-centric-ai-development-a-new-kind-of-benchmark/"
    },
    {
        "content": "Dear friends,We just launched a Data-Centric AI Resource Hub to help you improve the performance of AI systems by systematically engineering the underlying data. It offers new articles by Nvidia director of machine learning research Anima Anandkumar, Stanford computer science professor Michael Bernstein, and Google Brain director of engineering D. Sculley. It also includes talks from the NeurIPS Data-Centric AI Workshop that was held in December. We’ll be adding more helpful articles and videos in coming months.Working effectively with human labelers is a key part of Data-Centric AI. My friend Michael Bernstein is an expert in human-computer interface (HCI), a discipline that offers many insights for empowering labelers. His article explains some of the most important ones.For example, given a task in computer vision, natural language processing, or speech recognition, it’s common to ask several crowdsourced labelers to annotate the same example and take the mean or majority-vote label. Many clever ideas have been proposed to improve the labeling process, such as testing labeler accuracy, developing novel voting mechanisms, and routing examples to labelers in sophisticated ways.Surprisingly, Michael has found that it's often better to invest in hiring and training a few annotators than to focus on improving the process. Alternatively, the best process may be one that enables you to build a small team of skilled labelers.Working with a smaller, committed team also makes it easier to discover and fix ambiguities in your labeling instructions. Michael writes, “When something goes wrong, your reactions should be, ‘What did I do wrong in communicating my intent?,’ not, ‘Why weren’t they paying attention?’”Every machine learning engineer and data scientist can take advantage of Data-Centric AI techniques. And, because the data-centric approach changes the workflow of AI development, software engineers and product managers can also benefit. So please visit the Data-Centric AI Resource Hub, and tell your friends and colleagues about it, too.Keep learning!Andrew",
        "summary": "*   A Data-Centric AI Resource Hub was launched to improve AI system performance by systematically engineering data, featuring articles by experts and talks from the NeurIPS Data-Centric AI Workshop held in December.\n*   Working effectively with human labelers is a key part of Data-Centric AI, with insights from HCI discussed for empowering labelers.\n*   It is often better to invest in hiring and training a few annotators than to focus on improving the labeling process, as a small, committed team facilitates discovering and fixing ambiguities in instructions.\n*   Machine learning engineers, data scientists, software engineers, and product managers can benefit from Data-Centric AI techniques.",
        "publish_date": "Feb 16, 2022",
        "keywords": [
            "Data-Centric AI",
            "NeurIPS Data-Centric AI Workshop",
            "human labelers",
            "HCI",
            "systematically engineering data"
        ],
        "url": "https://www.deeplearning.ai/the-batch/new-data-centric-ai-resources/"
    },
    {
        "content": "Dear friends,Around the world, students are graduating. If you’re one of them, or if someone close to you is graduating, congratulations!!!My family swapped pictures on WhatsApp recently and came across this one, which was taken when I graduated from Carnegie Mellon (I’m standing in the middle). I was privileged to have already worked on a few AI projects thanks to my mentors in college, including Michael Kearns, Andrew McCallum, Andrew Moore and Tom Mitchell. But now, looking back, I reflect on how clueless I was and how little I knew about AI, business, people, and the world in general.To this day, I don’t feel particularly clued in. Every year or so, I look back and marvel at how clueless I was a year ago, and I’m pretty sure I’ll feel the same way a year from now. This helps me to act with humility and avoid expressing unwarranted certainty.If you’re graduating:Congratulations on all you’ve learned!I hope that you’ll have the pleasure of learning so much in each coming year that you, too, will marvel at how little you used to know.I also hope you’ll treasure the people around you. Of the people in the picture, I still zoom regularly with my parents (left) and brother (right), but my two grandparents who attended my commencement ceremony are no longer with us.If you’ve already graduated, I hope you’ll take joy in the success of those who are coming up behind you.Love to you all and keep learning,Andrew",
        "summary": "*   The author worked on a few AI projects in college.\n*   Mentors for these AI projects included Michael Kearns, Andrew McCallum, Andrew Moore, and Tom Mitchell.\n*   The author reflects on knowing very little about AI upon graduating.\n*   The author continues to learn significantly each year, realizing how little they knew previously.",
        "publish_date": "Jun 09, 2021",
        "keywords": [
            "AI",
            "Michael Kearns",
            "Andrew McCallum",
            "Andrew Moore",
            "Tom Mitchell"
        ],
        "url": "https://www.deeplearning.ai/the-batch/congratulations/"
    },
    {
        "content": "Dear friends,I’m writing this in Orlando, Florida, where I just spoke at the A3 Business Forum, a group that works to advance industrial automation through AI, robotics, and other tools. This was my first large conference since the pandemic started, and it was good to get out and meet more people (taking appropriate health precautions, of course).I was heartened by the number of AI people at A3. I met entrepreneurs working on computer vision systems for warehouse logistics (for example, finding and moving packages automatically), automated inspection (which I spoke about), controlling fleets of mobile robots, and building factory simulations.Some trends that I took away from the conference:Many attendees observed that manufacturing and industrial automation are still in an early phase of adopting cloud computing and AI, and the number of viable use cases is still small but growing.Several CEOs commented on the high cost of customizing systems for different environments and seemed to be considering vertical platforms — where the customer does the customization — as a promising solution.Some executives in manufacturing and AI told me about overhyped AI applications that had failed and poisoned the well for other teams now trying to follow. This speaks to the importance of avoiding hype.The supply-chain disruptions you read about in the news are real! I heard many stories about nearly-finished products that would have shipped months ago if they weren’t missing a part. It made me feel grateful that, in the software world, we can easily supply as many copies as a customer wishes to purchase.I was pleased to find, in an audience of manufacturing professionals, many learners taking online AI courses. On the flip side, I’m enjoying the opportunity to learn the lingo and techniques of industrial automation. And there is much for all of us to learn! For example, despite having developed and implemented sophisticated computer vision algorithms, many AI practitioners don’t yet appreciate the importance of imaging system design — to make sure your image data is of high quality — as part of building a practical system.Applied AI is inherently interdisciplinary. Melonee Wise, an old friend and roboticist who recently sold her company Fetch Robotics, gave me permission to share that her biggest regret was taking too long to bring in someone with warehouse experience. Let’s approach our work with an awareness that knowledge of other fields is critical to building useful systems. Stay curious and . . .Keep learning!Andrew",
        "summary": "*   AI is being applied in industrial automation for tasks such as computer vision in warehouse logistics, automated inspection, and controlling mobile robots.\n*   Manufacturing and industrial automation are in an early phase of adopting cloud computing and AI, with a small but growing number of viable use cases.\n*   Challenges include the high cost of customizing systems, leading to consideration of vertical platforms, and the negative impact of failed, overhyped AI applications.\n*   Applied AI is inherently interdisciplinary, requiring knowledge from other fields like imaging system design for computer vision or domain expertise for robotics.",
        "publish_date": "Feb 02, 2022",
        "keywords": [
            "Computer Vision",
            "Automated inspection",
            "Controlling mobile robots",
            "Applied AI",
            "Robotics"
        ],
        "url": "https://www.deeplearning.ai/the-batch/ai-for-industrial-automation-and-beyond/"
    },
    {
        "content": "Dear friends, I have a two-year-old daughter, and am expecting my son to be born later this week. When I think about what we can do to build a brighter future for our children, the most important thing is to create a foundation for education. Because education is knowledge, and knowledge is human progress. Today Coursera, which I co-founded almost nine years ago to transform lives through learning, became a publicly listed company. I remember building the machine learning course that wound up being the first course on Coursera. There were many Friday nights when I met friends for dinner and then headed back to the office to record videos until 3 a.m. I felt privileged and humbled sitting in a room by myself speaking to a webcam, knowing I was playing a small role in helping thousands of learners. Of course, Coursera quickly became much bigger than a professor and a webcam. I’m grateful to my cofounder Daphne Koller, my early team members, our university partners, instructors, investors, advisors, executives, board members, and 1,000-plus employees over the years. Special shout-out to the company’s CEO Jeff Maggioncalda, who treasures the education mission as much as I do. Most of all, I want to thank all the learners. Let's face it — learning is fun, but it can also be hard work. I remember once reading an article about the percentage of programmers who were self-taught. I couldn’t understand anything less than 100 percent, because I think all learners are self taught. Teachers can play a role, but ultimately it's up to learners to learn. So thank you for watching the online videos, doing homework, and spending your spare time to master these materials. Coursera was launched on April 18, 2012 (the company and I share a birthday!). I hope we’ll continue to reach more learners, because everyone should be a lifelong learner, and everyone should have the opportunity to transform their life through learning. The education mission is bigger than any person or single institution. If we can unlock the full potential in every person, we will move humanity forward. (This letter is excerpted from a speech I made at Coursera’s IPO event earlier today.) Keep learning! Andrew",
        "summary": "* The first course launched on Coursera was a machine learning course.\n* The founder built this initial machine learning course.\n* Building this course involved significant effort, including working until 3 a.m.",
        "publish_date": "Mar 31, 2021",
        "keywords": [
            "machine learning",
            "course",
            "building",
            "Coursera",
            "founder"
        ],
        "url": "https://www.deeplearning.ai/the-batch/coursera-goes-public/"
    },
    {
        "content": "Dear friends,So you’ve trained an accurate neural network model in a Jupyter notebook. You should celebrate! But . . . now what? Machine learning engineering in production is an emerging discipline that helps individual engineers and teams put models into the hands of users.That’s why I’m excited that DeepLearning.AI is launching Machine Learning Engineering for Production Specialization (MLOps). I teach this specialization along with co-instructors Robert Crowe and Laurence Moroney from Google. It also draws on insights from my team at Landing AI, which has worked with companies in a wide range of industries.The work of building and putting machine learning models into production is undergoing a dramatic shift from individually crafted, boutique systems to ones built using consistent processes and tools. This specialization will put you at the forefront of that movement.I remember doing code version control by emailing C++ files to collaborators as attachments with a note saying, “I’m done, you can edit this now.” The process was laborious and prone to error. Thank goodness we now have tools and practices for version control that make team coding more manageable. And I remember implementing neural networks in C++ or Python and working on the first version of distbelief, the precursor to TensorFlow. Tools like TensorFlow and PyTorch have made building complex neural networks much easier.Building and deploying production systems still requires a lot of manual work. Things like discovering and correcting data issues, spotting data drift and concept drift, managing training, carrying out error analysis, auditing performance, pushing models to production, and managing computation and scaling.But these tasks are becoming more systematic. MLOps, or machine learning operations, is a set of practices that promise to empower engineers to build, deploy, monitor, and maintain models reliably and repeatably at scale. Just as git, TensorFlow, and PyTorch made version control and model development easier, MLOps tools will make machine learning far more productive.For me, teaching this course was an unusual experience. MLOps standards and tools are still evolving, so it was exciting to survey the field and try to convey to you the cutting edge. I hope you will find it equally exciting to learn about this frontier of ML development, and that the skills you gain from this will help you build and deploy valuable ML systems.Keep learning!Andrew",
        "summary": "*   Machine learning engineering in production, or MLOps, is an emerging discipline focused on putting trained models into the hands of users reliably and repeatably at scale.\n*   MLOps addresses tasks such as discovering/correcting data issues, spotting data drift and concept drift, managing training, error analysis, auditing performance, pushing models to production, and managing computation and scaling.\n*   MLOps is a set of practices promising to empower engineers to build, deploy, monitor, and maintain models reliably and repeatably at scale.\n*   MLOps standards and tools are still evolving.",
        "publish_date": "May 12, 2021",
        "keywords": [
            "MLOps",
            "data drift",
            "concept drift",
            "production",
            "deploy"
        ],
        "url": "https://www.deeplearning.ai/the-batch/introducing-the-machine-learning-engineering-for-production-mlops-specialization/"
    },
    {
        "content": "Dear friends,Each year, the public relations agency Edelman produces a report on the online public’s trust in social institutions like government, media, and business. The latest Edelman Trust Barometer contains a worrisome finding: While technology was ranked the most trusted industry in the U.S. last year, this year we plunged to ninth place. Trust in the tech industry fell to new lows in the majority of 27 countries surveyed.Tech can be a huge force for moving the world forward, but many well meaning efforts will run into headwinds if we aren’t able to gain others’ trust. It’s more urgent than ever that we collectively act in a way that is genuinely deserving of the rest of society’s trust.Trust is much harder to build than to destroy. One company that hypes AI can do more damage than 10 others that speak about it responsibly. One company that makes misleading statements can do more damage than 10 that speak honestly.How can we regain trust? Several steps are needed, but to my mind, chief among them are:Straight talk. I think we’re all tired of hearing tech companies say they’re fighting for small businesses when they’re just fighting for their own bottom line. I realize that no company can address every issue under the sun, but when we speak about something, we owe it to the public to tell it like it is.Take responsibility. Tech’s influence on what people see and hear has a huge impact on their perception of reality. Our collective influence on automation has a huge impact on jobs. I hope that each organization will acknowledge the power it has and use it to benefit society.Engage and empathize. When someone who is honest and well meaning has a problem with what we do, our first step should be to try to understand their point of view, not to dismiss their concerns. Society has reasonable worries about tech’s concentration of power, fairness, and impact on jobs. Whether we agree or disagree in a certain instance, let's acknowledge the concern and see if we can address it honestly.Trying to fool the public and government officials doesn’t work. We often read in the news about politicians who know little about tech, and say things that reflect their lack of understanding. But let me tell you this: Every large government has at least a handful of people who are tech-savvy enough to see through the spin to the heart of an issue. Companies shouldn’t try to fool people and instead do the harder — but more effective — work of solving problems thoughtfully.On the plus side, 62 percent of respondents to Edelman’s survey agreed that employees have the power to force corporations to change. CEO aren’t the only people responsible for what companies do. All employees have a responsibility to help build trustworthy businesses. Wherever you work, I hope you’ll support straight talk, taking responsibility, and engaging and empathizing.Keep learning!Andrew",
        "summary": "* Trust in the tech industry has significantly declined, with concerns raised about issues like hyping AI.\n* Tech's influence, particularly automation, has a huge impact on jobs and raises societal worries about fairness and power concentration.\n* Regaining trust requires straight talk, taking responsibility for tech's impact (including automation on jobs), and engaging with concerns about fairness and power.",
        "publish_date": "Apr 07, 2021",
        "keywords": [
            "AI",
            "Automation",
            "Trust",
            "Fairness",
            "Jobs"
        ],
        "url": "https://www.deeplearning.ai/the-batch/can-tech-regain-the-publics-trust/"
    },
    {
        "content": "Dear friends,Earlier today, I spoke at a DeepLearning.AI event about MLOps, a field that aims to make building and deploying machine learning models more systematic. AI system development will move faster if we can shift from being model-centric to being data-centric. You can watch a video of the event here. Unlike traditional software, which is powered by code, AI systems are built using both code (including models and algorithms) and data:AI systems = Code + DataWhen a system isn’t performing well, many teams instinctually try to improve the Code. But for many practical applications, it’s more effective instead to focus on improving the Data.Progress in machine learning has been driven for decades by efforts to improve performance on benchmark datasets, in which researchers hold the Data fixed while improving the Code. But for many applications — especially ones where the dataset size is modest (<10,000 examples) — teams will make faster progress by focusing instead on making sure the dataset is good:Is the definition of y given x clear and unambiguous? For example, do different data labelers draw bounding boxes consistently? Do speech transcriptionists label ambiguous audio consistently, for instance, writing “um, yes please” rather than “um … yes please”?Does the input distribution x sufficiently cover the important cases?Does the data incorporate timely feedback from the production system, so we can track concept and data drift?It’s a common joke that 80 percent of machine learning is actually data cleaning, as though that were a lesser task. My view is that if 80 percent of our work is data preparation, then ensuring data quality is the important work of a machine learning team.Rather than counting on engineers to chance upon the best way to improve a dataset, I hope we can develop MLOps tools that help make building AI systems, including building high-quality datasets, more repeatable and systematic. MLOps is a nascent field, and different people define it differently. But I think the most important organizing principle of MLOps teams and tools should be to ensure the consistent and high-quality flow of data throughout all stages of a project. This will help many projects go more smoothly. I have much more to say on this topic, so check out my talk here. Thanks to my team at Landing AI for helping to crystalize these thoughts.Keep learning!Andrew",
        "summary": "*   MLOps aims to make building and deploying machine learning models more systematic.\n*   AI systems are built using both code and data (AI systems = Code + Data).\n*   Shifting from a model-centric to a data-centric approach, focusing on improving data quality, can be more effective for improving AI system performance, especially for modest datasets (<10,000 examples).\n*   Ensuring consistent and high-quality data flow throughout all stages should be a key organizing principle for MLOps teams and tools.",
        "publish_date": "Mar 24, 2021",
        "keywords": [
            "MLOps",
            "data-centric",
            "data quality",
            "data flow",
            "machine learning models"
        ],
        "url": "https://www.deeplearning.ai/the-batch/data-centric-ai-development-part-2-a-critical-shift-in-perspective/"
    },
    {
        "content": "Dear friends,Last week, I wrote about the grand challenge of artificial general intelligence. Other scientific and engineering grand challenges inspire me as well. For example, fusion energy, extended lifespans, and space colonization have massive potential to remake civilization (for good or ill).These grand challenges share a few traits:A solution would transform the way most people live, hopefully — but not necessarily — for the better.Brilliant engineers have been working toward these goals for decades. While they might be reached within our lifetimes, there’s no guarantee.They’re technically complex. Thus, it’s difficult for a layperson (and often even experts) to chart a path forward.Despite their extreme uncertainty, such projects fill my mind with hopes and dreams. Fusion energy promises a safe, clean, unlimited source of electricity. The ability to harvest energy from the fusion of atoms could mitigate climate change and remake geopolitics by empowering all countries to become energy-independent.Extended lifespans could enable people to accumulate greater wisdom. Of course, they could also concentrate wealth and power in the hands of the longest-lived individuals and create difficult demographic challenges. Purported longevity compounds like resveratrol have fallen short of their promise, but I’m excited by studies on the use of metformin and other compounds to lengthen lifespans.Space colonization that carries robots and, someday, humans to distant planets, solar systems, and ultimately galaxies would extend the future course of human history beyond the duration of Earth and into a practically unlimited future. Spacefaring technology would lead humanity into uncharted realms much like homo sapiens’ departure from Africa led to a global civilization.Like artificial general intelligence, these grand challenges have motivated their share of overhyped startups, scorn from skeptics, and tireless enthusiasm from believers. Yet I hope to see progress in all of them within my lifetime. (If we manage to extend lifetimes, that could be a very long time.)The most exciting thing is that AI developers can play a role in achieving them!DeepMind recently used AI to control fusion reactions. More generally, AI is helping to design and simulate large-scale physical systems.AI is making inroads into many aspects of healthcare including drug discovery. These include scientific research as well as startups that focus on human longevity.Automated control has a longstanding role in space exploration. The latency of communication between Earth and distant planets makes it infeasible to control in real time, say, a vehicle on Mars using a joystick on Earth. Fun fact: Jagriti Agrawal, a founding team member of Kira Learning (disclosure: an AI Fund portfolio company), wrote software that runs on NASA’s Perseverance Mars rover.AI is not panacea. But as a general-purpose technology, it can be applied to these grand challenges and others. Whenever I’m interested in a topic, be it climate change or quantum computing, my background in AI makes it easier to strike up a fruitful conversation with domain experts. All of us in AI have tools that could be useful to them.Keep learning!Andrew",
        "summary": "* Artificial General Intelligence (AGI) is presented as a grand scientific and engineering challenge.\n* AI is a general-purpose technology applicable to other grand challenges like fusion energy, extended lifespans, and space colonization.\n* Specific applications mentioned include DeepMind using AI to control fusion reactions, AI helping design/simulate large-scale physical systems, AI in healthcare/drug discovery/longevity startups, and automated control/software in space exploration (e.g., NASA's Perseverance Mars rover).",
        "publish_date": "Mar 16, 2022",
        "keywords": [
            "DeepMind",
            "Automated control",
            "Perseverance Mars rover",
            "Fusion reactions",
            "Drug discovery"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-ai-can-help-achieve-humanitys-grand-challenges/"
    },
    {
        "content": "Dear friends,I’ve always thought that building artificial general intelligence — a system that can learn to perform any mental task that a typical human can — is one of the grandest challenges of our time. In fact, nearly 17 years ago, I co-organized a NeurIPS workshop on building human-level AI.Artificial general intelligence (AGI) was a controversial topic back then and remains so today. But recent progress in self-supervised learning, which learns from unlabeled data, makes me nostalgic for the time when a larger percentage of deep learning researchers — even though it was a very small group — focused on algorithms that might play a role in mimicking how the human brain learns.Obviously, AGI would have extraordinary value. At the same time, it’s a highly technical topic, which makes it challenging for laypeople — and even experts — to judge which approaches are feasible and worthwhile. Over the years, the combination of AGI’s immense potential value and technical complexity has tempted entrepreneurs to start businesses on the argument that, if they have even a 1 percent chance of success, they could be very valuable. Around a decade ago, this led to a huge amount of hype around AGI, generated sometimes by entrepreneurs promoting their companies and sometimes by business titans who bought into the hype.Of course, AGI doesn’t exist yet and there’s no telling if or when it will. The volume of hype around it has made many respectable scientists shy away from talking about it. I’ve seen this in other disciplines as well. For decades, overoptimistic hopes that cold fusion would soon generate cheap, unlimited, safe electricity have been dashed repeatedly so that, for a time, even responsible scientists risked their reputations by talking about it.The hype around AGI has died down compared to a few years ago. That makes me glad, because it creates a better environment for doing the work required to make progress toward it. I continue to believe that some combination of learning algorithms, likely yet to be invented, will get us there someday. Sometimes I wonder whether scaling up certain existing unsupervised learning algorithms would allow neural networks to learn more complex patterns, for instance self-taught learning and self-supervised learning. Or — to go farther out on a limb — sparse coding algorithms that learn sparse feature representations. I look forward also to a foundation model that can learn rich representations of the world from hundreds of thousands of hours of video.If you dream of making progress toward AGI yourself, I encourage you to keep dreaming! Maybe some readers of The Batch one day will make significant contributions toward this grand challenge.Keep learning!Andrew",
        "summary": "*   Artificial General Intelligence (AGI), defined as a system capable of learning any typical human mental task, is a controversial topic that does not yet exist.\n*   AGI's immense potential value and technical complexity led to significant hype around a decade ago, which has since died down.\n*   Progress toward AGI may involve scaling existing unsupervised learning (like self-supervised learning), sparse coding algorithms, or foundation models learning from video, though new algorithms are likely needed.",
        "publish_date": "Mar 09, 2022",
        "keywords": [
            "self-supervised learning",
            "sparse coding algorithms",
            "foundation models",
            "unsupervised learning",
            "learning from video"
        ],
        "url": "https://www.deeplearning.ai/the-batch/artificial-general-intelligence-hope-or-hype/"
    },
    {
        "content": "Dear friends,Last week, DeepLearning.AI invited a group of learners to our Palo Alto office’s courtyard. We had a good time chatting about paths into AI, career trajectories, applications people were working on, and challenges they were facing. You can see the group below.A few people mentioned the challenge of persuading others to try a machine learning solution. Even at leading tech companies, it’s not uncommon for someone to say, “Yes, machine learning may work well for other applications, but for what we’re doing, non-learning software works fine.”Still, machine learning might work better. If you believe that a learning algorithm can help optimize server allocations, improve product recommendations, or automate some part of a business process, how can you push your idea forward?Here are some tips that have worked for me:Ask everyone who would be affected for their perspective, and share yours with them. AI projects can be complex, and many things can go wrong. Colleagues can alert you to issues you’ll need to address, such as difficulty gathering data, complexity of software integration, the need to reorganize workflows, how to manage the occasional incorrect prediction, as well as safety, fairness, and regulatory concerns.Bring evidence that a machine learning system could work. You might build a quick proof of concept. Or you might find related work, either in the academic literature or reports of other companies, to persuade others that it could work for your organization, too.Bring in outside consultants, advisors, or speakers. Their expertise can help persuade your team. (True story: I’ve met several people who have asked their non-technical teammates to take the AI for Everyone course. They’ve found that things move forward more easily when everyone involved has a basic business understanding of AI).Find allies. One forward-thinking partner can make all the difference! Persuading the first person is usually the hardest part. The first can help you persuade the second, and together you can persuade the third.Throughout this process, be open to learning that your idea isn’t sound after all or that it might need to change before it can be successful. I would guess that almost every successful AI application you read about in The Batch required someone to persuade others to give machine learning a shot.Don’t let the skeptics shut you down. Don’t give up, keep pushing, and . . .Keep learning!Andrew",
        "summary": "* The challenge exists in persuading others to adopt machine learning solutions over existing non-learning software, even at leading tech companies.\n* Strategies to push ML ideas forward include gathering perspectives from affected parties, addressing potential issues (data, integration, safety, etc.), and being open to modifying the idea.\n* Providing evidence like a proof of concept or related work, and leveraging outside expertise (consultants, courses like \"AI for Everyone\") can help persuade others.\n* Finding allies is crucial for building momentum, and persistence is needed despite skepticism.",
        "publish_date": "Mar 02, 2022",
        "keywords": [
            "machine learning",
            "data",
            "integration",
            "safety",
            "proof of concept"
        ],
        "url": "https://www.deeplearning.ai/the-batch/how-to-advocate-for-ai-in-your-organization/"
    },
    {
        "content": "Dear friends,I’ve always thought that how you treat those who are powerless shows your true character. People rarely mistreat others who have power over them -- for example, their boss — because they might suffer adverse consequences. But when you encounter someone whom you can either push down or lift up, with no risk of harm or possibility of gain, your choice reveals your character.Similarly, the way a nation treats those with less power — specifically refugees — shows its character. As Russia continues to attack Ukraine, millions of refugees are streaming across Europe. They join refugees from Afghanistan, Syria, Congo, Myanmar, Iraq and other nations in seeking safety. I’ve been heartened by news that Poland, Romania, Moldova, Hungary, Germany, France, Ireland, the United Kingdom and other countries are offering them a safe haven. I hope the U.S. will open its doors wider to all refugees.Historically, refugees have made rich contributions to their host nations. The U.S. would have been a very different country without Albert Einstein, Madeleine Albright, and Sergey Brin, all of whom were refugees. Countries that welcome refugees today may find tomorrow that they’ve adopted the next Einstein, yielding great prestige and prosperity.Of course, integrating refugees is not a trivial matter. They must adjust to a new home, their host country must adapt to a more diverse population, and local people may worry about competition for jobs and resources. But the need to welcome people fleeing for their lives is pressing. Surely we can find it in ourselves to share with those who have lost everything.Treating people well regardless of their power should be a key part of building in AI as well. I would love to see the AI community assist displaced Ukrainian engineers. At the same time, let’s help Russian engineers who don’t support the war and want to emigrate and build a new life in a different country.When developers write software, there’s an economic temptation to focus on serving people who have power: How can one show users of a website who have purchasing power an advertisement that motivates them to click? To build a fairer society, let’s also make sure that our software treats all people well, including the least powerful among us.Keep learning!AndrewP.S. I just spoke at Nvidia’s GPU Technology Conference about data-centric AI, where I showed the first public demo of data-centric features of LandingLens, an MLOps platform for computer vision built by my team at Landing AI. A highlight for me came during the question-and-answer session, when my friend Bryan Catanzaro, Nvidia’s vice president of applied research, mentioned that the company’s cutting-edge Deep Learning Super Sampling project, which applies deep learning to graphics, uses a data-centric approach. The neural network changes rarely but the team improves the data! You can register for conference and watch a video of the presentation here.",
        "summary": "*   Data-centric AI was discussed at Nvidia’s GPU Technology Conference.\n*   LandingLens, an MLOps platform for computer vision by Landing AI, features data-centric capabilities.\n*   Nvidia’s Deep Learning Super Sampling project, applying deep learning to graphics, uses a data-centric approach focused on improving data while the neural network changes rarely.\n*   Software/AI development has an economic temptation to focus on powerful users (e.g., targeted ads), emphasizing the need to ensure software treats all people well.",
        "publish_date": "Mar 23,  2022",
        "keywords": [
            "Data-centric AI",
            "LandingLens",
            "MLOps",
            "computer vision",
            "Deep Learning Super Sampling"
        ],
        "url": "https://www.deeplearning.ai/the-batch/welcoming-refugees/"
    }
]